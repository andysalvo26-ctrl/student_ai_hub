{"chunk_id": "www_library_jhu_edu__9981251c7844ce31::c0000", "stable_id": "www_library_jhu_edu__9981251c7844ce31", "url": "https://www.library.jhu.edu/news/2025/09/approaching-the-black-box-how-ai-works-in-5-definitions/", "final_url": "https://www.library.jhu.edu/news/2025/09/approaching-the-black-box-how-ai-works-in-5-definitions/", "retrieved_at": "2026-01-22T00:14:11.159400Z", "title": "Approaching the Black Box: How AI Works in 5 Definitions - Sheridan Libraries", "section": "AI Basics", "source_type": "Explainer / Guide", "content_hash": "26d10a9095ab88f63ea6e9be8bac3bf10497fa7ee733a4f6f73ad431909e52f9", "chunk_index": 0, "char_start": 0, "char_end": 1238, "text": "Welcome to our Responsible AI series, created by the Digital Scholarship and Data Services department as part of its ongoing commitment to promoting ethical and responsible AI practices for the Hopkins community.\n\nAI, or Artificial Intelligence, is technology developed with the goal of performing tasks typically requiring human intelligence, such as recognizing patterns, making decisions, and communicating. AI can refer to a vast array of technologies, but most AI mimics human cognitive abilities by creating a model of the world based on a limited set of information provided to it, and then guessing how the world it has modeled would behave. Generative AI can even generate content that it would expect to exist in that world, in response to prompts from humans.\n\nBecause of the many, often hidden layers of calculations and decisions that lead to the predictions AI systems make, AI is often referred to as a “black box.” While the innermost workings of some AI tools remain unknown – even to their creators – it is important for AI users to understand the foundations of the tools they may use to guide their research and scholarship. The following 5 definitions aim to demystify the building blocks of Generative AI technology."}
{"chunk_id": "www_library_jhu_edu__9981251c7844ce31::c0001", "stable_id": "www_library_jhu_edu__9981251c7844ce31", "url": "https://www.library.jhu.edu/news/2025/09/approaching-the-black-box-how-ai-works-in-5-definitions/", "final_url": "https://www.library.jhu.edu/news/2025/09/approaching-the-black-box-how-ai-works-in-5-definitions/", "retrieved_at": "2026-01-22T00:14:11.159400Z", "title": "Approaching the Black Box: How AI Works in 5 Definitions - Sheridan Libraries", "section": "AI Basics", "source_type": "Explainer / Guide", "content_hash": "26d10a9095ab88f63ea6e9be8bac3bf10497fa7ee733a4f6f73ad431909e52f9", "chunk_index": 1, "char_start": 1238, "char_end": 2478, "text": "- Algorithm: The foundation of all AI tools are algorithms. An algorithm is a series of steps that allows a computer to accomplish a certain task. A simple example is a set of instructions for sorting a list of numbers. One common algorithm for accomplishing this task is the Bubble Sort algorithm, which tells the computer to start at the beginning of a list, compare each number to the next and swap them if they are in the incorrect order. This is a simple example, but algorithms can become as complex as instructions for predicting who you might want to follow on a social media platform or predicting what word will come next in a sentence.\n\n- Machine Learning: In the number sorting example above, a human provided the instructions for the Bubble Sort algorithm to the computer based on what mathematicians know is an efficient approach. But what if the computer could figure out the best way to sort a list of numbers by itself, by evaluating how closely the results of different approaches match a correctly sorted list? Machine learning is the branch of artificial intelligence that teaches computers how to find rules and patterns in a large amount of information in order to make predictions about previously unseen information."}
{"chunk_id": "www_library_jhu_edu__9981251c7844ce31::c0002", "stable_id": "www_library_jhu_edu__9981251c7844ce31", "url": "https://www.library.jhu.edu/news/2025/09/approaching-the-black-box-how-ai-works-in-5-definitions/", "final_url": "https://www.library.jhu.edu/news/2025/09/approaching-the-black-box-how-ai-works-in-5-definitions/", "retrieved_at": "2026-01-22T00:14:11.159400Z", "title": "Approaching the Black Box: How AI Works in 5 Definitions - Sheridan Libraries", "section": "AI Basics", "source_type": "Explainer / Guide", "content_hash": "26d10a9095ab88f63ea6e9be8bac3bf10497fa7ee733a4f6f73ad431909e52f9", "chunk_index": 2, "char_start": 2478, "char_end": 4090, "text": "- Training data: The information used to develop machine learning models is called training data. For example, the machine learning model behind Google Image Search has been shown a vast set of images labeled by humans, which enables it to determine which images contain the terms you searched for. The training data in this example was labeled with the “right” answer by humans, but other types of models use calculations to map out the inherent structure of their training data without human input.\n\n- Large Language Model: One example of a machine learning model that uses this approach is a large language model, or LLM. LLMs are fed a vast amount of human-written text, often sourced from the internet, which they analyze to map out the probabilities underlying human language. LLMs use these models of likelihood to mimic human communication and can accomplish language-related tasks like text summarization, language translation, and question answering. Large language models form the foundation for Generative AI tools like Miscrosoft’s CoPilot, OpenAI’s GPTs, Meta’s LLaMA, xAI’s Grok, and Google’s Gemini.\n\n- Deep learning: Large language models have become so sophisticated by utilizing deep learning, an approach to machine learning in which a model calibrates itself to give more weight to computational tasks that help it perform best by comparing many (sometimes hundreds of) different approaches to making predictions. This technique is known as an “artificial neural network” because it mimics how the human brain processes information and requires large amounts of data and computational power."}
{"chunk_id": "www_library_jhu_edu__9981251c7844ce31::c0003", "stable_id": "www_library_jhu_edu__9981251c7844ce31", "url": "https://www.library.jhu.edu/news/2025/09/approaching-the-black-box-how-ai-works-in-5-definitions/", "final_url": "https://www.library.jhu.edu/news/2025/09/approaching-the-black-box-how-ai-works-in-5-definitions/", "retrieved_at": "2026-01-22T00:14:11.159400Z", "title": "Approaching the Black Box: How AI Works in 5 Definitions - Sheridan Libraries", "section": "AI Basics", "source_type": "Explainer / Guide", "content_hash": "26d10a9095ab88f63ea6e9be8bac3bf10497fa7ee733a4f6f73ad431909e52f9", "chunk_index": 3, "char_start": 4090, "char_end": 5260, "text": "Chatbot\n\nUser interfaces for interacting with generative AI tools that generate human-like text in response to prompts from humans.\n\nExamples: ChatGPT, Microsoft CoPilot, Gemini, DeepSeek, Perplexity, Grok, Claude, Hopkins AI Lab\n\nLarge Language Model\n\nMachine learning models that are designed for natural language processing, particularly language generation. All chatbots are powered by large language models. Some LLMs are made openly available and can be customized and deployed by users. LLMs are also used in many other applications.\n\nExamples: GPT-4, Llama 4, Claude 3, Gemini 3, DeepSeek-R1, Grok 3\n\nGenerative AI\n\nA general term referring to AI tools and technologies capable of generating human-like content in response to prompts, including text, images, music, or speech.\n\nWhere to Get Help with Generative AI at Hopkins\n\n- Hopkins AI Lab\n\n- Using AI Tools for Research\n\n- FAQ on Large Language Model (LLM) AI Tools (Johns Hopkins Data Trust)\n\n- The JHM Office of Human Subjects Research (OHSR)’s Consult Service for Research Involving AI\n\n- Data Science & AI Institute Workshops\n\n- Center for Teaching Excellence and Innovation (CTEI) AI in Practice Series"}
{"chunk_id": "ai_engineering_columbia_edu__8662d32a1c231998::c0000", "stable_id": "ai_engineering_columbia_edu__8662d32a1c231998", "url": "https://ai.engineering.columbia.edu/ai-vs-machine-learning/", "final_url": "https://ai.engineering.columbia.edu/ai-vs-machine-learning/", "retrieved_at": "2026-01-22T00:14:07.375159Z", "title": "Artificial Intelligence (AI) vs. Machine Learning | Columbia AI", "section": "AI Basics", "source_type": "Explainer / Guide", "content_hash": "90ddc3e6025efa86cb7b597ac4f10519588cc39bec2fd9f7056855c8a94b1273", "chunk_index": 0, "char_start": 0, "char_end": 1323, "text": "Artificial Intelligence (AI) vs. Machine Learning\n\nArtificial intelligence (AI) and machine learning are often used interchangeably, but machine learning is a subset of the broader category of AI.\n\nPut in context, artificial intelligence refers to the general ability of computers to emulate human thought and perform tasks in real-world environments, while machine learning refers to the technologies and algorithms that enable systems to identify patterns, make decisions, and improve themselves through experience and data.\n\nComputer programmers and software developers enable computers to analyze data and solve problems — essentially, they create artificial intelligence systems — by applying tools such as:\n\n- machine learning\n\n- deep learning\n\n- neural networks\n\n- computer vision\n\n- natural language processing\n\nBelow is a breakdown of the differences between artificial intelligence and machine learning as well as how they are being applied in organizations large and small today.\n\nWhat Is Artificial Intelligence?\n\nArtificial Intelligence is the field of developing computers and robots that are capable of behaving in ways that both mimic and go beyond human capabilities. AI-enabled programs can analyze and contextualize data to provide information or automatically trigger actions without human interference."}
{"chunk_id": "ai_engineering_columbia_edu__8662d32a1c231998::c0001", "stable_id": "ai_engineering_columbia_edu__8662d32a1c231998", "url": "https://ai.engineering.columbia.edu/ai-vs-machine-learning/", "final_url": "https://ai.engineering.columbia.edu/ai-vs-machine-learning/", "retrieved_at": "2026-01-22T00:14:07.375159Z", "title": "Artificial Intelligence (AI) vs. Machine Learning | Columbia AI", "section": "AI Basics", "source_type": "Explainer / Guide", "content_hash": "90ddc3e6025efa86cb7b597ac4f10519588cc39bec2fd9f7056855c8a94b1273", "chunk_index": 1, "char_start": 1323, "char_end": 2752, "text": "Today, artificial intelligence is at the heart of many technologies we use, including smart devices and voice assistants such as Siri on Apple devices. Companies are incorporating techniques such as natural language processing and computer vision — the ability for computers to use human language and interpret images — to automate tasks, accelerate decision making, and enable customer conversations with chatbots.\n\nWhat Is Machine Learning?\n\nMachine learning is a pathway to artificial intelligence. This subcategory of AI uses algorithms to automatically learn insights and recognize patterns from data, applying that learning to make increasingly better decisions.\n\nBy studying and experimenting with machine learning, programmers test the limits of how much they can improve the perception, cognition, and action of a computer system.\n\nDeep learning, an advanced method of machine learning, goes a step further. Deep learning models use large neural networks — networks that function like a human brain to logically analyze data — to learn complex patterns and make predictions independent of human input.\n\nHow Companies Use AI and Machine Learning\n\nTo be successful in nearly any industry, organizations must be able to transform their data into actionable insight. Artificial Intelligence and machine learning give organizations the advantage of automating a variety of manual processes involving data and decision making."}
{"chunk_id": "ai_engineering_columbia_edu__8662d32a1c231998::c0002", "stable_id": "ai_engineering_columbia_edu__8662d32a1c231998", "url": "https://ai.engineering.columbia.edu/ai-vs-machine-learning/", "final_url": "https://ai.engineering.columbia.edu/ai-vs-machine-learning/", "retrieved_at": "2026-01-22T00:14:07.375159Z", "title": "Artificial Intelligence (AI) vs. Machine Learning | Columbia AI", "section": "AI Basics", "source_type": "Explainer / Guide", "content_hash": "90ddc3e6025efa86cb7b597ac4f10519588cc39bec2fd9f7056855c8a94b1273", "chunk_index": 2, "char_start": 2752, "char_end": 3970, "text": "By incorporating AI and machine learning into their systems and strategic plans, leaders can understand and act on data-driven insights with greater speed and efficiency.\n\nAI in the Manufacturing Industry\n\nEfficiency is key to the success of an organization in the manufacturing industry. Artificial intelligence can help manufacturing leaders automate their business processes by applying data analytics and machine learning to applications such as the following:\n\n- Identifying equipment errors before malfunctions occur, using the internet of things (IoT), analytics, and machine learning\n\n- Using an AI application on a device, located within a factory, that monitors a production machine and predicts when to perform maintenance, so it doesn’t fail mid-shift\n\n- Studying HVAC energy consumption patterns and using machine learning to adjust to optimal energy saving and comfort level\n\nAI and Machine Learning in Banking\n\nData privacy and security are especially critical within the banking industry. Financial services leaders can keep customer data secure while increasing efficiencies using AI and machine learning in several ways:\n\n- Using machine learning to detect and prevent fraud and cybersecurity attacks"}
{"chunk_id": "ai_engineering_columbia_edu__8662d32a1c231998::c0003", "stable_id": "ai_engineering_columbia_edu__8662d32a1c231998", "url": "https://ai.engineering.columbia.edu/ai-vs-machine-learning/", "final_url": "https://ai.engineering.columbia.edu/ai-vs-machine-learning/", "retrieved_at": "2026-01-22T00:14:07.375159Z", "title": "Artificial Intelligence (AI) vs. Machine Learning | Columbia AI", "section": "AI Basics", "source_type": "Explainer / Guide", "content_hash": "90ddc3e6025efa86cb7b597ac4f10519588cc39bec2fd9f7056855c8a94b1273", "chunk_index": 3, "char_start": 3970, "char_end": 5628, "text": "- Integrating biometrics and computer vision to quickly authenticate user identities and process documents\n\n- Incorporating smart technologies such as chatbots and voice assistants to automate basic customer service functions\n\nAI Applications in Health Care\n\nThe health care field uses huge amounts of data and increasingly relies on informatics and analytics to provide accurate, efficient health services. AI tools can help improve patient outcomes, save time, and even help providers avoid burnout by:\n\n- Analyzing data from users’ electronic health records through machine learning to provide clinical decision support and automated insights\n\n- Integrating an AI system that predicts the outcomes of hospital visits to prevent readmissions and shorten the time patients are kept in hospitals\n\n- Capturing and recording provider-patient interactions in exams or telehealth appointments using natural-language understanding\n\nLearn more about how AI is changing the world of health care.\n\nIntegrate AI and Machine Learning into Your Company\n\nThe online Artificial Intelligence executive certificate program, offered through the Fu Foundation School of Engineering and Applied Science at Columbia University, prepares you with the skills and insights to drive AI strategy and adoption across your organization.\n\nWith courses that address algorithms, machine learning, data privacy, robotics, and other AI topics, this non-credit program is designed for forward-thinking team leaders and technically proficient professionals who want to gain a deeper understanding of the applications of AI. You can complete the program in 18 months while continuing to work."}
{"chunk_id": "library_educause_edu__5d89022942069cae::c0000", "stable_id": "library_educause_edu__5d89022942069cae", "url": "https://library.educause.edu/topics/infrastructure-and-research-technologies/artificial-intelligence-ai", "final_url": "https://library.educause.edu/topics/infrastructure-and-research-technologies/artificial-intelligence-ai", "retrieved_at": "2026-01-22T00:14:09.372967Z", "title": "Artificial Intelligence (AI) | EDUCAUSE Library", "section": "AI Basics", "source_type": "University / Official", "content_hash": "7c9c101f23f10f4fd55c62c17a21a118a546db2de401ec760afa9d5ad64e4053", "chunk_index": 0, "char_start": 0, "char_end": 519, "text": "Artificial Intelligence (AI)\n\nRecent Spotlight\n\n-\n\n-\n\nThree Years In: Reflections and Considerations for the Next Chapter of AI in Higher Education\n\nArtificial intelligence has the potential to transform higher education by connecting student experiences, improving operations, and enhancing outcomes. But unlocking its full potential will require better data integration, deeper cross-campus collaboration, well-defined ROI measures, increased environmental transparency, and more nuanced conversations about its role."}
{"chunk_id": "www_heinz_cmu_edu__8bda06f17ee8ff1a::c0000", "stable_id": "www_heinz_cmu_edu__8bda06f17ee8ff1a", "url": "https://www.heinz.cmu.edu/media/2023/July/artificial-intelligence-explained", "final_url": "https://www.heinz.cmu.edu/media/2023/July/artificial-intelligence-explained", "retrieved_at": "2026-01-22T00:14:12.821065Z", "title": "Artificial Intelligence, Explained | Carnegie Mellon University's Heinz College", "section": "AI Basics", "source_type": "Explainer / Guide", "content_hash": "e03282b12e05c6fd75a56c3dbd08e59e90a5bcd8b2852385d72a15c1200c9121", "chunk_index": 0, "char_start": 0, "char_end": 1207, "text": "Artificial Intelligence, Explained\n\nBy Jennifer Monahan\n\nSo why the current hype cycle around AI? What’s different now?\n\nThe most recent iterations of AI – called “generative” AI – can do things that look, sound, and feel eerily human.\n\nWhy It Matters\n\nAI has the potential to transform various industries, from finance and education to transportation and healthcare. AI can automate repetitive tasks, improve decision-making processes, and enhance the accuracy and speed of data analysis.While the potential benefits are enormous, AI presents significant ethical and societal concerns. Like any tool, AI can be used for good or harm. Carnegie Mellon University’s Block Center for Technology and Society was created to explore how technology can be leveraged for social good.\n\nAs of now, only a few technology super-companies have the capacity to create large-scale generative AI tools. The systems require massive amounts of both computing power and data. By default, a few people who lead these organizations are making decisions about the use of AI that will have widespread consequences for society. It behooves the rest of us to recognize the moment we’re in, and to engage in shaping the path forward."}
{"chunk_id": "www_heinz_cmu_edu__8bda06f17ee8ff1a::c0001", "stable_id": "www_heinz_cmu_edu__8bda06f17ee8ff1a", "url": "https://www.heinz.cmu.edu/media/2023/July/artificial-intelligence-explained", "final_url": "https://www.heinz.cmu.edu/media/2023/July/artificial-intelligence-explained", "retrieved_at": "2026-01-22T00:14:12.821065Z", "title": "Artificial Intelligence, Explained | Carnegie Mellon University's Heinz College", "section": "AI Basics", "source_type": "Explainer / Guide", "content_hash": "e03282b12e05c6fd75a56c3dbd08e59e90a5bcd8b2852385d72a15c1200c9121", "chunk_index": 1, "char_start": 1207, "char_end": 2409, "text": "Some Basic History and Definitions…What AI Is\n\nAlan Turing, one of the founders of AI, suggested in 1950 that if a machine can have a conversation with a human and the human can’t distinguish whether they are conversing with another human or with a machine, the machine has demonstrated human intelligence.\n\nMachine learning (ML) first entered the public consciousness in the 1950s, when television viewers watched a demonstration of Arthur Samuel’s Checkers program defeating its human opponent, Robert Nealy. For a long time, though, AI remained largely confined to the realm of tech geniuses and science fiction enthusiasts.\n\nThose tech geniuses accomplished a number of groundbreaking achievements over the last seven decades, including:\n\nA Timeline\n\n- In 1956, Allen Newell, Herbert Simon, and J.C. Shaw developed Logic Theorist, the first artificially intelligent computer program. They were part of a small group that coined the term “artificial intelligence.”\n\n- In 1957, Frank Rosenblatt developed the Perceptron, an early artificial neural network that recognized patterns.\n\n- In 1965, Joseph Weizenbaum developed ELIZA, the first chatbot; the system used limited natural language processing."}
{"chunk_id": "www_heinz_cmu_edu__8bda06f17ee8ff1a::c0002", "stable_id": "www_heinz_cmu_edu__8bda06f17ee8ff1a", "url": "https://www.heinz.cmu.edu/media/2023/July/artificial-intelligence-explained", "final_url": "https://www.heinz.cmu.edu/media/2023/July/artificial-intelligence-explained", "retrieved_at": "2026-01-22T00:14:12.821065Z", "title": "Artificial Intelligence, Explained | Carnegie Mellon University's Heinz College", "section": "AI Basics", "source_type": "Explainer / Guide", "content_hash": "e03282b12e05c6fd75a56c3dbd08e59e90a5bcd8b2852385d72a15c1200c9121", "chunk_index": 2, "char_start": 2409, "char_end": 3712, "text": "- 1960s and 70s: AI enters mainstream pop culture:\n\n- \"2001: A Space Odyssey\" premiered in movie theaters (1968).\n\n- C-3PO and R2-D2 are introduced to the world via \"Star Wars: A New Hope\" (1977).\n\n- Speak & Spell toy hits the shelves (1978).\n\n- 1974 - 1980: The first \"AI winter\" is a period of decreased funding and consequently slowed research in AI.\n\n- In 1981, the government of Japan allocated $850 million for the Fifth Generation Computer project; the goal was to create systems that could engage in conversation and reason like a human.\n\n- In 1984, NAVLab developed the first autonomous land vehicle.\n\n- The second AI winter occurred between 1987 - 1993.\n\n- In 1997, Deep Blue beat world chess champion Gary Kasparov.\n\n- In 2011, IBM’s Watson defeated Ken Jennings on Jeopardy and Apple added Siri to its iPhones.\n\nCommon Terms\n\nThe terminology around AI can be intimidating. Here’s a glossary of key terms you’ll often hear when people talk about AI.\n\nAlgorithm: a set of rules or instructions that tell a machine what to do with the data input into the system.\n\nDeep Learning: a method of machine learning that lets computers learn in a way that mimics a human brain, by analyzing lots of information and classifying that information into categories. Deep learning relies on a neural network."}
{"chunk_id": "www_heinz_cmu_edu__8bda06f17ee8ff1a::c0003", "stable_id": "www_heinz_cmu_edu__8bda06f17ee8ff1a", "url": "https://www.heinz.cmu.edu/media/2023/July/artificial-intelligence-explained", "final_url": "https://www.heinz.cmu.edu/media/2023/July/artificial-intelligence-explained", "retrieved_at": "2026-01-22T00:14:12.821065Z", "title": "Artificial Intelligence, Explained | Carnegie Mellon University's Heinz College", "section": "AI Basics", "source_type": "Explainer / Guide", "content_hash": "e03282b12e05c6fd75a56c3dbd08e59e90a5bcd8b2852385d72a15c1200c9121", "chunk_index": 3, "char_start": 3712, "char_end": 4954, "text": "Hallucination: a situation where an AI system produces fabricated, nonsensical, or inaccurate information. The wrong information is presented with confidence, which can make it difficult for the human user to know whether the answer is reliable.\n\nLarge Language Model (LLM): a computer program that has been trained on massive amounts of text data such as books, articles, website content, etc. An LLM is designed to understand and generate human-like text based on the patterns and information it has learned from its training. LLMs use natural language processing (NLP) techniques to learn to recognize patterns and identify relationships between words. Understanding those relationships helps LLMs generate responses that sound human—it’s the type of model that powers AI chatbots such as ChatGPT.\n\nMachine Learning (ML): a type of artificial intelligence that uses algorithms which allow machines to learn and adapt from evidence (often historical data), without being explicitly programmed to learn that particular thing.\n\nNatural Language Processing (NLP): the ability of machines to use algorithms to analyze large quantities of text, allowing the machines to simulate human conversation and to understand and work with human language."}
{"chunk_id": "www_heinz_cmu_edu__8bda06f17ee8ff1a::c0004", "stable_id": "www_heinz_cmu_edu__8bda06f17ee8ff1a", "url": "https://www.heinz.cmu.edu/media/2023/July/artificial-intelligence-explained", "final_url": "https://www.heinz.cmu.edu/media/2023/July/artificial-intelligence-explained", "retrieved_at": "2026-01-22T00:14:12.821065Z", "title": "Artificial Intelligence, Explained | Carnegie Mellon University's Heinz College", "section": "AI Basics", "source_type": "Explainer / Guide", "content_hash": "e03282b12e05c6fd75a56c3dbd08e59e90a5bcd8b2852385d72a15c1200c9121", "chunk_index": 4, "char_start": 4954, "char_end": 6739, "text": "Neural Network: a deep learning technique that loosely mimics the structure of a human brain. Just as the brain has interconnected neurons, a neural network has tiny interconnected nodes that work together to process information. Neural networks improve with feedback and training.\n\nToken: the building block of text that a chatbot uses to process and generate a response. For example, the sentence \"How are you today?\" might be separated into the following tokens: [\"How,\" \"are,\" \"you,\" \"today,\" \"?\"]. Tokenization helps the chatbot understand the structure and meaning of the input.\n\nTo understand what’s going on with AI today, it’s helpful to think of AI in phases of development. Early AI systems were machines that received an input – the data they were fed by humans - and then produced a recommendation. That response is based on the way the system was trained, and the algorithms (the math!) that tell the system what to do with the data. It’s computers that can play checkers or chess. It’s Netflix knowing that you loved \"Karate Kid\" and suggesting that you watch \"Cobra Kai.\"\n\nHow Generative AI Works\n\nGenerative AI is a step forward in the development phase. Instead of just reacting to data input, the system takes in data and then uses predictive algorithms (a set of step-by-step instructions) to create original content. In the case of a large language model (LLM), that content can take the form of original poems, songs, screenplays, and the like produced by AI chatbots such as ChatGPT and Google Bard. The “large” in LLMs indicates that the language model is trained on a massive quantity of data. Although the outcome makes it seem like the computer is engaged in creative expression, the system is actually just predicting a set of tokens and then selecting one."}
{"chunk_id": "www_heinz_cmu_edu__8bda06f17ee8ff1a::c0005", "stable_id": "www_heinz_cmu_edu__8bda06f17ee8ff1a", "url": "https://www.heinz.cmu.edu/media/2023/July/artificial-intelligence-explained", "final_url": "https://www.heinz.cmu.edu/media/2023/July/artificial-intelligence-explained", "retrieved_at": "2026-01-22T00:14:12.821065Z", "title": "Artificial Intelligence, Explained | Carnegie Mellon University's Heinz College", "section": "AI Basics", "source_type": "Explainer / Guide", "content_hash": "e03282b12e05c6fd75a56c3dbd08e59e90a5bcd8b2852385d72a15c1200c9121", "chunk_index": 5, "char_start": 6739, "char_end": 8336, "text": "“The model is just predicting the next word. It doesn't understand,” explains Rayid Ghani, professor of machine learning at Carnegie Mellon University’s Heinz College of Information Systems and Public Policy. “But as a user playing around with it, it seems to have amazing capabilities, while having very large blind spots.”\n\nModels like ChatGPT are programmed to select the next token, or word, but not necessarily the most commonly used next word. Chatbots might choose – for example – the fourth most common word in one attempt. When the user submits the exact same prompt to the chatbot the next time, the chatbot could randomly select the second most common word to complete the statement. That’s why we humans can ask a chatbot the same question and receive slightly different responses each time.\n\nTools like Copilot and ChatGPT use that token process to write computer code. Though not always perfect, the initial consensus in the tech industry suggests that these tools can save coders hours of tedious work.\n\nText-to-image models like DALL-E and Stable Diffusion work similarly. The program is trained on lots and lots of pictures and their corresponding descriptions. It learns to recognize patterns and understand the relationships between words and visual elements. So when you give it a prompt that describes an image, it uses those patterns and relationships to generate a new image that fits the description. As a result, these models can create never-before-seen art. A prompt for “Carnegie Mellon University Scotty Dog dancing, in the style of pointillism” produced this fun gem:"}
{"chunk_id": "www_heinz_cmu_edu__8bda06f17ee8ff1a::c0006", "stable_id": "www_heinz_cmu_edu__8bda06f17ee8ff1a", "url": "https://www.heinz.cmu.edu/media/2023/July/artificial-intelligence-explained", "final_url": "https://www.heinz.cmu.edu/media/2023/July/artificial-intelligence-explained", "retrieved_at": "2026-01-22T00:14:12.821065Z", "title": "Artificial Intelligence, Explained | Carnegie Mellon University's Heinz College", "section": "AI Basics", "source_type": "Explainer / Guide", "content_hash": "e03282b12e05c6fd75a56c3dbd08e59e90a5bcd8b2852385d72a15c1200c9121", "chunk_index": 6, "char_start": 8336, "char_end": 9638, "text": "Philosophers, artists, and creative types are actively debating whether these processes constitute creativity or plagiarism.\n\nWhat AI Is Not\n\nDespite the now famous creepy conversation between New York Times writer Kevin Roose and Microsoft’s Bing chatbot, we have not yet entered the phase of sentient AI – or artificial general intelligence (AGI). AGI is still a theoretical idea. Unlike generative AI, which seems to be able to do some of the things humans do, AGI systems would actually mimic or surpass human intelligence. Machines would become self-aware and have consciousness. And if you buy into the premise of movies like \"Terminator\" or \"The Matrix,\" things go south for the human race rather quickly after that. To be clear, that’s not where we are today.AI is also not infallible. Large language models like Bard and ChatGPT have an interesting flaw – sometimes they hallucinate. As in, a user enters a prompt and the system makes up an answer that’s not true in some way. The system might produce an intelligent-sounding essay explaining photosynthesis, and cite as its source a scholarly research paper that doesn’t actually exist. Sometimes the answer is just inaccurate. To complicate matters, the information is presented with confidence and authority; it looks and sounds legitimate."}
{"chunk_id": "www_heinz_cmu_edu__8bda06f17ee8ff1a::c0007", "stable_id": "www_heinz_cmu_edu__8bda06f17ee8ff1a", "url": "https://www.heinz.cmu.edu/media/2023/July/artificial-intelligence-explained", "final_url": "https://www.heinz.cmu.edu/media/2023/July/artificial-intelligence-explained", "retrieved_at": "2026-01-22T00:14:12.821065Z", "title": "Artificial Intelligence, Explained | Carnegie Mellon University's Heinz College", "section": "AI Basics", "source_type": "Explainer / Guide", "content_hash": "e03282b12e05c6fd75a56c3dbd08e59e90a5bcd8b2852385d72a15c1200c9121", "chunk_index": 7, "char_start": 9638, "char_end": 10911, "text": "“You can imagine a physician prompting an AI chatbot to list drugs that have recently been found useful for a particular disease,” explained Ghani. “The model is designed to produce a response that sounds realistic, but it’s not designed to produce factually correct information. It would produce a list of drugs. They might be real; they might be made up. While a physician may have the training and background to separate real from fake, a patient may not be able to do so if given access to such a tool.” You can see the problem.\n\nAI is not inherently fair and just. LLMs are trained on large quantities of data, much of which is scraped from the Internet. That data includes reliable sources right alongside the hate-speech and other sewage that lives in the depths of social media platforms. Technologists have put in some protections – asking ChatGPT to tell a sexist joke elicits the following response:\n\nI'm sorry, but I'm programmed to follow ethical guidelines, and that includes not promoting or sharing any form of sexist, offensive, or discriminatory content. I'm here to help answer questions, engage in meaningful conversations, and provide useful information. If you have any non-offensive questions or topics you'd like to discuss, please feel free to ask."}
{"chunk_id": "www_heinz_cmu_edu__8bda06f17ee8ff1a::c0008", "stable_id": "www_heinz_cmu_edu__8bda06f17ee8ff1a", "url": "https://www.heinz.cmu.edu/media/2023/July/artificial-intelligence-explained", "final_url": "https://www.heinz.cmu.edu/media/2023/July/artificial-intelligence-explained", "retrieved_at": "2026-01-22T00:14:12.821065Z", "title": "Artificial Intelligence, Explained | Carnegie Mellon University's Heinz College", "section": "AI Basics", "source_type": "Explainer / Guide", "content_hash": "e03282b12e05c6fd75a56c3dbd08e59e90a5bcd8b2852385d72a15c1200c9121", "chunk_index": 8, "char_start": 10911, "char_end": 12247, "text": "Humans employing more creative prompts can often circumvent the protections in the AI chatbots. And sometimes the AI system itself is biased, as in the case of hiring tools that discriminate against women or facial recognition software that doesn’t recognize people of color. Bias inherent in an AI model has the potential to exacerbate existing injustice.\n\nMoving Forward\n\nAI is changing the way we live, work, and interact with machines. When all that’s at stake is our Spotify playlist or which Netflix show we watch next, understanding how AI works is probably not important for a large percentage of the population. But with the advent of generative AI into mainstream consciousness, it’s time for all of us to start paying attention and to decide what kind of society we want to live in.\n\nInterested in how machine learning and artificial intelligence will shape the future?\n\nHeinz College empowers data scientists via our Master of Science in Business Intelligence and Data Analytics and Public Policy and Data Analytics programs. The Block Center focuses on how emerging technologies will alter the future of work, how AI and analytics can be harnessed for social good, and how innovation in these spaces can be more inclusive and generate targeted, relevant solutions that reduce inequality and improve quality of life for all."}
{"chunk_id": "mitsloan_mit_edu__b85f152879c15db1::c0000", "stable_id": "mitsloan_mit_edu__b85f152879c15db1", "url": "https://mitsloan.mit.edu/ideas-made-to-matter/machine-learning-explained", "final_url": "https://mitsloan.mit.edu/ideas-made-to-matter/machine-learning-explained", "retrieved_at": "2026-01-22T00:14:16.123232Z", "title": "Machine learning, explained | MIT Sloan", "section": "AI Basics", "source_type": "Explainer / Guide", "content_hash": "b87b1219d40ec6de06d4039de42fbd20f8b51b18a51fcabd6fcae1c61b8a7e0a", "chunk_index": 0, "char_start": 0, "char_end": 1315, "text": "Credit: Andriy Onufriyenko / Getty Images\n\nMachine learning is behind chatbots and predictive text, language translation apps, the shows Netflix suggests to you, and how your social media feeds are presented. It powers autonomous vehicles and machines that can diagnose medical conditions based on images.\n\nWhen companies today deploy artificial intelligence programs, they are most likely using machine learning — so much so that the terms are often used interchangeably, and sometimes ambiguously. Machine learning is a subfield of artificial intelligence that gives computers the ability to learn without explicitly being programmed.\n\n“In just the last five or 10 years, machine learning has become a critical way, arguably the most important way, most parts of AI are done,” said MIT Sloan professor the founding director of the MIT Center for Collective Intelligence. “So that's why some people use the terms AI and machine learning almost as synonymous … most of the current advances in AI have involved machine learning.”\n\nWith the growing ubiquity of machine learning, everyone in business is likely to encounter it and will need some working knowledge about this field. A 2020 Deloitte survey found that 67% of companies are using machine learning, and 97% are using or planning to use it in the next year."}
{"chunk_id": "mitsloan_mit_edu__b85f152879c15db1::c0001", "stable_id": "mitsloan_mit_edu__b85f152879c15db1", "url": "https://mitsloan.mit.edu/ideas-made-to-matter/machine-learning-explained", "final_url": "https://mitsloan.mit.edu/ideas-made-to-matter/machine-learning-explained", "retrieved_at": "2026-01-22T00:14:16.123232Z", "title": "Machine learning, explained | MIT Sloan", "section": "AI Basics", "source_type": "Explainer / Guide", "content_hash": "b87b1219d40ec6de06d4039de42fbd20f8b51b18a51fcabd6fcae1c61b8a7e0a", "chunk_index": 1, "char_start": 1315, "char_end": 2519, "text": "From manufacturing to retail and banking to bakeries, even legacy companies are using machine learning to unlock new value or boost efficiency. “Machine learning is changing, or will change, every industry, and leaders need to understand the basic principles, the potential, and the limitations,” said MIT computer science professor Aleksander Madry, director of the MIT Center for Deployable Machine Learning.\n\nWhile not everyone needs to know the technical details, they should understand what the technology does and what it can and cannot do, Madry added. “I don’t think anyone can afford not to be aware of what’s happening.”\n\nThat includes being aware of the social, societal, and ethical implications of machine learning. “It's important to engage and begin to understand these tools, and then think about how you're going to use them well. We have to use these [tools] for the good of everybody,” said Dr. Joan LaRovere, MBA ’16, a pediatric cardiac intensive care physician and co-founder of the nonprofit The Virtue Foundation. “AI has so much potential to do good, and we need to really keep that in our lenses as we're thinking about this. How do we use this to do good and better the world?”"}
{"chunk_id": "mitsloan_mit_edu__b85f152879c15db1::c0002", "stable_id": "mitsloan_mit_edu__b85f152879c15db1", "url": "https://mitsloan.mit.edu/ideas-made-to-matter/machine-learning-explained", "final_url": "https://mitsloan.mit.edu/ideas-made-to-matter/machine-learning-explained", "retrieved_at": "2026-01-22T00:14:16.123232Z", "title": "Machine learning, explained | MIT Sloan", "section": "AI Basics", "source_type": "Explainer / Guide", "content_hash": "b87b1219d40ec6de06d4039de42fbd20f8b51b18a51fcabd6fcae1c61b8a7e0a", "chunk_index": 2, "char_start": 2519, "char_end": 3859, "text": "What is machine learning?\n\nMachine learning is a subfield of artificial intelligence, which is broadly defined as the capability of a machine to imitate intelligent human behavior. Artificial intelligence systems are used to perform complex tasks in a way that is similar to how humans solve problems.\n\nThe goal of AI is to create computer models that exhibit “intelligent behaviors” like humans, according to Boris Katz, a principal research scientist and head of the InfoLab Group at CSAIL. This means machines that can recognize a visual scene, understand a text written in natural language, or perform an action in the physical world.\n\nMachine learning is one way to use AI. It was defined in the 1950s by AI pioneer Arthur Samuel as “the field of study that gives computers the ability to learn without explicitly being programmed.”\n\nThe definition holds true, according to a lecturer at MIT Sloan and head of machine learning at Kensho, which specializes in artificial intelligence for the finance and U.S. intelligence communities. He compared the traditional way of programming computers, or “software 1.0,” to baking, where a recipe calls for precise amounts of ingredients and tells the baker to mix for an exact amount of time. Traditional programming similarly requires creating detailed instructions for the computer to follow."}
{"chunk_id": "mitsloan_mit_edu__b85f152879c15db1::c0003", "stable_id": "mitsloan_mit_edu__b85f152879c15db1", "url": "https://mitsloan.mit.edu/ideas-made-to-matter/machine-learning-explained", "final_url": "https://mitsloan.mit.edu/ideas-made-to-matter/machine-learning-explained", "retrieved_at": "2026-01-22T00:14:16.123232Z", "title": "Machine learning, explained | MIT Sloan", "section": "AI Basics", "source_type": "Explainer / Guide", "content_hash": "b87b1219d40ec6de06d4039de42fbd20f8b51b18a51fcabd6fcae1c61b8a7e0a", "chunk_index": 3, "char_start": 3859, "char_end": 5134, "text": "But in some cases, writing a program for the machine to follow is time-consuming or impossible, such as training a computer to recognize pictures of different people. While humans can do this task easily, it’s difficult to tell a computer how to do it. Machine learning takes the approach of letting computers learn to program themselves through experience.\n\nMachine learning starts with data — numbers, photos, or text, like bank transactions, pictures of people or even bakery items, repair records, time series data from sensors, or sales reports. The data is gathered and prepared to be used as training data, or the information the machine learning model will be trained on. The more data, the better the program.\n\nFrom there, programmers choose a machine learning model to use, supply the data, and let the computer model train itself to find patterns or make predictions. Over time the human programmer can also tweak the model, including changing its parameters, to help push it toward more accurate results. (Research scientist Janelle Shane’s website AI Weirdness is an entertaining look at how machine learning algorithms learn and how they can get things wrong — as happened when an algorithm tried to generate recipes and created Chocolate Chicken Chicken Cake.)"}
{"chunk_id": "mitsloan_mit_edu__b85f152879c15db1::c0004", "stable_id": "mitsloan_mit_edu__b85f152879c15db1", "url": "https://mitsloan.mit.edu/ideas-made-to-matter/machine-learning-explained", "final_url": "https://mitsloan.mit.edu/ideas-made-to-matter/machine-learning-explained", "retrieved_at": "2026-01-22T00:14:16.123232Z", "title": "Machine learning, explained | MIT Sloan", "section": "AI Basics", "source_type": "Explainer / Guide", "content_hash": "b87b1219d40ec6de06d4039de42fbd20f8b51b18a51fcabd6fcae1c61b8a7e0a", "chunk_index": 4, "char_start": 5134, "char_end": 6432, "text": "Some data is held out from the training data to be used as evaluation data, which tests how accurate the machine learning model is when it is shown new data. The result is a model that can be used in the future with different sets of data.\n\nSuccessful machine learning algorithms can do different things, Malone wrote in a recent research brief about AI and the future of work that was co-authored by MIT professor and CSAIL director Daniela Rus and Robert Laubacher, the associate director of the MIT Center for Collective Intelligence.\n\n“The function of a machine learning system can be descriptive, meaning that the system uses the data to explain what happened; predictive, meaning the system uses the data to predict what will happen; or prescriptive, meaning the system will use the data to make suggestions about what action to take,” the researchers wrote.\n\nThere are three subcategories of machine learning:\n\nSupervised machine learning models are trained with labeled data sets, which allow the models to learn and grow more accurate over time. For example, an algorithm would be trained with pictures of dogs and other things, all labeled by humans, and the machine would learn ways to identify pictures of dogs on its own. Supervised machine learning is the most common type used today."}
{"chunk_id": "mitsloan_mit_edu__b85f152879c15db1::c0005", "stable_id": "mitsloan_mit_edu__b85f152879c15db1", "url": "https://mitsloan.mit.edu/ideas-made-to-matter/machine-learning-explained", "final_url": "https://mitsloan.mit.edu/ideas-made-to-matter/machine-learning-explained", "retrieved_at": "2026-01-22T00:14:16.123232Z", "title": "Machine learning, explained | MIT Sloan", "section": "AI Basics", "source_type": "Explainer / Guide", "content_hash": "b87b1219d40ec6de06d4039de42fbd20f8b51b18a51fcabd6fcae1c61b8a7e0a", "chunk_index": 5, "char_start": 6432, "char_end": 7781, "text": "In unsupervised machine learning, a program looks for patterns in unlabeled data. Unsupervised machine learning can find patterns or trends that people aren’t explicitly looking for. For example, an unsupervised machine learning program could look through online sales data and identify different types of clients making purchases.\n\nReinforcement machine learning trains machines through trial and error to take the best action by establishing a reward system. Reinforcement learning can train models to play games or train autonomous vehicles to drive by telling the machine when it made the right decisions, which helps it learn over time what actions it should take.\n\nIn the Work of the Future brief, Malone noted that machine learning is best suited for situations with lots of data — thousands or millions of examples, like recordings from previous conversations with customers, sensor logs from machines, or ATM transactions. For example, Google Translate was possible because it “trained” on the vast amount of information on the web, in different languages.\n\nIn some cases, machine learning can gain insight or automate decision-making in cases where humans would not be able to, Madry said. “It may not only be more efficient and less costly to have an algorithm do this, but sometimes humans just literally are not able to do it,” he said."}
{"chunk_id": "mitsloan_mit_edu__b85f152879c15db1::c0006", "stable_id": "mitsloan_mit_edu__b85f152879c15db1", "url": "https://mitsloan.mit.edu/ideas-made-to-matter/machine-learning-explained", "final_url": "https://mitsloan.mit.edu/ideas-made-to-matter/machine-learning-explained", "retrieved_at": "2026-01-22T00:14:16.123232Z", "title": "Machine learning, explained | MIT Sloan", "section": "AI Basics", "source_type": "Explainer / Guide", "content_hash": "b87b1219d40ec6de06d4039de42fbd20f8b51b18a51fcabd6fcae1c61b8a7e0a", "chunk_index": 6, "char_start": 7781, "char_end": 9029, "text": "Google search is an example of something that humans can do, but never at the scale and speed at which the Google models are able to show potential answers every time a person types in a query, Malone said. “That’s not an example of computers putting people out of work. It's an example of computers doing things that would not have been remotely economically feasible if they had to be done by humans.”\n\nMachine learning is also associated with several other artificial intelligence subfields:\n\nNatural language processing\n\nNatural language processing is a field of machine learning in which machines learn to understand natural language as spoken and written by humans, instead of the data and numbers normally used to program computers. This allows machines to recognize language, understand it, and respond to it, as well as create new text and translate between languages. Natural language processing enables familiar technology like chatbots and digital assistants like Siri or Alexa.\n\nNeural networks\n\nNeural networks are a commonly used, specific class of machine learning algorithms. Artificial neural networks are modeled on the human brain, in which thousands or millions of processing nodes are interconnected and organized into layers."}
{"chunk_id": "mitsloan_mit_edu__b85f152879c15db1::c0007", "stable_id": "mitsloan_mit_edu__b85f152879c15db1", "url": "https://mitsloan.mit.edu/ideas-made-to-matter/machine-learning-explained", "final_url": "https://mitsloan.mit.edu/ideas-made-to-matter/machine-learning-explained", "retrieved_at": "2026-01-22T00:14:16.123232Z", "title": "Machine learning, explained | MIT Sloan", "section": "AI Basics", "source_type": "Explainer / Guide", "content_hash": "b87b1219d40ec6de06d4039de42fbd20f8b51b18a51fcabd6fcae1c61b8a7e0a", "chunk_index": 7, "char_start": 9029, "char_end": 10340, "text": "In an artificial neural network, cells, or nodes, are connected, with each cell processing inputs and producing an output that is sent to other neurons. Labeled data moves through the nodes, or cells, with each cell performing a different function. In a neural network trained to identify whether a picture contains a cat or not, the different nodes would assess the information and arrive at an output that indicates whether a picture features a cat.\n\nDeep learning\n\nDeep learning networks are neural networks with many layers. The layered network can process extensive amounts of data and determine the “weight” of each link in the network — for example, in an image recognition system, some layers of the neural network might detect individual features of a face, like eyes, nose, or mouth, while another layer would be able to tell whether those features appear in a way that indicates a face.\n\nLike neural networks, deep learning is modeled on the way the human brain works and powers many machine learning uses, like autonomous vehicles, chatbots, and medical diagnostics.\n\n“The more layers you have, the more potential you have for doing complex things well,” Malone said.\n\nDeep learning requires a great deal of computing power, which raises concerns about its economic and environmental sustainability."}
{"chunk_id": "mitsloan_mit_edu__b85f152879c15db1::c0008", "stable_id": "mitsloan_mit_edu__b85f152879c15db1", "url": "https://mitsloan.mit.edu/ideas-made-to-matter/machine-learning-explained", "final_url": "https://mitsloan.mit.edu/ideas-made-to-matter/machine-learning-explained", "retrieved_at": "2026-01-22T00:14:16.123232Z", "title": "Machine learning, explained | MIT Sloan", "section": "AI Basics", "source_type": "Explainer / Guide", "content_hash": "b87b1219d40ec6de06d4039de42fbd20f8b51b18a51fcabd6fcae1c61b8a7e0a", "chunk_index": 8, "char_start": 10340, "char_end": 11558, "text": "How businesses are using machine learning\n\nMachine learning is the core of some companies’ business models, like in the case of Netflix’s suggestions algorithm or Google’s search engine. Other companies are engaging deeply with machine learning, though it’s not their main business proposition.\n\nOthers are still trying to determine how to use machine learning in a beneficial way. “In my opinion, one of the hardest problems in machine learning is figuring out what problems I can solve with machine learning,” Shulman said. “There’s still a gap in the understanding.”\n\nLeading the AI-Driven Organization\n\nIn person at MIT Sloan\n\nRegister Now\n\nIn a 2018 paper, researchers from the MIT Initiative on the Digital Economy outlined a 21-question rubric to determine whether a task is suitable for machine learning. The researchers found that no occupation will be untouched by machine learning, but no occupation is likely to be completely taken over by it. The way to unleash machine learning success, the researchers found, was to reorganize jobs into discrete tasks, some which can be done by machine learning, and others that require a human.\n\nCompanies are already using machine learning in several ways, including:"}
{"chunk_id": "mitsloan_mit_edu__b85f152879c15db1::c0009", "stable_id": "mitsloan_mit_edu__b85f152879c15db1", "url": "https://mitsloan.mit.edu/ideas-made-to-matter/machine-learning-explained", "final_url": "https://mitsloan.mit.edu/ideas-made-to-matter/machine-learning-explained", "retrieved_at": "2026-01-22T00:14:16.123232Z", "title": "Machine learning, explained | MIT Sloan", "section": "AI Basics", "source_type": "Explainer / Guide", "content_hash": "b87b1219d40ec6de06d4039de42fbd20f8b51b18a51fcabd6fcae1c61b8a7e0a", "chunk_index": 9, "char_start": 11558, "char_end": 12947, "text": "Recommendation algorithms. The recommendation engines behind Netflix and YouTube suggestions, what information appears on your Facebook feed, and product recommendations are fueled by machine learning. “[The algorithms] are trying to learn our preferences,” Madry said. “They want to learn, like on Twitter, what tweets we want them to show us, on Facebook, what ads to display, what posts or liked content to share with us.”\n\nImage analysis and object detection. Machine learning can analyze images for different information, like learning to identify people and tell them apart — though facial recognition algorithms are controversial. Business uses for this vary. Shulman noted that hedge funds famously use machine learning to analyze the number of cars in parking lots, which helps them learn how companies are performing and make good bets.\n\nFraud detection. Machines can analyze patterns, like how someone normally spends or where they normally shop, to identify potentially fraudulent credit card transactions, log-in attempts, or spam emails.\n\nAutomatic helplines or chatbots. Many companies are deploying online chatbots, in which customers or clients don’t speak to humans, but instead interact with a machine. These algorithms use machine learning and natural language processing, with the bots learning from records of past conversations to come up with appropriate responses."}
{"chunk_id": "mitsloan_mit_edu__b85f152879c15db1::c0010", "stable_id": "mitsloan_mit_edu__b85f152879c15db1", "url": "https://mitsloan.mit.edu/ideas-made-to-matter/machine-learning-explained", "final_url": "https://mitsloan.mit.edu/ideas-made-to-matter/machine-learning-explained", "retrieved_at": "2026-01-22T00:14:16.123232Z", "title": "Machine learning, explained | MIT Sloan", "section": "AI Basics", "source_type": "Explainer / Guide", "content_hash": "b87b1219d40ec6de06d4039de42fbd20f8b51b18a51fcabd6fcae1c61b8a7e0a", "chunk_index": 10, "char_start": 12947, "char_end": 14164, "text": "Self-driving cars. Much of the technology behind self-driving cars is based on machine learning, deep learning in particular.\n\nMedical imaging and diagnostics. Machine learning programs can be trained to examine medical images or other information and look for certain markers of illness, like a tool that can predict cancer risk based on a mammogram.\n\nRead report: Artificial Intelligence and the Future of Work\n\nHow machine learning works: promises and challenges\n\nWhile machine learning is fueling technology that can help workers or open new possibilities for businesses, there are several things business leaders should know about machine learning and its limits.\n\nExplainability\n\nOne area of concern is what some experts call explainability, or the ability to be clear about what the machine learning models are doing and how they make decisions. “Understanding why a model does what it does is actually a very difficult question, and you always have to ask yourself that,” Madry said. “You should never treat this as a black box, that just comes as an oracle … yes, you should use it, but then try to get a feeling of what are the rules of thumb that it came up with? And then validate them.”\n\nRelated Articles"}
{"chunk_id": "mitsloan_mit_edu__b85f152879c15db1::c0011", "stable_id": "mitsloan_mit_edu__b85f152879c15db1", "url": "https://mitsloan.mit.edu/ideas-made-to-matter/machine-learning-explained", "final_url": "https://mitsloan.mit.edu/ideas-made-to-matter/machine-learning-explained", "retrieved_at": "2026-01-22T00:14:16.123232Z", "title": "Machine learning, explained | MIT Sloan", "section": "AI Basics", "source_type": "Explainer / Guide", "content_hash": "b87b1219d40ec6de06d4039de42fbd20f8b51b18a51fcabd6fcae1c61b8a7e0a", "chunk_index": 11, "char_start": 14164, "char_end": 15560, "text": "This is especially important because systems can be fooled and undermined, or just fail on certain tasks, even those humans can perform easily. For example, adjusting the metadata in images can confuse computers — with a few adjustments, a machine identifies a picture of a dog as an ostrich.\n\nMadry pointed out another example in which a machine learning algorithm examining X-rays seemed to outperform physicians. But it turned out the algorithm was correlating results with the machines that took the image, not necessarily the image itself. Tuberculosis is more common in developing countries, which tend to have older machines. The machine learning program learned that if the X-ray was taken on an older machine, the patient was more likely to have tuberculosis. It completed the task, but not in the way the programmers intended or would find useful.\n\nThe importance of explaining how a model is working — and its accuracy — can vary depending on how it’s being used, Shulman said. While most well-posed problems can be solved through machine learning, he said, people should assume right now that the models only perform to about 95% of human accuracy. It might be okay with the programmer and the viewer if an algorithm recommending movies is 95% accurate, but that level of accuracy wouldn’t be enough for a self-driving vehicle or a program designed to find serious flaws in machinery."}
{"chunk_id": "mitsloan_mit_edu__b85f152879c15db1::c0012", "stable_id": "mitsloan_mit_edu__b85f152879c15db1", "url": "https://mitsloan.mit.edu/ideas-made-to-matter/machine-learning-explained", "final_url": "https://mitsloan.mit.edu/ideas-made-to-matter/machine-learning-explained", "retrieved_at": "2026-01-22T00:14:16.123232Z", "title": "Machine learning, explained | MIT Sloan", "section": "AI Basics", "source_type": "Explainer / Guide", "content_hash": "b87b1219d40ec6de06d4039de42fbd20f8b51b18a51fcabd6fcae1c61b8a7e0a", "chunk_index": 12, "char_start": 15560, "char_end": 16822, "text": "Bias and unintended outcomes\n\nMachines are trained by humans, and human biases can be incorporated into algorithms — if biased information, or data that reflects existing inequities, is fed to a machine learning program, the program will learn to replicate it and perpetuate forms of discrimination. Chatbots trained on how people converse on Twitter can pick up on offensive and racist language, for example.\n\nIn some cases, machine learning models create or exacerbate social problems. For example, Facebook has used machine learning as a tool to show users ads and content that will interest and engage them — which has led to models showing people extreme content that leads to polarization and the spread of conspiracy theories when people are shown incendiary, partisan, or inaccurate content.\n\nWays to fight against bias in machine learning including carefully vetting training data and putting organizational support behind ethical artificial intelligence efforts, like making sure your organization embraces human-centered AI, the practice of seeking input from people of different backgrounds, experiences, and lifestyles when designing AI systems. Initiatives working on this issue include the Algorithmic Justice League and The Moral Machine project."}
{"chunk_id": "mitsloan_mit_edu__b85f152879c15db1::c0013", "stable_id": "mitsloan_mit_edu__b85f152879c15db1", "url": "https://mitsloan.mit.edu/ideas-made-to-matter/machine-learning-explained", "final_url": "https://mitsloan.mit.edu/ideas-made-to-matter/machine-learning-explained", "retrieved_at": "2026-01-22T00:14:16.123232Z", "title": "Machine learning, explained | MIT Sloan", "section": "AI Basics", "source_type": "Explainer / Guide", "content_hash": "b87b1219d40ec6de06d4039de42fbd20f8b51b18a51fcabd6fcae1c61b8a7e0a", "chunk_index": 13, "char_start": 16822, "char_end": 18062, "text": "Putting machine learning to work\n\nShulman said executives tend to struggle with understanding where machine learning can actually add value to their company. What’s gimmicky for one company is core to another, and businesses should avoid trends and find business use cases that work for them.\n\nThe way machine learning works for Amazon is probably not going to translate at a car company, Shulman said — while Amazon has found success with voice assistants and voice-operated speakers, that doesn’t mean car companies should prioritize adding speakers to cars. More likely, he said, the car company might find a way to use machine learning on the factory line that saves or makes a great deal of money.\n\n“The field is moving so quickly, and that's awesome, but it makes it hard for executives to make decisions about it and to decide how much resourcing to pour into it,” Shulman said.\n\nIt’s also best to avoid looking at machine learning as a solution in search of a problem, Shulman said. Some companies might end up trying to backport machine learning into a business use. Instead of starting with a focus on technology, businesses should start with a focus on a business problem or customer need that could be met with machine learning."}
{"chunk_id": "mitsloan_mit_edu__b85f152879c15db1::c0014", "stable_id": "mitsloan_mit_edu__b85f152879c15db1", "url": "https://mitsloan.mit.edu/ideas-made-to-matter/machine-learning-explained", "final_url": "https://mitsloan.mit.edu/ideas-made-to-matter/machine-learning-explained", "retrieved_at": "2026-01-22T00:14:16.123232Z", "title": "Machine learning, explained | MIT Sloan", "section": "AI Basics", "source_type": "Explainer / Guide", "content_hash": "b87b1219d40ec6de06d4039de42fbd20f8b51b18a51fcabd6fcae1c61b8a7e0a", "chunk_index": 14, "char_start": 18062, "char_end": 19013, "text": "A basic understanding of machine learning is important, LaRovere said, but finding the right machine learning use ultimately rests on people with different expertise working together. “I'm not a data scientist. I'm not doing the actual data engineering work — all the data acquisition, processing, and wrangling to enable machine learning applications — but I understand it well enough to be able to work with those teams to get the answers we need and have the impact we need,” she said. “You really have to work in a team.”\n\nLearn more:\n\nSign-up for a Machine Learning in Business Course.\n\nWatch an Introduction to Machine Learning through MIT OpenCourseWare.\n\nRead about how an AI pioneer thinks companies can use machine learning to transform.\n\nWatch a discussion with two AI experts about machine learning strides and limitations.\n\nTake a look at the seven steps of machine learning.\n\nRead next: 7 lessons for successful machine learning projects"}
{"chunk_id": "www_ibm_com__4c73255b7dd33ab9::c0000", "stable_id": "www_ibm_com__4c73255b7dd33ab9", "url": "https://www.ibm.com/think/topics/artificial-intelligence", "final_url": "https://www.ibm.com/think/topics/artificial-intelligence", "retrieved_at": "2026-01-22T00:13:57.906642Z", "title": "What Is Artificial Intelligence (AI)? | IBM", "section": "AI Basics", "source_type": "Explainer / Guide", "content_hash": "3489f555e274409f7a2bd3079626e5e8741d5424256896d243022f44309390f9", "chunk_index": 0, "char_start": 0, "char_end": 1326, "text": "Artificial intelligence (AI) is technology that enables computers and machines to simulate human learning, comprehension, problem solving, decision making, creativity and autonomy.\n\nApplications and devices equipped with AI can see and identify objects. They can understand and respond to human language. They can learn from new information and experience. They can make detailed recommendations to users and experts. They can act independently, replacing the need for human intelligence or intervention (a classic example being a self-driving car).\n\nBut in 2024, most AI researchers, practitioners and most AI-related headlines are focused on breakthroughs in generative AI (gen AI), a technology that can create original text, images, video and other content. To fully understand generative AI, it’s important to first understand the technologies on which generative AI tools are built: machine learning (ML) and deep learning.\n\nThink Newsletter\n\nStay up to date on the most important—and intriguing—industry trends on AI, automation, data and beyond with the Think newsletter. See the IBM Privacy Statement.\n\nYour subscription will be delivered in English. You will find an unsubscribe link in every newsletter. You can manage your subscriptions or unsubscribe here. Refer to our IBM Privacy Statement for more information."}
{"chunk_id": "www_ibm_com__4c73255b7dd33ab9::c0001", "stable_id": "www_ibm_com__4c73255b7dd33ab9", "url": "https://www.ibm.com/think/topics/artificial-intelligence", "final_url": "https://www.ibm.com/think/topics/artificial-intelligence", "retrieved_at": "2026-01-22T00:13:57.906642Z", "title": "What Is Artificial Intelligence (AI)? | IBM", "section": "AI Basics", "source_type": "Explainer / Guide", "content_hash": "3489f555e274409f7a2bd3079626e5e8741d5424256896d243022f44309390f9", "chunk_index": 1, "char_start": 1326, "char_end": 2541, "text": "A simple way to think about AI is as a series of nested or derivative concepts that have emerged over more than 70 years:\n\nDirectly underneath AI, we have machine learning, which involves creating models by training an algorithm to make predictions or decisions based on data. It encompasses a broad range of techniques that enable computers to learn from and make inferences based on data without being explicitly programmed for specific tasks.\n\nThere are many types of machine learning techniques or algorithms, including linear regression, logistic regression, decision trees, random forest, support vector machines (SVMs), k-nearest neighbor (KNN), clustering and more. Each of these approaches is suited to different kinds of problems and data.\n\nBut one of the most popular types of machine learning algorithm is called a neural network (or artificial neural network). Neural networks are modeled after the human brain's structure and function. A neural network consists of interconnected layers of nodes (analogous to neurons) that work together to process and analyze complex data. Neural networks are well suited to tasks that involve identifying complex patterns and relationships in large amounts of data."}
{"chunk_id": "www_ibm_com__4c73255b7dd33ab9::c0002", "stable_id": "www_ibm_com__4c73255b7dd33ab9", "url": "https://www.ibm.com/think/topics/artificial-intelligence", "final_url": "https://www.ibm.com/think/topics/artificial-intelligence", "retrieved_at": "2026-01-22T00:13:57.906642Z", "title": "What Is Artificial Intelligence (AI)? | IBM", "section": "AI Basics", "source_type": "Explainer / Guide", "content_hash": "3489f555e274409f7a2bd3079626e5e8741d5424256896d243022f44309390f9", "chunk_index": 2, "char_start": 2541, "char_end": 4006, "text": "The simplest form of machine learning is called supervised learning, which involves the use of labeled data sets to train algorithms to classify data or predict outcomes accurately. In supervised learning, humans pair each training example with an output label. The goal is for the model to learn the mapping between inputs and outputs in the training data, so it can predict the labels of new, unseen data.\n\nDeep learning is a subset of machine learning that uses multilayered neural networks, called deep neural networks, that more closely simulate the complex decision-making power of the human brain.\n\nDeep neural networks include an input layer, at least three but usually hundreds of hidden layers, and an output layer, unlike neural networks used in classic machine learning models, which usually have only one or two hidden layers.\n\nThese multiple layers enable unsupervised learning: they can automate the extraction of features from large, unlabeled and unstructured data sets, and make their own predictions about what the data represents.\n\nBecause deep learning doesn’t require human intervention, it enables machine learning at a tremendous scale. It is well suited to natural language processing (NLP), computer vision, and other tasks that involve the fast, accurate identification complex patterns and relationships in large amounts of data. Some form of deep learning powers most of the artificial intelligence (AI) applications in our lives today."}
{"chunk_id": "www_ibm_com__4c73255b7dd33ab9::c0003", "stable_id": "www_ibm_com__4c73255b7dd33ab9", "url": "https://www.ibm.com/think/topics/artificial-intelligence", "final_url": "https://www.ibm.com/think/topics/artificial-intelligence", "retrieved_at": "2026-01-22T00:13:57.906642Z", "title": "What Is Artificial Intelligence (AI)? | IBM", "section": "AI Basics", "source_type": "Explainer / Guide", "content_hash": "3489f555e274409f7a2bd3079626e5e8741d5424256896d243022f44309390f9", "chunk_index": 3, "char_start": 4006, "char_end": 5227, "text": "Deep learning also enables:\n\nGenerative AI, sometimes called \"gen AI\", refers to deep learning models that can create complex original content such as long-form text, high-quality images, realistic video or audio and more in response to a user’s prompt or request.\n\nAt a high level, generative models encode a simplified representation of their training data, and then draw from that representation to create new work that’s similar, but not identical, to the original data.\n\nGenerative models have been used for years in statistics to analyze numerical data. But over the last decade, they evolved to analyze and generate more complex data types. This evolution coincided with the emergence of three sophisticated deep learning model types:\n\nIn general, generative AI operates in three phases:\n\nGenerative AI begins with a \"foundation model\"; a deep learning model that serves as the basis for multiple different types of generative AI applications.\n\nThe most common foundation models today are large language models (LLMs), created for text generation applications. But there are also foundation models for image, video, sound or music generation, and multimodal foundation models that support several kinds of content."}
{"chunk_id": "www_ibm_com__4c73255b7dd33ab9::c0004", "stable_id": "www_ibm_com__4c73255b7dd33ab9", "url": "https://www.ibm.com/think/topics/artificial-intelligence", "final_url": "https://www.ibm.com/think/topics/artificial-intelligence", "retrieved_at": "2026-01-22T00:13:57.906642Z", "title": "What Is Artificial Intelligence (AI)? | IBM", "section": "AI Basics", "source_type": "Explainer / Guide", "content_hash": "3489f555e274409f7a2bd3079626e5e8741d5424256896d243022f44309390f9", "chunk_index": 4, "char_start": 5227, "char_end": 6674, "text": "To create a foundation model, practitioners train a deep learning algorithm on huge volumes of relevant raw, unstructured, unlabeled data, such as terabytes or petabytes of data text or images or video from the internet. The training yields a neural network of billions of parameters encoded representations of the entities, patterns and relationships in the data that can generate content autonomously in response to prompts. This is the foundation model.\n\nThis training process is compute-intensive, time-consuming and expensive. It requires thousands of clustered graphics processing units (GPUs) and weeks of processing, all of which typically costs millions of dollars. Open source foundation model projects, such as Meta's Llama-2, enable gen AI developers to avoid this step and its costs.\n\nNext, the model must be tuned to a specific content generation task. This can be done in various ways, including:\n\nDevelopers and users regularly assess the outputs of their generative AI apps, and further tune the model even as often as once a week for greater accuracy or relevance. In contrast, the foundation model itself is updated much less frequently, perhaps every year or 18 months.\n\nAnother option for improving a gen AI app's performance is retrieval augmented generation (RAG), a technique for extending the foundation model to use relevant sources outside of the training data to refine the parameters for greater accuracy or relevance."}
{"chunk_id": "www_ibm_com__4c73255b7dd33ab9::c0005", "stable_id": "www_ibm_com__4c73255b7dd33ab9", "url": "https://www.ibm.com/think/topics/artificial-intelligence", "final_url": "https://www.ibm.com/think/topics/artificial-intelligence", "retrieved_at": "2026-01-22T00:13:57.906642Z", "title": "What Is Artificial Intelligence (AI)? | IBM", "section": "AI Basics", "source_type": "Explainer / Guide", "content_hash": "3489f555e274409f7a2bd3079626e5e8741d5424256896d243022f44309390f9", "chunk_index": 5, "char_start": 6674, "char_end": 7994, "text": "An AI agent is an autonomous AI program, it can perform tasks and accomplish goals on behalf of a user or another system without human intervention, by designing its own workflow and using available tools (other applications or services).\n\nAgentic AI is a system of multiple AI agents, the efforts of which are coordinated, or orchestrated, to accomplish a more complex task or a greater goal than any single agent in the system could accomplish.\n\nUnlike chatbots and other AI models which operate within predefined constraints and require human intervention, AI agents and agentic AI exhibit autonomy, goal-driven behavior and adaptability to changing circumstances. The terms “agent” and “agentic” refer to these models’ agency, or their capacity to act independently and purposefully.\n\nOne way to think of agents is as a natural next step after generative AI. Gen AI models focus on creating content based on learned patterns; agents use that content to interact with each other and other tools to make decisions, solve problems and complete tasks. For example, a gen AI app might be able to tell you the best time to climb Mt. Everest given your work schedule, but an agent can tell you this, and then use an online travel service to book you the best flight and reserve a room in the most convenient hotel in Nepal."}
{"chunk_id": "www_ibm_com__4c73255b7dd33ab9::c0006", "stable_id": "www_ibm_com__4c73255b7dd33ab9", "url": "https://www.ibm.com/think/topics/artificial-intelligence", "final_url": "https://www.ibm.com/think/topics/artificial-intelligence", "retrieved_at": "2026-01-22T00:13:57.906642Z", "title": "What Is Artificial Intelligence (AI)? | IBM", "section": "AI Basics", "source_type": "Explainer / Guide", "content_hash": "3489f555e274409f7a2bd3079626e5e8741d5424256896d243022f44309390f9", "chunk_index": 6, "char_start": 7994, "char_end": 9217, "text": "AI offers numerous benefits across various industries and applications. Some of the most commonly cited benefits include:\n\nAI can automate routine, repetitive and often tedious tasks including digital tasks such as data collection, entering and preprocessing, and physical tasks such as warehouse stock-picking and manufacturing processes. This automation frees to work on higher value, more creative work.\n\nWhether used for decision support or for fully automated decision-making, AI enables faster, more accurate predictions and reliable, data-driven decisions. Combined with automation, AI enables businesses to act on opportunities and respond to crises as they emerge, in real time and without human intervention.\n\nAI can reduce human errors in various ways, from guiding people through the proper steps of a process, to flagging potential errors before they occur, and fully automating processes without human intervention. This is especially important in industries such as healthcare where, for example, AI-guided surgical robotics enable consistent precision.\n\nMachine learning algorithms can continually improve their accuracy and further reduce errors as they're exposed to more data and \"learn\" from experience."}
{"chunk_id": "www_ibm_com__4c73255b7dd33ab9::c0007", "stable_id": "www_ibm_com__4c73255b7dd33ab9", "url": "https://www.ibm.com/think/topics/artificial-intelligence", "final_url": "https://www.ibm.com/think/topics/artificial-intelligence", "retrieved_at": "2026-01-22T00:13:57.906642Z", "title": "What Is Artificial Intelligence (AI)? | IBM", "section": "AI Basics", "source_type": "Explainer / Guide", "content_hash": "3489f555e274409f7a2bd3079626e5e8741d5424256896d243022f44309390f9", "chunk_index": 7, "char_start": 9217, "char_end": 10418, "text": "AI is always on, available around the clock, and delivers consistent performance every time. Tools such as AI chatbots or virtual assistants can lighten staffing demands for customer service or support. In other applications such as materials processing or production lines, AI can help maintain consistent work quality and output levels when used to complete repetitive or tedious tasks.\n\nBy automating dangerous work such as animal control, handling explosives, performing tasks in deep ocean water, high altitudes or in outer space, AI can eliminate the need to put human workers at risk of injury or worse. While they have yet to be perfected, self-driving cars and other vehicles offer the potential to reduce the risk of injury to passengers.\n\nThe real-world applications of AI are many. Here is just a small sampling of use cases across various industries to illustrate its potential:\n\nCompanies can implement AI-powered chatbots and virtual assistants to handle customer inquiries, support tickets and more. These tools use natural language processing (NLP) and generative AI capabilities to understand and respond to customer questions about order status, product details and return policies."}
{"chunk_id": "www_ibm_com__4c73255b7dd33ab9::c0008", "stable_id": "www_ibm_com__4c73255b7dd33ab9", "url": "https://www.ibm.com/think/topics/artificial-intelligence", "final_url": "https://www.ibm.com/think/topics/artificial-intelligence", "retrieved_at": "2026-01-22T00:13:57.906642Z", "title": "What Is Artificial Intelligence (AI)? | IBM", "section": "AI Basics", "source_type": "Explainer / Guide", "content_hash": "3489f555e274409f7a2bd3079626e5e8741d5424256896d243022f44309390f9", "chunk_index": 8, "char_start": 10418, "char_end": 11856, "text": "Chatbots and virtual assistants enable always-on support, provide faster answers to frequently asked questions (FAQs), free human agents to focus on higher-level tasks, and give customers faster, more consistent service.\n\nMachine learning and deep learning algorithms can analyze transaction patterns and flag anomalies, such as unusual spending or login locations, that indicate fraudulent transactions. This enables organizations to respond more quickly to potential fraud and limit its impact, giving themselves and customers greater peace of mind.\n\nRetailers, banks and other customer-facing companies can use AI to create personalized customer experiences and marketing campaigns that delight customers, improve sales and prevent churn. Based on data from customer purchase history and behaviors, deep learning algorithms can recommend products and services customers are likely to want, and even generate personalized copy and special offers for individual customers in real time.\n\nAI-driven recruitment platforms can streamline hiring by screening resumes, matching candidates with job descriptions, and even conducting preliminary interviews using video analysis. These and other tools can dramatically reduce the mountain of administrative paperwork associated with fielding a large volume of candidates. It can also reduce response times and time-to-hire, improving the experience for candidates whether they get the job or not."}
{"chunk_id": "www_ibm_com__4c73255b7dd33ab9::c0009", "stable_id": "www_ibm_com__4c73255b7dd33ab9", "url": "https://www.ibm.com/think/topics/artificial-intelligence", "final_url": "https://www.ibm.com/think/topics/artificial-intelligence", "retrieved_at": "2026-01-22T00:13:57.906642Z", "title": "What Is Artificial Intelligence (AI)? | IBM", "section": "AI Basics", "source_type": "Explainer / Guide", "content_hash": "3489f555e274409f7a2bd3079626e5e8741d5424256896d243022f44309390f9", "chunk_index": 9, "char_start": 11856, "char_end": 13131, "text": "Generative AI code generation tools and automation tools can streamline repetitive coding tasks associated with application development, and accelerate the migration and modernization (reformatting and replatorming) of legacy applications at scale. These tools can speed up tasks, help ensure code consistency and reduce errors.\n\nMachine learning models can analyze data from sensors, Internet of Things (IoT) devices and operational technology (OT) to forecast when maintenance will be required and predict equipment failures before they occur. AI-powered preventive maintenance helps prevent downtime and enables you to stay ahead of supply chain issues before they affect the bottom line.\n\nOrganizations are scrambling to take advantage of the latest AI technologies and capitalize on AI's many benefits. This rapid adoption is necessary, but adopting and maintaining AI workflows comes with challenges and risks.\n\nAI systems rely on data sets that might be vulnerable to data poisoning, data tampering, data bias or cyberattacks that can lead to data breaches. Organizations can mitigate these risks by protecting data integrity and implementing security and availability throughout the entire AI lifecycle, from development to training and deployment and postdeployment."}
{"chunk_id": "www_ibm_com__4c73255b7dd33ab9::c0010", "stable_id": "www_ibm_com__4c73255b7dd33ab9", "url": "https://www.ibm.com/think/topics/artificial-intelligence", "final_url": "https://www.ibm.com/think/topics/artificial-intelligence", "retrieved_at": "2026-01-22T00:13:57.906642Z", "title": "What Is Artificial Intelligence (AI)? | IBM", "section": "AI Basics", "source_type": "Explainer / Guide", "content_hash": "3489f555e274409f7a2bd3079626e5e8741d5424256896d243022f44309390f9", "chunk_index": 10, "char_start": 13131, "char_end": 14628, "text": "Threat actors can target AI models for theft, reverse engineering or unauthorized manipulation. Attackers might compromise a model’s integrity by tampering with its architecture, weights or parameters; the core components that determine a model’s behavior, accuracy and performance.\n\nLike all technologies, models are susceptible to operational risks such as model drift, bias and breakdowns in the governance structure. Left unaddressed, these risks can lead to system failures and cybersecurity vulnerabilities that threat actors can use.\n\nIf organizations don’t prioritize safety and ethics when developing and deploying AI systems, they risk committing privacy violations and producing biased outcomes. For example, biased training data used for hiring decisions might reinforce gender or racial stereotypes and create AI models that favor certain demographic groups over others.\n\nAI ethics is a multidisciplinary field that studies how to optimize AI's beneficial impact while reducing risks and adverse outcomes. Principles of AI ethics are applied through a system of AI governance consisted of guardrails that help ensure that AI tools and systems remain safe and ethical.\n\nAI governance encompasses oversight mechanisms that address risks. An ethical approach to AI governance requires the involvement of a wide range of stakeholders, including developers, users, policymakers and ethicists, helping to ensure that AI-related systems are developed and used to align with society's values."}
{"chunk_id": "www_ibm_com__4c73255b7dd33ab9::c0011", "stable_id": "www_ibm_com__4c73255b7dd33ab9", "url": "https://www.ibm.com/think/topics/artificial-intelligence", "final_url": "https://www.ibm.com/think/topics/artificial-intelligence", "retrieved_at": "2026-01-22T00:13:57.906642Z", "title": "What Is Artificial Intelligence (AI)? | IBM", "section": "AI Basics", "source_type": "Explainer / Guide", "content_hash": "3489f555e274409f7a2bd3079626e5e8741d5424256896d243022f44309390f9", "chunk_index": 11, "char_start": 14628, "char_end": 15881, "text": "Here are common values associated with AI ethics and responsible AI:\n\nAs AI becomes more advanced, humans are challenged to comprehend and retrace how the algorithm came to a result. Explainable AI is a set of processes and methods that enables human users to interpret, comprehend and trust the results and output created by algorithms.\n\nAlthough machine learning, by its very nature, is a form of statistical discrimination, the discrimination becomes objectionable when it places privileged groups at systematic advantage and certain unprivileged groups at systematic disadvantage, potentially causing varied harms. To encourage fairness, practitioners can try to minimize algorithmic bias across data collection and model design, and to build more diverse and inclusive teams.\n\nRobust AI effectively handles exceptional conditions, such as abnormalities in input or malicious attacks, without causing unintentional harm. It is also built to withstand intentional and unintentional interference by protecting against exposed vulnerabilities.\n\nOrganizations should implement clear responsibilities and governance\n\nstructures for the development, deployment and outcomes of AI systems.\n\nIn addition, users should be able to see how an AI service works,"}
{"chunk_id": "www_ibm_com__4c73255b7dd33ab9::c0012", "stable_id": "www_ibm_com__4c73255b7dd33ab9", "url": "https://www.ibm.com/think/topics/artificial-intelligence", "final_url": "https://www.ibm.com/think/topics/artificial-intelligence", "retrieved_at": "2026-01-22T00:13:57.906642Z", "title": "What Is Artificial Intelligence (AI)? | IBM", "section": "AI Basics", "source_type": "Explainer / Guide", "content_hash": "3489f555e274409f7a2bd3079626e5e8741d5424256896d243022f44309390f9", "chunk_index": 12, "char_start": 15881, "char_end": 17467, "text": "evaluate its functionality, and comprehend its strengths and\n\nlimitations. Increased transparency provides information for AI\n\nconsumers to better understand how the AI model or service was created.\n\nMany regulatory frameworks, including GDPR, mandate that organizations abide by certain privacy principles when processing personal information. It is crucial to be able to protect AI models that might contain personal information, control what data goes into the model in the first place, and to build adaptable systems that can adjust to changes in regulation and attitudes around AI ethics.\n\nIn order to contextualize the use of AI at various levels of complexity and sophistication, researchers have defined several types of AI that refer to its level of sophistication:\n\nWeak AI: Also known as “narrow AI,” defines AI systems designed to perform a specific task or a set of tasks. Examples might include “smart” voice assistant apps, such as Amazon’s Alexa, Apple’s Siri, a social media chatbot or the autonomous vehicles promised by Tesla.\n\nStrong AI: Also known as “artificial general intelligence” (AGI) or “general AI,” possess the ability to understand, learn and apply knowledge across a wide range of tasks at a level equal to or surpassing human intelligence. This level of AI is currently theoretical and no known AI systems approach this level of sophistication. Researchers argue that if AGI is even possible, it requires major increases in computing power. Despite recent advances in AI development, self-aware AI systems of science fiction remain firmly in that realm."}
{"chunk_id": "www_ibm_com__4c73255b7dd33ab9::c0013", "stable_id": "www_ibm_com__4c73255b7dd33ab9", "url": "https://www.ibm.com/think/topics/artificial-intelligence", "final_url": "https://www.ibm.com/think/topics/artificial-intelligence", "retrieved_at": "2026-01-22T00:13:57.906642Z", "title": "What Is Artificial Intelligence (AI)? | IBM", "section": "AI Basics", "source_type": "Explainer / Guide", "content_hash": "3489f555e274409f7a2bd3079626e5e8741d5424256896d243022f44309390f9", "chunk_index": 13, "char_start": 17467, "char_end": 19003, "text": "The idea of \"a machine that thinks\" dates back to ancient Greece. But since the advent of electronic computing (and relative to some of the topics discussed in this article) important events and milestones in the evolution of AI include the following:\n\n1950\n\nAlan Turing publishes Computing Machinery and Intelligence. In this paper, Turing famous for breaking the German ENIGMA code during WWII and often referred to as the \"father of computer science\" asks the following question: \"Can machines think?\"\n\nFrom there, he offers a test, now famously known as the \"Turing Test,\" where a human interrogator would try to distinguish between a computer and human text response. While this test has undergone much scrutiny since it was published, it remains an important part of the history of AI, and an ongoing concept within philosophy as it uses ideas around linguistics.\n\n1956\n\nJohn McCarthy coins the term \"artificial intelligence\" at the first-ever AI conference at Dartmouth College. (McCarthy went on to invent the Lisp language.) Later that year, Allen Newell, J.C. Shaw and Herbert Simon create the Logic Theorist, the first-ever running AI computer program.\n\n1967\n\nFrank Rosenblatt builds the Mark 1 Perceptron, the first computer based on a neural network that \"learned\" through trial and error. Just a year later, Marvin Minsky and Seymour Papert publish a book titled Perceptrons, which becomes both the landmark work on neural networks and, at least for a while, an argument against future neural network research initiatives."}
{"chunk_id": "www_ibm_com__4c73255b7dd33ab9::c0014", "stable_id": "www_ibm_com__4c73255b7dd33ab9", "url": "https://www.ibm.com/think/topics/artificial-intelligence", "final_url": "https://www.ibm.com/think/topics/artificial-intelligence", "retrieved_at": "2026-01-22T00:13:57.906642Z", "title": "What Is Artificial Intelligence (AI)? | IBM", "section": "AI Basics", "source_type": "Explainer / Guide", "content_hash": "3489f555e274409f7a2bd3079626e5e8741d5424256896d243022f44309390f9", "chunk_index": 14, "char_start": 19003, "char_end": 20509, "text": "1980\n\nNeural networks, which use a backpropagation algorithm to train itself, became widely used in AI applications.\n\n1995\n\nStuart Russell and Peter Norvig publish Artificial Intelligence: A Modern Approach, which becomes one of the leading textbooks in the study of AI. In it, they delve into four potential goals or definitions of AI, which differentiates computer systems based on rationality and thinking versus acting.\n\n1997\n\nIBM's Deep Blue beats then world chess champion Garry Kasparov, in a chess match (and rematch).\n\n2004\n\nJohn McCarthy writes a paper, What Is Artificial Intelligence?, and proposes an often-cited definition of AI. By this time, the era of big data and cloud computing is underway, enabling organizations to manage ever-larger data estates, which will one day be used to train AI models.\n\n2011\n\nIBM Watson® beats champions Ken Jennings and Brad Rutter at Jeopardy! Also, around this time, data science begins to emerge as a popular discipline.\n\n2015\n\nBaidu's Minwa supercomputer uses a special deep neural network called a convolutional neural network to identify and categorize images with a higher rate of accuracy than the average human.\n\n2016\n\nDeepMind's AlphaGo program, powered by a deep neural network, beats Lee Sodol, the world champion Go player, in a five-game match. The victory is significant given the huge number of possible moves as the game progresses (over 14.5 trillion after just four moves). Later, Google purchased DeepMind for a reported USD 400 million."}
{"chunk_id": "www_ibm_com__4c73255b7dd33ab9::c0015", "stable_id": "www_ibm_com__4c73255b7dd33ab9", "url": "https://www.ibm.com/think/topics/artificial-intelligence", "final_url": "https://www.ibm.com/think/topics/artificial-intelligence", "retrieved_at": "2026-01-22T00:13:57.906642Z", "title": "What Is Artificial Intelligence (AI)? | IBM", "section": "AI Basics", "source_type": "Explainer / Guide", "content_hash": "3489f555e274409f7a2bd3079626e5e8741d5424256896d243022f44309390f9", "chunk_index": 15, "char_start": 20509, "char_end": 21180, "text": "2022\n\nA rise in large language models or LLMs, such as OpenAI’s ChatGPT, creates an enormous change in performance of AI and its potential to drive enterprise value. With these new generative AI practices, deep-learning models can be pretrained on large amounts of data.\n\n2024\n\nThe latest AI trends point to a continuing AI renaissance. Multimodal models that can take multiple types of data as input are providing richer, more robust experiences. These models bring together computer vision image recognition and NLP speech recognition capabilities. Smaller models are also making strides in an age of diminishing returns with massive models with large parameter counts."}
{"chunk_id": "ischool_syracuse_edu__34a1e846df86ce05::c0000", "stable_id": "ischool_syracuse_edu__34a1e846df86ce05", "url": "https://ischool.syracuse.edu/what-is-machine-learning/", "final_url": "https://ischool.syracuse.edu/what-is-machine-learning/", "retrieved_at": "2026-01-22T00:14:19.236972Z", "title": "What Is Machine Learning? Key Concepts and Real-World Uses", "section": "AI Basics", "source_type": "Explainer / Guide", "content_hash": "276553d3bd3a6e61bf3217c6bd03e7cd55dbad82881ec28ab18b27ae9943756b", "chunk_index": 0, "char_start": 0, "char_end": 1382, "text": "Key Takeaways\n\n- Machine learning is a subset of AI, while deep learning is a specialized branch of machine learning.\n\n- The main types of machine learning are supervised, unsupervised, and reinforcement learning.\n\n- Machine learning follows a structured process, starting with data collection and preprocessing, then model selection and training, followed by testing and evaluation to ensure accurate pattern recognition and predictions.\n\nWe are surrounded by machine learning-based technology—search engines somehow know just what we’re looking for, email filters keep our inboxes clean, cameras adjust to capture faces in perfect focus, and fraud detection systems flag suspicious transactions before we even realize something’s wrong.\n\nMachine learning makes it possible for technology to adapt, predict, and continuously improve without the need for human intervention at each step. But what is machine learning exactly, and how does it work? The answers are necessary to decide if this field is the right fit for you.\n\nWhat Is Machine Learning?\n\nMachine learning refers to the process by which computers are able to recognize patterns and improve their performance over time without needing to be programmed for every possible scenario. Instead of following a rigid set of rules, these systems analyze data, make predictions, and adjust their approach based on their learning."}
{"chunk_id": "ischool_syracuse_edu__34a1e846df86ce05::c0001", "stable_id": "ischool_syracuse_edu__34a1e846df86ce05", "url": "https://ischool.syracuse.edu/what-is-machine-learning/", "final_url": "https://ischool.syracuse.edu/what-is-machine-learning/", "retrieved_at": "2026-01-22T00:14:19.236972Z", "title": "What Is Machine Learning? Key Concepts and Real-World Uses", "section": "AI Basics", "source_type": "Explainer / Guide", "content_hash": "276553d3bd3a6e61bf3217c6bd03e7cd55dbad82881ec28ab18b27ae9943756b", "chunk_index": 1, "char_start": 1382, "char_end": 2683, "text": "This adapting ability makes machine learning one of the most powerful tools in modern technology. Thanks to it, computers can perform tasks that once required human intuition—like identifying objects in images, understanding spoken language, or detecting fraudulent transactions.\n\nThough the concept might seem like a product of modern advancements, the idea has been around for decades. In 1959, Arthur Samuel, one of the pioneers in computer science, defined it as “the field of study that gives computers the ability to learn without being explicitly programmed.” This means that instead of relying on fixed rules, machine learning systems develop their own insights by analyzing vast amounts of data and adjusting accordingly.\n\nThis idea was later reinforced by Herbert Simon, considered a founding father of artificial intelligence, who explained that machine learning is fundamentally about improving performance through experience—just as humans get better at tasks through practice.\n\nMachine Learning Versus Other Similar Fields\n\nMachine learning is intertwined with many other fields that deal with data, computing, and intelligent decision-making. It shares strong connections with artificial intelligence, data science, deep learning, robotics, natural language processing, and many others."}
{"chunk_id": "ischool_syracuse_edu__34a1e846df86ce05::c0002", "stable_id": "ischool_syracuse_edu__34a1e846df86ce05", "url": "https://ischool.syracuse.edu/what-is-machine-learning/", "final_url": "https://ischool.syracuse.edu/what-is-machine-learning/", "retrieved_at": "2026-01-22T00:14:19.236972Z", "title": "What Is Machine Learning? Key Concepts and Real-World Uses", "section": "AI Basics", "source_type": "Explainer / Guide", "content_hash": "276553d3bd3a6e61bf3217c6bd03e7cd55dbad82881ec28ab18b27ae9943756b", "chunk_index": 2, "char_start": 2683, "char_end": 4059, "text": "Machine learning vs. deep learning\n\nDeep learning is a branch of machine learning that focuses on the use of layered neural networks—often called deep neural networks—to process data in sophisticated ways. While both aim to teach machines to recognize patterns and improve performance, deep learning is a more specialized and advanced version.\n\nIn traditional machine learning, humans still need to tell the computer what features to focus on. For example, if you’re training a model to recognize cats in pictures, you might have to manually tell it to look at specific features like the shape of the ears.\n\nDeep learning removes this manual step using neural networks, a type of computer system designed to work similarly to the human brain. These networks have multiple layers, allowing them to automatically find and refine features on their own.\n\nHowever, deep learning needs a lot more data and computing power to work well, unlike traditional machine learning, which can work with smaller datasets.\n\nMachine learning vs. artificial intelligence\n\nMachine learning is part of artificial intelligence (AI), as the latter is a much broader concept. AI is all about creating systems that can simulate human-like thinking and problem-solving through logic-based programming, expert systems, or machine learning techniques. Machine learning is one of the ways AI achieves this."}
{"chunk_id": "ischool_syracuse_edu__34a1e846df86ce05::c0003", "stable_id": "ischool_syracuse_edu__34a1e846df86ce05", "url": "https://ischool.syracuse.edu/what-is-machine-learning/", "final_url": "https://ischool.syracuse.edu/what-is-machine-learning/", "retrieved_at": "2026-01-22T00:14:19.236972Z", "title": "What Is Machine Learning? Key Concepts and Real-World Uses", "section": "AI Basics", "source_type": "Explainer / Guide", "content_hash": "276553d3bd3a6e61bf3217c6bd03e7cd55dbad82881ec28ab18b27ae9943756b", "chunk_index": 3, "char_start": 4059, "char_end": 5274, "text": "Data science relates to both AI and machine learning by providing the structured data and analytical techniques that fuel them. It prepares the data that machine learning learns from. Then, AI uses those machine learning models to automate and make decisions.\n\nTypes of Machine Learning\n\nNot all machine learning models work the same way—different approaches exist since there are different problems to deal with. The top three types of learning include:\n\nSupervised learning\n\nSupervised learning works like learning with a tutor who provides the correct answers. The system is trained on data that comes with labels, meaning the correct outcome is already known. By recognizing patterns in labeled data, the model learns to make predictions on new data.\n\nFor example, an email filter can be trained to detect spam by being provided with thousands of emails labeled as either spam or not spam. By analyzing these labeled examples, the model learns which words, phrases, or senders are commonly associated with spam and applies this knowledge to filter incoming messages. This method is widely used in speech recognition, medical diagnosis, fraud detection, and product recommendation systems.\n\nUnsupervised learning"}
{"chunk_id": "ischool_syracuse_edu__34a1e846df86ce05::c0004", "stable_id": "ischool_syracuse_edu__34a1e846df86ce05", "url": "https://ischool.syracuse.edu/what-is-machine-learning/", "final_url": "https://ischool.syracuse.edu/what-is-machine-learning/", "retrieved_at": "2026-01-22T00:14:19.236972Z", "title": "What Is Machine Learning? Key Concepts and Real-World Uses", "section": "AI Basics", "source_type": "Explainer / Guide", "content_hash": "276553d3bd3a6e61bf3217c6bd03e7cd55dbad82881ec28ab18b27ae9943756b", "chunk_index": 4, "char_start": 5274, "char_end": 6624, "text": "Unsupervised learning takes a different approach—it works without labeled data, meaning the system must identify patterns and relationships on its own. Instead of being told what to look for, it processes large amounts of data and organizes it based on similarities or differences.\n\nOne example is anomaly detection in emails. Without prior labels, the system analyzes thousands of “normal” emails and learns what a typical email looks like. When a new email arrives that doesn’t fit the usual pattern—perhaps containing unusual wording, suspicious links, or an unfamiliar sender—it flags it as potentially fraudulent. Unsupervised learning is often used for fraud detection, customer segmentation, and content recommendations, where patterns emerge naturally from the data.\n\nIn certain cases, there can also be semi-supervised learning, which combines aspects of both supervised and unsupervised learning—the model first learns from the small labeled dataset and then improves its accuracy by identifying patterns in the much larger unlabeled dataset.\n\nReinforcement learning\n\nThis type of learning is based on trial and error. Instead of learning from a fixed dataset, the system interacts with its environment, makes decisions, and receives feedback through rewards or penalties. Over time, it refines its strategies to maximize positive outcomes."}
{"chunk_id": "ischool_syracuse_edu__34a1e846df86ce05::c0005", "stable_id": "ischool_syracuse_edu__34a1e846df86ce05", "url": "https://ischool.syracuse.edu/what-is-machine-learning/", "final_url": "https://ischool.syracuse.edu/what-is-machine-learning/", "retrieved_at": "2026-01-22T00:14:19.236972Z", "title": "What Is Machine Learning? Key Concepts and Real-World Uses", "section": "AI Basics", "source_type": "Explainer / Guide", "content_hash": "276553d3bd3a6e61bf3217c6bd03e7cd55dbad82881ec28ab18b27ae9943756b", "chunk_index": 5, "char_start": 6624, "char_end": 7887, "text": "A self-driving car is a good example. It does not have a predefined set of instructions for every possible situation it may encounter on the road. Instead, it learns by trying different actions, such as accelerating, braking, or turning, and observing the results. When an action brings it closer to safe and efficient driving, it is reinforced as a good choice. Reinforcement learning is widely used in robotics, stock market predictions, and optimizing logistics.\n\nHow Does Machine Learning Work?\n\nWhile the specific approaches taken can vary, most machine learning models follow a similar workflow that starts with gathering data and ends with algorithms that can easily recognize patterns and make predictions as needed.\n\nIt all begins with data collection, where large amounts of information are gathered. The data can be collected from various sources, such as online transactions, customer interactions, sensor readings, medical records, and more.\n\nOnce the data is collected, the data undergoes preprocessing. This step guarantees the information passed to the next stage is clean and structured by eliminating duplicate entries, filling in missing values, standardizing numerical data, and converting categorical variables into a machine-readable format."}
{"chunk_id": "ischool_syracuse_edu__34a1e846df86ce05::c0006", "stable_id": "ischool_syracuse_edu__34a1e846df86ce05", "url": "https://ischool.syracuse.edu/what-is-machine-learning/", "final_url": "https://ischool.syracuse.edu/what-is-machine-learning/", "retrieved_at": "2026-01-22T00:14:19.236972Z", "title": "What Is Machine Learning? Key Concepts and Real-World Uses", "section": "AI Basics", "source_type": "Explainer / Guide", "content_hash": "276553d3bd3a6e61bf3217c6bd03e7cd55dbad82881ec28ab18b27ae9943756b", "chunk_index": 6, "char_start": 7887, "char_end": 9280, "text": "With clean and structured data in hand, model selection and training begins. As stated, the choice of model depends on the specific task, as different algorithms specialize in different types of problems. Training the model involves feeding it data and adjusting its internal parameters so that it learns to make accurate predictions. The more relevant examples it is given, the better it gets at identifying patterns and making decisions.\n\nHowever, even if a model performs well during training, that doesn’t necessarily mean it’s ready to be used in real-world applications. To confirm it can handle unseen data, it must undergo testing and evaluation. Therefore, a separate dataset—one the model hasn’t encountered before—is used to measure how well it responds to new information rather than simply memorizing past examples. Performance is assessed using different metrics depending on the task.\n\nBeyond the model itself, no matter how advanced an algorithm is, without high-quality data and well-crafted features, it won’t deliver useful results.\n\nMachine Learning Algorithms\n\nMachine learning algorithms come in a variety of forms—some are quite straightforward and easy to interpret, while others are more complex and require additional computational resources. The choice between them depends on the problem being solved, the type of data available, and the level of accuracy required."}
{"chunk_id": "ischool_syracuse_edu__34a1e846df86ce05::c0007", "stable_id": "ischool_syracuse_edu__34a1e846df86ce05", "url": "https://ischool.syracuse.edu/what-is-machine-learning/", "final_url": "https://ischool.syracuse.edu/what-is-machine-learning/", "retrieved_at": "2026-01-22T00:14:19.236972Z", "title": "What Is Machine Learning? Key Concepts and Real-World Uses", "section": "AI Basics", "source_type": "Explainer / Guide", "content_hash": "276553d3bd3a6e61bf3217c6bd03e7cd55dbad82881ec28ab18b27ae9943756b", "chunk_index": 7, "char_start": 9280, "char_end": 10622, "text": "Some of the most commonly used machine learning algorithms include:\n\nLinear Regression\n\nLinear regression is one of the most widely used machine learning algorithms for predicting numerical values. It works by finding the best-fitting straight line (or hyperplane in higher dimensions) that describes the relationship between input variables (features) and an output variable.\n\nDecision Trees\n\nDecision trees are intuitive, rule-based models that split data into branches based on yes/no questions, ultimately leading to a decision. The tree starts with a root node that represents the entire dataset, and as it branches out, it makes sequential decisions based on different features.\n\nSupport Vector Machines (SVM)\n\nSupport Vector Machines (SVM) are powerful classification algorithms that work by finding the optimal boundary (or hyperplane) that best separates different categories in a dataset. The goal of an SVM is to maximize the margin between different classes, ensuring that new data points can be classified with high accuracy.\n\nK-Nearest Neighbors (KNN)\n\nK-Nearest Neighbors is a classification and regression algorithm that assigns a label to a new data point based on the majority class of its closest neighbors. It doesn’t explicitly learn from training data but memorizes the dataset and makes predictions based on similarity."}
{"chunk_id": "ischool_syracuse_edu__34a1e846df86ce05::c0008", "stable_id": "ischool_syracuse_edu__34a1e846df86ce05", "url": "https://ischool.syracuse.edu/what-is-machine-learning/", "final_url": "https://ischool.syracuse.edu/what-is-machine-learning/", "retrieved_at": "2026-01-22T00:14:19.236972Z", "title": "What Is Machine Learning? Key Concepts and Real-World Uses", "section": "AI Basics", "source_type": "Explainer / Guide", "content_hash": "276553d3bd3a6e61bf3217c6bd03e7cd55dbad82881ec28ab18b27ae9943756b", "chunk_index": 8, "char_start": 10622, "char_end": 11877, "text": "Random Forest\n\nRandom Forest is an ensemble learning method combining the output of multiple decision trees to produce a single result. Instead of relying on only one decision tree, Random Forest trains many trees on random subsets of data and averages their outputs (for regression) or selects the majority vote (for classification).\n\nNaïve Bayes\n\nNaïve Bayes is a probability-based classification algorithm that assumes all features are independent, even though this may not always be the case in real-world scenarios. It applies Bayes’ theorem to calculate the likelihood that a data point belongs to a specific category based on prior knowledge.\n\nNeural Networks\n\nNeural networks, commonly referred to as artificial neural networks, are inspired by the structure of the human brain and consist of layers of interconnected nodes (neurons) that process and transform data. They are particularly powerful in deep learning applications, where large amounts of data need to be analyzed for patterns.\n\nPros and Cons of Machine Learning Algorithms\n\nLike any field that pushes the boundaries of technology, machine learning also comes with both advantages and some challenges. It provides excellent results, but the work to get those isn’t always the easiest."}
{"chunk_id": "ischool_syracuse_edu__34a1e846df86ce05::c0009", "stable_id": "ischool_syracuse_edu__34a1e846df86ce05", "url": "https://ischool.syracuse.edu/what-is-machine-learning/", "final_url": "https://ischool.syracuse.edu/what-is-machine-learning/", "retrieved_at": "2026-01-22T00:14:19.236972Z", "title": "What Is Machine Learning? Key Concepts and Real-World Uses", "section": "AI Basics", "source_type": "Explainer / Guide", "content_hash": "276553d3bd3a6e61bf3217c6bd03e7cd55dbad82881ec28ab18b27ae9943756b", "chunk_index": 9, "char_start": 11877, "char_end": 13195, "text": "Some of its most notable advantages include:\n\n- Scalability and automation, as machine learning models can process large amounts of data at high speeds and handle repetitive tasks without constant human intervention.\n\n- Enhanced decision-making with data-driven insights, allowing organizations to analyze patterns, detect trends, and make more informed choices.\n\n- Potential to solve complex problems, enabling breakthroughs in different fields.\n\nAlongside those, some of the biggest challenges include:\n\n- Data dependency and quality concerns, including any inaccuracies, biases, or missing information. They can impact performance and reliability.\n\n- Overfitting and underfitting, where a model may either become too specialized to its training data and fail to generalize well to new inputs or be too simplistic, missing important patterns and leading to poor predictions.\n\n- Ethical and privacy issues, such as the use of sensitive personal data in machine learning. These raise concerns about bias, fairness, transparency, and potential misuse.\n\nReal-World Machine Learning Applications Across Industries\n\nFrom predicting what you’ll buy next to diagnosing diseases with greater accuracy, machine learning has found use everywhere. Its application has brought significant improvement in the following industries:"}
{"chunk_id": "ischool_syracuse_edu__34a1e846df86ce05::c0010", "stable_id": "ischool_syracuse_edu__34a1e846df86ce05", "url": "https://ischool.syracuse.edu/what-is-machine-learning/", "final_url": "https://ischool.syracuse.edu/what-is-machine-learning/", "retrieved_at": "2026-01-22T00:14:19.236972Z", "title": "What Is Machine Learning? Key Concepts and Real-World Uses", "section": "AI Basics", "source_type": "Explainer / Guide", "content_hash": "276553d3bd3a6e61bf3217c6bd03e7cd55dbad82881ec28ab18b27ae9943756b", "chunk_index": 10, "char_start": 13195, "char_end": 14555, "text": "Healthcare\n\nBy analyzing patient records, genetic data, and medical imaging, machine learning models can detect patterns that may not be apparent to human doctors, leading to earlier identification of conditions like cancer, heart disease, and diabetes.\n\nSuch algorithms also help tailor treatments to each patient. Algorithms that analyze how different people respond to medications can optimize dosages, predict potential side effects, and suggest the most effective treatment plans.\n\nHospitals also often use machine learning for predictive analytics in order to estimate patient admission rates and optimize staff allocation for better care.\n\nFinance\n\nMachine learning is used in security systems to analyze millions of transactions in real time and then flag suspicious activity based on unusual spending behavior.\n\nCredit scoring also benefits from machine learning. Traditional credit evaluation relied on a handful of financial factors, but modern machine learning models assess a wider range of data, including spending habits and transaction history, to determine a borrower’s creditworthiness more accurately.\n\nBanks and investment firms also use machine learning for market analysis and automated trading, where algorithms predict stock trends and execute trades at lightning speed, optimizing investment portfolios with minimal human intervention."}
{"chunk_id": "ischool_syracuse_edu__34a1e846df86ce05::c0011", "stable_id": "ischool_syracuse_edu__34a1e846df86ce05", "url": "https://ischool.syracuse.edu/what-is-machine-learning/", "final_url": "https://ischool.syracuse.edu/what-is-machine-learning/", "retrieved_at": "2026-01-22T00:14:19.236972Z", "title": "What Is Machine Learning? Key Concepts and Real-World Uses", "section": "AI Basics", "source_type": "Explainer / Guide", "content_hash": "276553d3bd3a6e61bf3217c6bd03e7cd55dbad82881ec28ab18b27ae9943756b", "chunk_index": 11, "char_start": 14555, "char_end": 15916, "text": "Retail\n\nE-commerce platforms use machine learning for recommendation systems to analyze browsing history, past purchases, and even how long potential customers linger on a product page to suggest items tailored to their preferences.\n\nIt also improves inventory management by analyzing buying trends, seasonal shifts, and supply chain data so it can predict demand and avoid overordering or running out of inventory.\n\nCustomer service chatbots powered by machine learning have also become a trend. They provide instant assistance to clients without the need for human intervention.\n\nTransportation\n\nSelf-driving cars, a wonder of the 21st century, rely on deep learning models, as a specialized form of machine learning, to process sensor data, recognize road conditions, and make real-time driving decisions. These systems improve with experience, learning from millions of miles driven to navigate safely and efficiently.\n\nAnother key application is predictive maintenance, where machine learning models can analyze vehicle performance data to detect potential mechanical failures before they occur.\n\nMachine Learning: A Tool, a Field, and a Future\n\nMachine learning is everywhere. Its impact only continues to grow, and with it, so does the demand for those who understand how to use its power, refine its capabilities, and push the limits of what’s possible."}
{"chunk_id": "ischool_syracuse_edu__34a1e846df86ce05::c0012", "stable_id": "ischool_syracuse_edu__34a1e846df86ce05", "url": "https://ischool.syracuse.edu/what-is-machine-learning/", "final_url": "https://ischool.syracuse.edu/what-is-machine-learning/", "retrieved_at": "2026-01-22T00:14:19.236972Z", "title": "What Is Machine Learning? Key Concepts and Real-World Uses", "section": "AI Basics", "source_type": "Explainer / Guide", "content_hash": "276553d3bd3a6e61bf3217c6bd03e7cd55dbad82881ec28ab18b27ae9943756b", "chunk_index": 12, "char_start": 15916, "char_end": 17478, "text": "If you’re interested in pursuing a career in this field, Syracuse University’s iSchool provides the perfect starting point. You can begin exploring the fundamentals through our Applied Data Analytics Bachelor’s Degree and the Applied Data Analytics Minor. Alternatively, you can explore our Master’s in Artificial Intelligence or the one in Applied Data Science. All programs are designed to equip you with the knowledge, tools, and hands-on experience that is needed to make an impact in this field of work.\n\nMachine learning may have the ability to adapt and improve on its own, but it still depends on the people who build, train, and guide it. So join us, and you might be the one to achieve the next breakthrough.\n\nFrequently Asked Questions (FAQs)\n\nWhat are the benefits of machine learning?\n\nIts benefits include enhanced efficiency, automated complex tasks, improved decision-making with data-driven insights, personalized experiences, and innovation across industries.\n\nCan I learn machine learning online?\n\nTechnically, yes. However, a structured university education offers something those courses can’t: a well-rounded foundation, hands-on experience, mentorship from experts, and the opportunity to work on real-world projects.\n\nIs a university degree necessary to become a machine learning engineer?\n\nYes, most machine learning engineers hold degrees in artificial intelligence, computer science, or related fields, as these programs provide the necessary technical knowledge, mathematical foundations, and practical training required by employers."}
{"chunk_id": "hai_stanford_edu__da87f2b213eb9272::c0000", "stable_id": "hai_stanford_edu__da87f2b213eb9272", "url": "https://hai.stanford.edu/ai-index/2025-ai-index-report", "final_url": "https://hai.stanford.edu/ai-index/2025-ai-index-report", "retrieved_at": "2026-01-22T00:14:03.745663Z", "title": "The 2025 AI Index Report | Stanford HAI", "section": "AI News That Matters", "source_type": "Research / Academic", "content_hash": "e876b272576015acf7fcba6cce1a086777ef90bec42e390119bcdb311fdabc6b", "chunk_index": 0, "char_start": 0, "char_end": 1329, "text": "Get the latest news, advances in research, policy work, and education program updates from HAI in your inbox weekly.\n\nSign Up For Latest News\n\nWelcome to the seventh edition of the AI Index report. The 2024 Index is our most comprehensive to date and arrives at an important moment when AI’s influence on society has never been more pronounced.\n\nThe AI Index is an independent initiative at the Stanford Institute for Human-Centered Artificial Intelligence (HAI), led by the AI Index Steering Committee, an interdisciplinary group of experts from across academia and industry.\n\nThe AI Index is an independent initiative at the Stanford Institute for Human-Centered Artificial Intelligence (HAI), led by the AI Index Steering Committee, an interdisciplinary group of experts from across academia and industry.\n\nThis year we significantly expanded the amount of data available in the report, worked with a broader set of external organizations to calibrate our data, and deepened our connections with Stanford HAI.\n\nThe AI Index Report tracks, collates, distills, and visualizes data relating to artificial intelligence.\n\nIts mission is to provide unbiased, rigorous, and comprehensive data for policymakers, researchers, journalists, executives, and the general public to develop a deeper understanding of the complex field of AI."}
{"chunk_id": "hai_stanford_edu__da87f2b213eb9272::c0001", "stable_id": "hai_stanford_edu__da87f2b213eb9272", "url": "https://hai.stanford.edu/ai-index/2025-ai-index-report", "final_url": "https://hai.stanford.edu/ai-index/2025-ai-index-report", "retrieved_at": "2026-01-22T00:14:03.745663Z", "title": "The 2025 AI Index Report | Stanford HAI", "section": "AI News That Matters", "source_type": "Research / Academic", "content_hash": "e876b272576015acf7fcba6cce1a086777ef90bec42e390119bcdb311fdabc6b", "chunk_index": 1, "char_start": 1329, "char_end": 2819, "text": "Artificial Intelligence has leapt to the forefront of global discourse, garnering increased attention from practitioners, industry leaders, policymakers, and the general public. The diversity of opinions and debates gathered from news articles this year illustrates just how broadly AI is being investigated, studied, and applied. However, the field of AI is still evolving rapidly and even experts have a hard time understanding and tracking progress across the field.\n\nArtificial Intelligence has leapt to the forefront of global discourse, garnering increased attention from practitioners, industry leaders, policymakers, and the general public. The diversity of opinions and debates gathered from news articles this year illustrates just how broadly AI is being investigated, studied, and applied. However, the field of AI is still evolving rapidly and even experts have a hard time understanding and tracking progress across the field.\n\nAt Stanford HAI, we believe AI is poised to be the most transformative technology of the 21st century. But its benefits won’t be evenly distributed unless we guide its development thoughtfully. The AI Index offers one of the most comprehensive, data-driven views of artificial intelligence. Recognized as a trusted resource by global media, governments, and leading companies, the AI Index equips policymakers, business leaders, and the public with rigorous, objective insights into AI’s technical progress, economic influence, and societal impact."}
{"chunk_id": "hai_stanford_edu__da87f2b213eb9272::c0002", "stable_id": "hai_stanford_edu__da87f2b213eb9272", "url": "https://hai.stanford.edu/ai-index/2025-ai-index-report", "final_url": "https://hai.stanford.edu/ai-index/2025-ai-index-report", "retrieved_at": "2026-01-22T00:14:03.745663Z", "title": "The 2025 AI Index Report | Stanford HAI", "section": "AI News That Matters", "source_type": "Research / Academic", "content_hash": "e876b272576015acf7fcba6cce1a086777ef90bec42e390119bcdb311fdabc6b", "chunk_index": 2, "char_start": 2819, "char_end": 4242, "text": "Read the translation\n\nIn 2023, researchers introduced new benchmarks—MMMU, GPQA, and SWE-bench—to test the limits of advanced AI systems. Just a year later, performance sharply increased: scores rose by 18.8, 48.9, and 67.3 percentage points on MMMU, GPQA, and SWE-bench, respectively. Beyond benchmarks, AI systems made major strides in generating high-quality video, and in some settings, language model agents even outperformed humans in programming tasks with limited time budgets.\n\nFrom healthcare to transportation, AI is rapidly moving from the lab to daily life. In 2023, the FDA approved 223 AI-enabled medical devices, up from just six in 2015. On the roads, self-driving cars are no longer experimental: Waymo, one of the largest U.S. operators, provides over 150,000 autonomous rides each week, while Baidu’s affordable Apollo Go robotaxi fleet now serves numerous cities across China.\n\nIn 2024, U.S. private AI investment grew to $109.1 billion—nearly 12 times China’s $9.3 billion and 24 times the U.K.’s $4.5 billion. Generative AI saw particularly strong momentum, attracting $33.9 billion globally in private investment—an 18.7% increase from 2023. AI business usage is also accelerating: 78% of organizations reported using AI in 2024, up from 55% the year before. Meanwhile, a growing body of research confirms that AI boosts productivity and, in most cases, helps narrow skill gaps across the workforce."}
{"chunk_id": "hai_stanford_edu__da87f2b213eb9272::c0003", "stable_id": "hai_stanford_edu__da87f2b213eb9272", "url": "https://hai.stanford.edu/ai-index/2025-ai-index-report", "final_url": "https://hai.stanford.edu/ai-index/2025-ai-index-report", "retrieved_at": "2026-01-22T00:14:03.745663Z", "title": "The 2025 AI Index Report | Stanford HAI", "section": "AI News That Matters", "source_type": "Research / Academic", "content_hash": "e876b272576015acf7fcba6cce1a086777ef90bec42e390119bcdb311fdabc6b", "chunk_index": 3, "char_start": 4242, "char_end": 5930, "text": "In 2024, U.S.-based institutions produced 40 notable AI models, significantly outpacing China’s 15 and Europe’s three. While the U.S. maintains its lead in quantity, Chinese models have rapidly closed the quality gap: performance differences on major benchmarks such as MMLU and HumanEval shrank from double digits in 2023 to near parity in 2024. Meanwhile, China continues to lead in AI publications and patents. At the same time, model development is increasingly global, with notable launches from regions such as the Middle East, Latin America, and Southeast Asia.\n\nAI-related incidents are rising sharply, yet standardized RAI evaluations remain rare among major industrial model developers. However, new benchmarks like HELM Safety, AIR-Bench, and FACTS offer promising tools for assessing factuality and safety. Among companies, a gap persists between recognizing RAI risks and taking meaningful action. In contrast, governments are showing increased urgency: In 2024, global cooperation on AI governance intensified, with organizations including the OECD, EU, U.N., and African Union releasing frameworks focused on transparency, trustworthiness, and other core responsible AI principles.\n\nIn countries like China (83%), Indonesia (80%), and Thailand (77%), strong majorities see AI products and services as more beneficial than harmful. In contrast, optimism remains far lower in places like Canada (40%), the United States (39%), and the Netherlands (36%). Still, sentiment is shifting: since 2022, optimism has grown significantly in several previously skeptical countries—including Germany (+10%), France (+10%), Canada (+8%), Great Britain (+8%), and the United States (+4%)."}
{"chunk_id": "hai_stanford_edu__da87f2b213eb9272::c0004", "stable_id": "hai_stanford_edu__da87f2b213eb9272", "url": "https://hai.stanford.edu/ai-index/2025-ai-index-report", "final_url": "https://hai.stanford.edu/ai-index/2025-ai-index-report", "retrieved_at": "2026-01-22T00:14:03.745663Z", "title": "The 2025 AI Index Report | Stanford HAI", "section": "AI News That Matters", "source_type": "Research / Academic", "content_hash": "e876b272576015acf7fcba6cce1a086777ef90bec42e390119bcdb311fdabc6b", "chunk_index": 4, "char_start": 5930, "char_end": 7502, "text": "Driven by increasingly capable small models, the inference cost for a system performing at the level of GPT-3.5 dropped over 280-fold between November 2022 and October 2024. At the hardware level, costs have declined by 30% annually, while energy efficiency has improved by 40% each year. Open-weight models are also closing the gap with closed models, reducing the performance difference from 8% to just 1.7% on some benchmarks in a single year. Together, these trends are rapidly lowering the barriers to advanced AI.\n\nIn 2024, U.S. federal agencies introduced 59 AI-related regulations—more than double the number in 2023—and issued by twice as many agencies. Globally, legislative mentions of AI rose 21.3% across 75 countries since 2023, marking a ninefold increase since 2016. Alongside growing attention, governments are investing at scale: Canada pledged $2.4 billion, China launched a $47.5 billion semiconductor fund, France committed €109 billion, India pledged $1.25 billion, and Saudi Arabia’s Project Transcendence represents a $100 billion initiative.\n\nTwo-thirds of countries now offer or plan to offer K–12 CS education—twice as many as in 2019—with Africa and Latin America making the most progress. In the U.S., the number of graduates with bachelor’s degrees in computing has increased 22% over the last 10 years. Yet access remains limited in many African countries due to basic infrastructure gaps like electricity. In the U.S., 81% of K–12 CS teachers say AI should be part of foundational CS education, but less than half feel equipped to teach it."}
{"chunk_id": "hai_stanford_edu__da87f2b213eb9272::c0005", "stable_id": "hai_stanford_edu__da87f2b213eb9272", "url": "https://hai.stanford.edu/ai-index/2025-ai-index-report", "final_url": "https://hai.stanford.edu/ai-index/2025-ai-index-report", "retrieved_at": "2026-01-22T00:14:03.745663Z", "title": "The 2025 AI Index Report | Stanford HAI", "section": "AI News That Matters", "source_type": "Research / Academic", "content_hash": "e876b272576015acf7fcba6cce1a086777ef90bec42e390119bcdb311fdabc6b", "chunk_index": 5, "char_start": 7502, "char_end": 8970, "text": "Nearly 90% of notable AI models in 2024 came from industry, up from 60% in 2023, while academia remains the top source of highly cited research. Model scale continues to grow rapidly—training compute doubles every five months, datasets every eight, and power use annually. Yet performance gaps are shrinking: the score difference between the top and 10th-ranked models fell from 11.9% to 5.4% in a year, and the top two are now separated by just 0.7%. The frontier is increasingly competitive—and increasingly crowded.\n\nAI’s growing importance is reflected in major scientific awards: two Nobel Prizes recognized work that led to deep learning (physics), and to its application to protein folding (chemistry), while the Turing Award honored groundbreaking contributions to reinforcement learning.\n\nAI models excel at tasks like International Mathematical Olympiad problems but still struggle with complex reasoning benchmarks like PlanBench. They often fail to reliably solve logic tasks even when provably correct solutions exist, limiting their effectiveness in high-stakes settings where precision is critical.\n\nThe AI Index report tracks, collates, distills, and visualizes data related to artificial intelligence (AI). Our mission is to provide unbiased, rigorously vetted, broadly sourced data in order for policymakers, researchers, executives, journalists, and the general public to develop a more thorough and nuanced understanding of the complex field of AI."}
{"chunk_id": "hai_stanford_edu__da87f2b213eb9272::c0006", "stable_id": "hai_stanford_edu__da87f2b213eb9272", "url": "https://hai.stanford.edu/ai-index/2025-ai-index-report", "final_url": "https://hai.stanford.edu/ai-index/2025-ai-index-report", "retrieved_at": "2026-01-22T00:14:03.745663Z", "title": "The 2025 AI Index Report | Stanford HAI", "section": "AI News That Matters", "source_type": "Research / Academic", "content_hash": "e876b272576015acf7fcba6cce1a086777ef90bec42e390119bcdb311fdabc6b", "chunk_index": 6, "char_start": 8970, "char_end": 10261, "text": "See Also: Report Revisions\n\nPolicymakers use the AI Index to inform their understanding and decisions about AI. We curated a summary of highlights from the AI Index Report 2025 that are particularly relevant to policymakers and other policy audiences.\n\nDownload the Policy Highlights\n\nThis chapter explores trends in AI research and development, beginning with an analysis of AI publications, patents, and notable AI systems.\n\nThe Technical Performance section of this year’s AI Index provides a comprehensive overview of AI advancements in 2024.\n\nArtificial intelligence is now deeply integrated into nearly every aspect of our lives. It is reshaping sectors like education, finance, and healthcare, where algorithm-driven insights guide critical decisions.\n\nGlobal private AI investment hits record high...\n\nThis chapter explores key trends in AI-driven science and medicine, reflecting the technology’s growing impact in these fields.\n\nAI’s advancing capabilities have captured policymakers’ attention, leading to an increase in AI-related policies worldwide.\n\nAI has entered the public consciousness through generative AI’s impact on work...\n\nAs AI continues to permeate broad swaths of society, it is becoming increasingly important to understand public sentiment around the technology."}
{"chunk_id": "aiguidebook_vcu_edu__c78139a06334a4fe::c0000", "stable_id": "aiguidebook_vcu_edu__c78139a06334a4fe", "url": "https://aiguidebook.vcu.edu/tools/", "final_url": "https://aiguidebook.vcu.edu/tools/", "retrieved_at": "2026-01-22T00:14:51.015831Z", "title": "AI Tools - Artificial Intelligence Guidebook - Virginia Commonwealth University", "section": "AI Tools You Might Use", "source_type": "University / Official", "content_hash": "d80128b96092e10032ad8172b503f4a5d7889c919fa531f2ca39108143c43aab", "chunk_index": 0, "char_start": 0, "char_end": 1577, "text": "AI Tools\n\nVCU Technology Services licenses a variety of generative AI tools that are vetted, secured and available to VCU faculty, staff and students to use for free. See our list of enterprise tools below for more information, including approved data designations.\n\nGoogle Gemini is an AI tool that combines advanced language understanding and generation capabilities to assist users in various tasks, such as writing, researching, and creating content. It leverages Google's extensive data and AI expertise to deliver accurate and context-aware assistance.\n\nOur licensed access to Gemini is approved for Category 2 data and ensures that your prompts and Gemini’s responses are:\n\n- Not available to other customers\n\n- Not used to train or improve any third-party products or services (such as OpenAI models)\n\n- Not used to train or improve Google AI models\n\nNotebookLM is a Google Gemini-powered note-taking and research assistant offered as part of your VCU Google Workspace account. You can upload PDFs, Google Docs, web URLs, and YouTube videos into a \"notebook.\" Once the sources are uploaded, NotebookLM becomes an expert on that content, allowing you to ask questions and receive answers with citations pointing back to the original text. Key features include the ability to quickly summarize documents, generate different content formats such as briefing documents and study guides, and podcast-style audio overviews. It aims to streamline research, enhance productivity, and improve information retention by providing AI-driven insights grounded in your own documents."}
{"chunk_id": "aiguidebook_vcu_edu__c78139a06334a4fe::c0001", "stable_id": "aiguidebook_vcu_edu__c78139a06334a4fe", "url": "https://aiguidebook.vcu.edu/tools/", "final_url": "https://aiguidebook.vcu.edu/tools/", "retrieved_at": "2026-01-22T00:14:51.015831Z", "title": "AI Tools - Artificial Intelligence Guidebook - Virginia Commonwealth University", "section": "AI Tools You Might Use", "source_type": "University / Official", "content_hash": "d80128b96092e10032ad8172b503f4a5d7889c919fa531f2ca39108143c43aab", "chunk_index": 1, "char_start": 1577, "char_end": 2923, "text": "VCU has licensed Microsoft Copilot, a generative artificial intelligence chatbot for all faculty, staff, and students to use for free. Navigate to go.vcu.edu/copilot and login with your VCU email address and password. Copilot is built with the same security, privacy, and compliance standards as other Microsoft products. Our licensed access to Copilot is approved for Category 2 data and ensures that your prompts and Copilot’s responses are:\n\n- Not available to other customers.\n\n- Not used to train or improve any third-party products or services (such as OpenAI models)\n\n- Not used to train or improve Microsoft AI models\n\nImportant update: as of November 2024 the Copilot app integration and file upload features no longer work for all VCU accounts. Microsoft now requires OneDrive to be enabled for this feature and we do not currently have OneDrive available for all VCU accounts. If you are interested in getting access to these features, please contact emergingtech@vcu.edu.\n\nAdobe Express is a versatile design tool that allows users to create social graphics, flyers, logos, and more with ease. It offers a user-friendly interface, a variety of templates, and seamless integration with other Adobe Creative Cloud apps, making it ideal for both beginners and experienced designers looking to create professional-quality content quickly."}
{"chunk_id": "aiguidebook_vcu_edu__c78139a06334a4fe::c0002", "stable_id": "aiguidebook_vcu_edu__c78139a06334a4fe", "url": "https://aiguidebook.vcu.edu/tools/", "final_url": "https://aiguidebook.vcu.edu/tools/", "retrieved_at": "2026-01-22T00:14:51.015831Z", "title": "AI Tools - Artificial Intelligence Guidebook - Virginia Commonwealth University", "section": "AI Tools You Might Use", "source_type": "University / Official", "content_hash": "d80128b96092e10032ad8172b503f4a5d7889c919fa531f2ca39108143c43aab", "chunk_index": 2, "char_start": 2923, "char_end": 4198, "text": "The Zoom AI Companion feature allows for AI generated transcription and meeting summary features for use in Zoom meetings. VCU has vetted and approved this AI feature for use in meetings, including those that discuss sensitive or confidential information.\n\nMeetings may contain sensitive information that requires the guarantee of confidentiality and security. VCU has vetted and approved the use of this transcription tool in our Zoom platform. We recommend avoiding use of any other third-party bots in virtual meetings at this time. Third-party bots can scrape your calendar for info and keep a written account or recorded minutes of meetings. They may save meetings in unknown places and join meetings when you’re not there.\n\nOur Arabic professor asked us to use Generative AI to teach Arabic grammar and evaluate how it performed. Spoiler alert, it didn’t do very well, but it was a good exercise to learn, like how AI can get stuff right and wrong, and how it can be a teaching tool.\n\nJack Glagola\n\nPhilosophy, ‘26\n\nI’ll put it in a Generative AI, “Hey can you give me four questions and can you explain or expand upon the topic we’re talking about” It’ll give me different topics and actually give me a mini test. It’s almost like a mini tutor in your pocket for free."}
{"chunk_id": "aiguidebook_vcu_edu__c78139a06334a4fe::c0003", "stable_id": "aiguidebook_vcu_edu__c78139a06334a4fe", "url": "https://aiguidebook.vcu.edu/tools/", "final_url": "https://aiguidebook.vcu.edu/tools/", "retrieved_at": "2026-01-22T00:14:51.015831Z", "title": "AI Tools - Artificial Intelligence Guidebook - Virginia Commonwealth University", "section": "AI Tools You Might Use", "source_type": "University / Official", "content_hash": "d80128b96092e10032ad8172b503f4a5d7889c919fa531f2ca39108143c43aab", "chunk_index": 3, "char_start": 4198, "char_end": 4664, "text": "Abriana Villegas\n\nInterdisciplinary Studies, ‘27\n\nExamples of AI in Industry\n\nArtificial Intelligence isn't just a buzzword -- it's revolutionizing industries across the globe. From streamlining operations to enhancing customer experiences, AI is making a significant impact in countless fields. Let's explore how AI is transforming six key industries. Hover over each box to discover specific applications and innovative companies leading the charge in AI adoption."}
{"chunk_id": "guides_library_georgetown_edu__3fee8ba3bf16d12d::c0000", "stable_id": "guides_library_georgetown_edu__3fee8ba3bf16d12d", "url": "https://guides.library.georgetown.edu/ai/tools", "final_url": "https://guides.library.georgetown.edu/ai/tools", "retrieved_at": "2026-01-22T00:14:52.870452Z", "title": "AI Tools for Research - Artificial Intelligence (Generative) Resources - Guides at Georgetown University", "section": "AI Tools You Might Use", "source_type": "University / Official", "content_hash": "43529a1f9603c10d3af3a8409576f51d4466e82788bc715b022e333d1f6b4bd6", "chunk_index": 0, "char_start": 0, "char_end": 1327, "text": "The resources described in the table represent an incomplete list of tools specifically geared towards exploring and synthesizing research. As generative AI becomes more integrated in online search tools, even the very early stages of research and topic development could incorporate AI. If you have any questions about using these tools for your research, please Email a Librarian.\n\nAI tools for research can help you to discover new sources for your literature review or research assignment. These tools will synthesize information from large databases of scholarly output with the aim of finding the most relevant articles and saving researchers' time. As with our research databases or any other search tool, however, it's important not to rely on one tool for all of your research, as you will risk missing important information on your topic of interest.\n\n| AI-Powered Research Tools | ||||\n\n|---|---|---|---|---|\n\n| NAME | WHAT IT DOES | UNDERLYING DATA | IS IT FREE? | MORE INFORMATION |\n\n| Connected Papers | Like Research Rabbit (see below), Connected Papers focuses on the relationships between research papers to find similar research. You can also use Connected Papers to get a visual overview of an academic field. | Semantic Scholar database | Free with paid subscriptions available. | Connected Papers - About |"}
{"chunk_id": "guides_library_georgetown_edu__3fee8ba3bf16d12d::c0001", "stable_id": "guides_library_georgetown_edu__3fee8ba3bf16d12d", "url": "https://guides.library.georgetown.edu/ai/tools", "final_url": "https://guides.library.georgetown.edu/ai/tools", "retrieved_at": "2026-01-22T00:14:52.870452Z", "title": "AI Tools for Research - Artificial Intelligence (Generative) Resources - Guides at Georgetown University", "section": "AI Tools You Might Use", "source_type": "University / Official", "content_hash": "43529a1f9603c10d3af3a8409576f51d4466e82788bc715b022e333d1f6b4bd6", "chunk_index": 1, "char_start": 1327, "char_end": 2728, "text": "| Consensus | Consensus uses large language models (LLMs) to help researchers find and synthesize answers to research questions, focusing on the scholarly authors' findings and claims in each paper. | Semantic Scholar database | Free with paid subscriptions available. | Consensus FAQs |\n\n| Elicit | Elicit uses LLMs to find papers relevant to your topic by searching through papers and citations and extracting and synthesizing key information. | Semantic Scholar database | Free with paid subscriptions available. | Elicit FAQs |\n\n| Keenious | Keenious is a recommendation tool for academic articles and topics based on papers you upload. | Open Alex | Free with paid subscriptions available. | Keenious Help File |\n\n| Research Rabbit | Research Rabbit is a citation-based mapping tool that focuses on the relationships between research works. It uses visualizations to help researchers find similar papers and other researchers in their field. | Open Alex, Semantic Scholar, and other databases | Research Rabbit is currently free. | Research Rabbit FAQs |\n\n| scite | scite has a suite of products that help researchers develop their topics, find papers, and search citations in context (describing whether the article provides supporting or contrasting evidence) | Many different sources (an incomplete list can be found on this page) | No. See pricing information. | scite FAQs; how scite works |"}
{"chunk_id": "guides_library_georgetown_edu__3fee8ba3bf16d12d::c0002", "stable_id": "guides_library_georgetown_edu__3fee8ba3bf16d12d", "url": "https://guides.library.georgetown.edu/ai/tools", "final_url": "https://guides.library.georgetown.edu/ai/tools", "retrieved_at": "2026-01-22T00:14:52.870452Z", "title": "AI Tools for Research - Artificial Intelligence (Generative) Resources - Guides at Georgetown University", "section": "AI Tools You Might Use", "source_type": "University / Official", "content_hash": "43529a1f9603c10d3af3a8409576f51d4466e82788bc715b022e333d1f6b4bd6", "chunk_index": 2, "char_start": 2728, "char_end": 4019, "text": "| Scholarcy | Scholarcy summarizes key points and claims of articles into 'summary cards' that researchers can read, share, and annotate when compiling research on a given topic. | Scholarcy only uses research papers uploaded or linked by the researcher themselves. It works as a way to help you read and summarize your research, but is not a search engine. | Free with paid subscriptions available. | Scholarcy FAQs |\n\n| Semantic Scholar | Semantic Scholar (which supplies underlying data for many of the other tools on this list) provides brief summaries ('TLDR's) of the main objectives and results of papers. | Semantic Scholar database | Semantic Scholar is currently free. | Semantic Scholar FAQs |\n\n| Undermind | An AI research assistant that works with you to refine your research question and find relevant papers. | Semantic Scholar database | Free with paid subscriptions available. | Undermind FAQs (scroll down for FAQs) |\n\n| AI-Powered Large Language Models (LLMs) | ||||\n\n| NAME | WHAT IT DOES | UNDERLYING DATA | IS IT FREE? | MORE INFORMATION |\n\n| ChatGPT | While the AI chatbot ChatGPT is typically thought of as a writing tool, it can be used in the initial idea development phase of research. (Remember to always look up claims and sources to verify their credibility.) |"}
{"chunk_id": "guides_library_georgetown_edu__3fee8ba3bf16d12d::c0003", "stable_id": "guides_library_georgetown_edu__3fee8ba3bf16d12d", "url": "https://guides.library.georgetown.edu/ai/tools", "final_url": "https://guides.library.georgetown.edu/ai/tools", "retrieved_at": "2026-01-22T00:14:52.870452Z", "title": "AI Tools for Research - Artificial Intelligence (Generative) Resources - Guides at Georgetown University", "section": "AI Tools You Might Use", "source_type": "University / Official", "content_hash": "43529a1f9603c10d3af3a8409576f51d4466e82788bc715b022e333d1f6b4bd6", "chunk_index": 3, "char_start": 4019, "char_end": 5294, "text": "The LLM is regularly updated. Logged in users can use ChatGPT to search the web. |\n\nThere is a free version available. | OpenAI Help Center - ChatGPT |\n\n| Claude | An AI-powered chatbot trained by Anthropic using Constitutional AI to be safe, accurate, and secure. It can be used in developmental stages of research for brainstorming and data analysis. | Publicly available information via the Internet along with licensed data sets. Data is updated regularly with each model version having a different cut-off date. | Free with paid subscriptions available. | Claude FAQ and Help Center |\n\n| Gemini | Designed by Google, Gemini is an AI-powered chatbot that responds to natural language queries with relevant information. As with ChatGPT, researchers can use Gemini to aid in topic development and initial source discovery. | Gemini can currently connect to the Internet. | Free with paid subscriptions available. Personal Google account required to use as GU has not authorized Gemini for georgetown.edu email addresses. | Gemini FAQ |\n\n| Perplexity | Using LLMs, Perplexity is a search engine that provides AI-generated answers, including citations which are linked above the summaries. | Internal search index | Free with paid subscriptions available. | Perplexity FAQs |"}
{"chunk_id": "guides_library_georgetown_edu__3fee8ba3bf16d12d::c0004", "stable_id": "guides_library_georgetown_edu__3fee8ba3bf16d12d", "url": "https://guides.library.georgetown.edu/ai/tools", "final_url": "https://guides.library.georgetown.edu/ai/tools", "retrieved_at": "2026-01-22T00:14:52.870452Z", "title": "AI Tools for Research - Artificial Intelligence (Generative) Resources - Guides at Georgetown University", "section": "AI Tools You Might Use", "source_type": "University / Official", "content_hash": "43529a1f9603c10d3af3a8409576f51d4466e82788bc715b022e333d1f6b4bd6", "chunk_index": 4, "char_start": 5294, "char_end": 5827, "text": "Georgetown University's Center for New Designs in Learning and Scholarship (CNDLS) offers a list of additional AI tools with a range of different purposes including visual design, writing, time management, and more.\n\nIthaka S+R has created a Generative AI Product Tracker which lists tools by their primary purposes and includes pricing information, as well as updates on the tools' features and limitations.\n\nThis work is licensed under a Creative Commons Attribution NonCommercial 4.0 International License. | Details of our policy"}
{"chunk_id": "www_huit_harvard_edu__8f8b35cba5ce9e90::c0000", "stable_id": "www_huit_harvard_edu__8f8b35cba5ce9e90", "url": "https://www.huit.harvard.edu/ai/tools", "final_url": "https://www.huit.harvard.edu/ai/tools", "retrieved_at": "2026-01-22T00:14:49.288151Z", "title": "Generative AI Tool Comparison | Harvard University Information Technology", "section": "AI Tools You Might Use", "source_type": "University / Official", "content_hash": "1795e65baa4d9e4ec04756d560e4558fb8a6fcf472ceed290e6f5d332770f4ad", "chunk_index": 0, "char_start": 0, "char_end": 1566, "text": "Generative AI Tool Comparison\n\nThe below table shows Generative AI tools currently available from HUIT, including the level of confidential Harvard data for which these tools are approved. Your School’s local IT department may also offer additional tools.\n\nMany other AI tools are available to the public, including tools that are free to use. However, per University guidelines, you should not enter data classified as confidential (Level 2 and above) into publicly-available Generative AI tools.\n\nAs always, if you’re considering using Generative AI tools for Harvard work, you must follow the University's initial guidelines for use.\n\nAI tools for general use\n\nThis category of tools includes “chatbots” and AI assistants for general use and productivity. They are designed to understand and generate human-like responses to text-based, natural language prompts. They can generate text, code, and images, translate languages, write different kinds of creative content, or integrate with productivity and collaboration tools.\n\nAI meeting assistants (aka “AI note takers” or “bots”) can transcribe and summarize online meetings. Although these tools have legitimate uses (e.g., for accessibility and capturing notes for later reference), they can also pose substantial privacy, regulatory, and legal risks, and have the potential to stifle conversation and open inquiry. AI meeting assistants should not be used in Harvard meetings, with the exception of approved tools with contractual protections. See guidelines for more information about risks and availability."}
{"chunk_id": "www_huit_harvard_edu__8f8b35cba5ce9e90::c0001", "stable_id": "www_huit_harvard_edu__8f8b35cba5ce9e90", "url": "https://www.huit.harvard.edu/ai/tools", "final_url": "https://www.huit.harvard.edu/ai/tools", "retrieved_at": "2026-01-22T00:14:49.288151Z", "title": "Generative AI Tool Comparison | Harvard University Information Technology", "section": "AI Tools You Might Use", "source_type": "University / Official", "content_hash": "1795e65baa4d9e4ec04756d560e4558fb8a6fcf472ceed290e6f5d332770f4ad", "chunk_index": 1, "char_start": 1566, "char_end": 2897, "text": "Note: all data classification levels listed below apply only to the Harvard-offered versions of these tools and not to publicly-available versions of these tools (which should not be used for Harvard work).\n\n| Tool | Overview | Availability | Data classification level |\n\n|---|---|---|---|\n\nExperiment with multiple LLMs in secure environment. Features: Code generation, Creative writing, Data analysis, Summarizing, Text generation and editing, Image generation, Translation | Faculty, staff, and researchers in Central Administration, FAS, College, GSAS, SEAS, GSD, GSE, HBS, HDS, HKS, HLS, HMS (Quad), HSDM (Quad), Radcliffe, SPH; Students in College, GSAS, SEAS, GSE, HDS, HKS, HLS, HMS (Quad), HSDM (Quad), SPH | ||\n\n(via Harvard Google account) | Versatile chatbot integrated with Google Workspace. Features: Chat, search, coding, writing, data analysis, image generation, translation and more. Includes Deep Research and NotebookLM. | Basic Gemini features are available in all Harvard Google accounts, including Harvard College. | |\n\n(via Harvard Microsoft 365 account) | Versatile chatbot integrated with Microsoft 365. Features: Chat, search, coding, writing, data analysis, image generation, translation and more. Includes Deep Research. | Basic Copilot Chat features available in all Harvard Microsoft 365 accounts. | |"}
{"chunk_id": "www_huit_harvard_edu__8f8b35cba5ce9e90::c0002", "stable_id": "www_huit_harvard_edu__8f8b35cba5ce9e90", "url": "https://www.huit.harvard.edu/ai/tools", "final_url": "https://www.huit.harvard.edu/ai/tools", "retrieved_at": "2026-01-22T00:14:49.288151Z", "title": "Generative AI Tool Comparison | Harvard University Information Technology", "section": "AI Tools You Might Use", "source_type": "University / Official", "content_hash": "1795e65baa4d9e4ec04756d560e4558fb8a6fcf472ceed290e6f5d332770f4ad", "chunk_index": 2, "char_start": 2897, "char_end": 4155, "text": "Versatile chatbot able to generate text, code, images, and more. Features: Chatbot customization, Code generation, Creative writing, Data analysis, Image generation, Summarizing, Text generation and editing, Translation | Access provided and coordinated by Schools and Units; contact your local IT department for details. | ||\n\nGenerate images and text effects by simply typing key words or a description. Trained on stock images, openly licensed and public domain content. Also integrated into Adobe apps. Features: Image generation, Image editing | Available to Harvard faculty, staff, students, and researchers as part of Harvard Adobe Creative Cloud license. |\n\nAI developer tools\n\nThis category of tools includes AI assistants and API access to enable developers to integrate Large Language Models (LLMs) into their own applications, products, or services. This includes chatbot creation and customization, building and testing applications, access to model training and deployment, coding, predictive analytics, and more. Code and low/no-code offerings are available. These tools are subject to change based on availability. Contact HUIT for more information and advice based on use case.\n\n| Tool | Overview | Availability | Data classification level |"}
{"chunk_id": "www_huit_harvard_edu__8f8b35cba5ce9e90::c0003", "stable_id": "www_huit_harvard_edu__8f8b35cba5ce9e90", "url": "https://www.huit.harvard.edu/ai/tools", "final_url": "https://www.huit.harvard.edu/ai/tools", "retrieved_at": "2026-01-22T00:14:49.288151Z", "title": "Generative AI Tool Comparison | Harvard University Information Technology", "section": "AI Tools You Might Use", "source_type": "University / Official", "content_hash": "1795e65baa4d9e4ec04756d560e4558fb8a6fcf472ceed290e6f5d332770f4ad", "chunk_index": 3, "char_start": 4155, "char_end": 4625, "text": "|---|---|---|---|\n\nSelf-service AI APIs (HarvardKey-protected) | Self-service AI API access for eligible users is available through Harvard’s API Platform. These include pay-as-you-go or limited-access, credit-redemption options. | Some self-service AI APIs in the portal have limited availability. Where this is the case, it's noted in the documentation for the API. | |\n\nAI platforms, assistants, and APIs for building and using AI. | Available by request from HUIT. |"}
{"chunk_id": "www_sba_gov__f3b30e5beeef2707::c0000", "stable_id": "www_sba_gov__f3b30e5beeef2707", "url": "https://www.sba.gov/business-guide/manage-your-business/ai-small-business", "final_url": "https://www.sba.gov/business-guide/manage-your-business/ai-small-business", "retrieved_at": "2026-01-22T00:14:38.527375Z", "title": "AI for small business | U.S. Small Business Administration", "section": "How Businesses Are Using AI", "source_type": "University / Official", "content_hash": "e2c2a6040c8a68a5aeb48a132e9ca93966580b51c656f7c1186dbc97873c76c1", "chunk_index": 0, "char_start": 0, "char_end": 1294, "text": "Content\n\nTechnology allows small businesses to be more competitive in today’s fast-paced economy. The federal government has adopted Artificial intelligence (AI) as a way to help them better serve the public. As a small business owner, AI can help your small businesses do more with less.\n\nSBA is dedicated to informing small businesses about the ethical use of AI tools. We also want to help you think about effective ways to implement AI into your business practices. AI is relatively new, so start small. If you are unsure what tool you may need, many AI tools offer basic services for free or at a lower cost. Try testing them to see if they add value to your business. You may find they improve internal efficiencies, freeing up your time to focus on growing your business.\n\nRead on to find out about both the benefits and risks of using AI in your small business. If you're new to AI terminology, our list of common AI terms can help you make informed decisions.\n\nHow AI can benefit your small business\n\nAI can improve efficiency, which can help business owners save time. It can also save on costs and help your business stay competitive in times of mounting inflation. If a job market is experiencing labor shortages, AI can help compensate for skilled labor. AI can help your business:"}
{"chunk_id": "www_sba_gov__f3b30e5beeef2707::c0001", "stable_id": "www_sba_gov__f3b30e5beeef2707", "url": "https://www.sba.gov/business-guide/manage-your-business/ai-small-business", "final_url": "https://www.sba.gov/business-guide/manage-your-business/ai-small-business", "retrieved_at": "2026-01-22T00:14:38.527375Z", "title": "AI for small business | U.S. Small Business Administration", "section": "How Businesses Are Using AI", "source_type": "University / Official", "content_hash": "e2c2a6040c8a68a5aeb48a132e9ca93966580b51c656f7c1186dbc97873c76c1", "chunk_index": 1, "char_start": 1294, "char_end": 2742, "text": "- Solve problems before they happen. Tracking traffic and flight delays can help you to avoid delivery and travel issues. Monitoring floodplains can help you prepare for or avoid disaster damage. Rate optimizers can help lessen shipping costs. AI can even help you find ways to mitigate your business’s environmental impact that won’t break the bank.\n\n- Safeguard your data. Look into security software or vendors that use AI technology. The ability to automate security functions can help security professionals process more data at a faster rate. The ability to quickly react to an attack and apply a security patch can make all the difference to your small business.\n\n- Make better business decisions. AI can help you analyze your small business data and pick out common themes. Use your own client data to make better strategic decisions. Data inclusive tools can also help you compare your business to similar businesses and find gaps that you can address or advantages you can exploit to give your business an extra boost.\n\n- Take on repeat tasks. Use a voice assistant to program monthly meetings. Set calendar reminders that remind employees of important work deadlines. Sort your email into inboxes by task. Update to-do lists or restock inventory without having to stop what you’re doing. AI can also be used to record and summarize your team meetings. You can also generate reusable templates that reduce time and improve communications."}
{"chunk_id": "www_sba_gov__f3b30e5beeef2707::c0002", "stable_id": "www_sba_gov__f3b30e5beeef2707", "url": "https://www.sba.gov/business-guide/manage-your-business/ai-small-business", "final_url": "https://www.sba.gov/business-guide/manage-your-business/ai-small-business", "retrieved_at": "2026-01-22T00:14:38.527375Z", "title": "AI for small business | U.S. Small Business Administration", "section": "How Businesses Are Using AI", "source_type": "University / Official", "content_hash": "e2c2a6040c8a68a5aeb48a132e9ca93966580b51c656f7c1186dbc97873c76c1", "chunk_index": 2, "char_start": 2742, "char_end": 4053, "text": "- Create business content. Get help when editing photos or videos. Draft a business plan. Write job postings and blogs. AI can also use your original marketing content to develop e-commerce product descriptions or generate and schedule social media posts across multiple platforms or develop engaging content based on trending hashtags and topics. To keep your budget down, make variations you can use on different sites or over several months.\n\n- Collaborate and brainstorm. Get ideas for a logo design that uses your company colors, or to come up with a marketing plan that meets your budget. Ask about a solution for a business blocker. If you work alone, AI can help you collaborate on business solutions. You can also use it to find problems in upcoming projects, such as hidden costs or possible financial risks.\n\n- Improve customer service. Add a website chatbot that can answer common questions or complete an order. Automate your phone service to route calls to the right department. Fine-tune ads to better target your customers’ interests and needs. Write courteous, thoughtful replies to online reviews.\n\nRisks of AI use\n\nUsing AI can mean your business is assuming a certain amount of risk. If it is part of software you have purchased, those creators are responsible for their product’s use of AI."}
{"chunk_id": "www_sba_gov__f3b30e5beeef2707::c0003", "stable_id": "www_sba_gov__f3b30e5beeef2707", "url": "https://www.sba.gov/business-guide/manage-your-business/ai-small-business", "final_url": "https://www.sba.gov/business-guide/manage-your-business/ai-small-business", "retrieved_at": "2026-01-22T00:14:38.527375Z", "title": "AI for small business | U.S. Small Business Administration", "section": "How Businesses Are Using AI", "source_type": "University / Official", "content_hash": "e2c2a6040c8a68a5aeb48a132e9ca93966580b51c656f7c1186dbc97873c76c1", "chunk_index": 3, "char_start": 4053, "char_end": 5519, "text": "If you are using free AI tools or software in your small business, have another person review all AI products. This will help make sure they are being used ethically, securely, and in a manner that accurately represents your business.\n\nIf your small business has an attorney, consider a consultation. Your lawyer can help make sure you are using AI consistent with local laws or best business practices. Other considerations include:\n\n- Intellectual property. AI sources content from the web. Make sure anything you produce doesn’t infringe on any patents, copyrights or trademarks.\n\n- Security risks. Try not to feed any sensitive data or proprietary information. This will help keep it out of the data pool that the system uses when producing content. Keep an eye out for phishing campaigns that might have been written using AI.\n\n- Customer trust. Tools that recognize AI-generated content may mark it as spam, or as not generated with real knowledge of the recipient. This could create customer resistance to future outreach efforts. Make sure a person assesses all the messages and outreach campaigns that AI generates.\n\n- Ethical concerns. Monitor and review content to make sure it reflects your business’s culture and principles. While there are currently no federal or state laws that require businesses to disclose the use of AI, it is becoming an expected best practice. Consider drafting a public statement that discloses how your small business uses AI."}
{"chunk_id": "www_sba_gov__f3b30e5beeef2707::c0004", "stable_id": "www_sba_gov__f3b30e5beeef2707", "url": "https://www.sba.gov/business-guide/manage-your-business/ai-small-business", "final_url": "https://www.sba.gov/business-guide/manage-your-business/ai-small-business", "retrieved_at": "2026-01-22T00:14:38.527375Z", "title": "AI for small business | U.S. Small Business Administration", "section": "How Businesses Are Using AI", "source_type": "University / Official", "content_hash": "e2c2a6040c8a68a5aeb48a132e9ca93966580b51c656f7c1186dbc97873c76c1", "chunk_index": 4, "char_start": 5519, "char_end": 6793, "text": "Common terms\n\nAI is a set of systems that program computers to solve problems or work through tasks. These systems adjust and adapt to new information. This process is meant to imitate human intelligence.\n\nA list of specific rules or instructions. These help a computer system perform a task or solve a problem.\n\nThis is a field within AI. It refers to the process of using data to produce models that can perform complex tasks. The computers can then “learn” from the data, without more programming.\n\nA language model is a type of machine learning model. It uses math to predict the next word in a series of words. It can also help fill in the missing word in a phrase. An example of this is the autocomplete function on a cell phone.\n\nA LLM learns language patterns. It does this by training on a massive amount of data. This allows the machine to recognize the patterns. LLMs can be used for both writing and translation. They are also used to answer questions, like on a chatbot.\n\nThis is an LLM that can create new content. GenAI can make text, images, even videos and music. It learns patterns from existing data. GenAI then uses those patterns to create new and similar data. This technology is what most people know about. GenAI is relatively new and changing daily."}
{"chunk_id": "www_sba_gov__f3b30e5beeef2707::c0005", "stable_id": "www_sba_gov__f3b30e5beeef2707", "url": "https://www.sba.gov/business-guide/manage-your-business/ai-small-business", "final_url": "https://www.sba.gov/business-guide/manage-your-business/ai-small-business", "retrieved_at": "2026-01-22T00:14:38.527375Z", "title": "AI for small business | U.S. Small Business Administration", "section": "How Businesses Are Using AI", "source_type": "University / Official", "content_hash": "e2c2a6040c8a68a5aeb48a132e9ca93966580b51c656f7c1186dbc97873c76c1", "chunk_index": 5, "char_start": 6793, "char_end": 7344, "text": "This refers to the information you feed into GenAI. This can be a sentence, question or other information. The AI model will use this information and its pattern models to generate a response.\n\nAdditional resources\n\n- Harvard University’s Information Technology Department has an article with helpful tips on how to write better AI prompts.\n\n- If you are using AI to innovate new products or technologies, the U.S. Patent Office has published Guidance for AI-assisted Inventions that will help you determine if your invention can qualify for a patent."}
{"chunk_id": "www_flagler_edu__77ae53fb510874a9::c0000", "stable_id": "www_flagler_edu__77ae53fb510874a9", "url": "https://www.flagler.edu/news-events/news/ai-business-education-new-era-opportunity", "final_url": "https://www.flagler.edu/news-events/news/ai-business-education-new-era-opportunity", "retrieved_at": "2026-01-22T00:14:34.113914Z", "title": "AI in Business & Education: A new era of opportunity | Flagler College", "section": "How Businesses Are Using AI", "source_type": "University / Official", "content_hash": "8d9b6bc5dae7cffe831a6bdddc38cde3aaf2a2a71e592c13311fec9d214d2fb2", "chunk_index": 0, "char_start": 0, "char_end": 1402, "text": "During Flagler College's Business Week last month, the “AI & Your Careers” panel featured three industry experts discussing AI's impact on the business world and its practical applications for professionals:\n\n- Harrison Santiago | Software Engineer at NLP Logix\n\n- Sunil Kumar | Global Business Development & Go-to-Market Lead at Amazon Web Services\n\n- Stephen Hudson | CEO of AI Tsunami\n\nEach panelist shared their real-world AI experiences, offering unique perspectives that enriched the discussion and sparked thought-provoking conversations.\n\nSome background on myself: My interest in AI began with ChatGPT's release, and I've been working with it ever since. Alongside my studies at Flagler, I work as an AI consultant with clients ranging from private equity firms to education companies and bridal shops. I won the 2024 Flagler Lion’s Cage competition with \"Suits.ai,\" an AI chatbot designed to simplify complex contracts and democratize the contract process for everyday people.\n\nAI in the modern era\n\nWhen we talk about “AI,” it’s crucial to distinguish between traditional AI and the newer wave of generative AI that’s taken the public by storm.\n\n“We need to make a distinction between the two,” Stephen Hudson said. “[Traditional] AI and machine learning have been around since the 50s [while] generative AI has only been around since [OpenAI’s GPT-3 model launched in 2020, GPT-4 in 2023].”"}
{"chunk_id": "www_flagler_edu__77ae53fb510874a9::c0001", "stable_id": "www_flagler_edu__77ae53fb510874a9", "url": "https://www.flagler.edu/news-events/news/ai-business-education-new-era-opportunity", "final_url": "https://www.flagler.edu/news-events/news/ai-business-education-new-era-opportunity", "retrieved_at": "2026-01-22T00:14:34.113914Z", "title": "AI in Business & Education: A new era of opportunity | Flagler College", "section": "How Businesses Are Using AI", "source_type": "University / Official", "content_hash": "8d9b6bc5dae7cffe831a6bdddc38cde3aaf2a2a71e592c13311fec9d214d2fb2", "chunk_index": 1, "char_start": 1402, "char_end": 2874, "text": "The internet revolutionized information access, and two decades later, generative AI has further empowered individuals to expand their knowledge and productivity.\n\nWe’ve been inundated with claims about how AI will streamline our lives, work, and education. Amid this rapid technological change, the reality is clear, and it involves a reckoning: If you’re not utilizing AI individually or in the workplace, you’re being left behind—or, at the very least, leaving a huge opportunity on the table.\n\nThe panelists emphasized AI's benefits for personal and professional life, from refining resumes with ChatGPT to guiding business ideas and developing minimum viable products (MVPs).\n\n“Even if you don’t know how to code, AI can be a tool for you to learn small things—like building a resume, preparing for interviews, or even learning how to code itself,” Sunil Kumar said. “There’s no better time to use these tools to take control of your future.\"\n\nMy take: Don’t be scared of AI! Historically, everyone who has been afraid of innovation and resisted it has been crushed by those who can adapt.\n\nAI in Education\n\nAI's role in education was another key theme. I don’t think it's a secret to anyone that students love to use ChatGPT to complete their homework. This is quite a concern for teachers and professors; they feel like students are no longer getting the most out of these assignments. I know many professors who now spend time trying to AI-proof their assignments."}
{"chunk_id": "www_flagler_edu__77ae53fb510874a9::c0002", "stable_id": "www_flagler_edu__77ae53fb510874a9", "url": "https://www.flagler.edu/news-events/news/ai-business-education-new-era-opportunity", "final_url": "https://www.flagler.edu/news-events/news/ai-business-education-new-era-opportunity", "retrieved_at": "2026-01-22T00:14:34.113914Z", "title": "AI in Business & Education: A new era of opportunity | Flagler College", "section": "How Businesses Are Using AI", "source_type": "University / Official", "content_hash": "8d9b6bc5dae7cffe831a6bdddc38cde3aaf2a2a71e592c13311fec9d214d2fb2", "chunk_index": 2, "char_start": 2874, "char_end": 4351, "text": "If you’re an educator in that boat, I’d venture to warn against wasted time. There’s no way around a tech-savvy student with access to an AI model. But more importantly, creating an assignment that forces students to not use AI may be counterproductive to the benefit of students. In my opinion, it would be much more beneficial to teach students how to utilize AI to learn more and push further.\n\n“Professors should be teaching students how to think—not just what to think,” Stephen Hudson said. “That’s the challenge now: helping students use AI to go deeper, spot patterns, and truly learn—not just regurgitate.”\n\nAI is like the calculator; it didn't make math obsolete but enabled more complex equations, pushing mathematics to new heights.\n\nWhen AI tools like ChatGPT make traditional assignments obsolete, it is necessary to go back to the first principle of what our education is supposed to do: to prepare students for the real world. The ability to think critically is the most important skill for students to achieve success.\n\nTheoretically, the playing field of information access has never been more level. So, how can you stand out? How can you take that information and apply it to your situation to make a difference or advance your goals? The challenge isn’t stopping misuse—it’s helping students learn how to apply AI critically and creatively rather than just regurgitating information. The focus should be on developing analytical and problem-solving skills."}
{"chunk_id": "www_flagler_edu__77ae53fb510874a9::c0003", "stable_id": "www_flagler_edu__77ae53fb510874a9", "url": "https://www.flagler.edu/news-events/news/ai-business-education-new-era-opportunity", "final_url": "https://www.flagler.edu/news-events/news/ai-business-education-new-era-opportunity", "retrieved_at": "2026-01-22T00:14:34.113914Z", "title": "AI in Business & Education: A new era of opportunity | Flagler College", "section": "How Businesses Are Using AI", "source_type": "University / Official", "content_hash": "8d9b6bc5dae7cffe831a6bdddc38cde3aaf2a2a71e592c13311fec9d214d2fb2", "chunk_index": 3, "char_start": 4351, "char_end": 5825, "text": "Most students are more familiar with the latest AI tools than professors, so professors should embrace AI as a collaborative tool rather than viewing it as a threat.\n\nFor example, professors can lead by example and leverage AI to provide more engaging and up-to-date course content, using real-world case studies and explicitly tailored examples instead of generic or overly broad textbook materials. AI can even help professors create more interactive lesson plans and learning experiences rather than just lecturing from outdated slides.\n\nMy take: AI should be seen as a collaborative tool in education, not a threat. Educators should focus on teaching students how to think critically and use AI to deepen their understanding and problem-solving skills, much like how calculators revolutionized math education. This also gives students an opportunity to have a highly transferrable and in-demand skill as they enter the workforce.\n\nAI in business\n\nAI in business was another crucial topic. The panelists underscored that organizations not using AI are falling behind, but incorporating AI doesn't require custom, expensive models.\n\nUsing AI can be as simple as leveraging pre-existing models to help write emails or optimize your calendar. Business platforms like Google Workspace and Microsoft 365 already integrate AI features into familiar apps like Gmail and Excel, enabling teams to gradually incorporate AI into their workflows without overhauling existing systems."}
{"chunk_id": "www_flagler_edu__77ae53fb510874a9::c0004", "stable_id": "www_flagler_edu__77ae53fb510874a9", "url": "https://www.flagler.edu/news-events/news/ai-business-education-new-era-opportunity", "final_url": "https://www.flagler.edu/news-events/news/ai-business-education-new-era-opportunity", "retrieved_at": "2026-01-22T00:14:34.113914Z", "title": "AI in Business & Education: A new era of opportunity | Flagler College", "section": "How Businesses Are Using AI", "source_type": "University / Official", "content_hash": "8d9b6bc5dae7cffe831a6bdddc38cde3aaf2a2a71e592c13311fec9d214d2fb2", "chunk_index": 4, "char_start": 5825, "char_end": 7029, "text": "AI is the next frontier in business, both for internal processes and for building external, consumer-facing products and marketing.\n\n“Think of AI as your personal executive assistant,” Hudson said. “It’s there to take 40% of the menial stuff off your plate so you can focus on what you were actually hired to do.”\n\nOne of the best internal use cases for AI is data analysis. Every business collects and utilizes data, whether it’s a restaurant tracking popular dishes or a tech company conducting market research for a new product. AI can analyze this data and draw inferences about what customers are most receptive to.\n\n“You don’t need to learn how to code,” Hudson said. “But you do need to understand how AI integrates into the tools your company already uses—whether that’s Excel, QuickBooks, or CRM systems. That’s where the real productivity gains are.”\n\nWith a simple chatbot like ChatGPT, you can drop in an Excel file and ask it to create a one-page summary of the data, complete with visuals and graphs, to share with your team or stakeholders in minutes. AI can also write advertising copy. Provide a chatbot with your organization's background and specify campaign goals to tailor responses."}
{"chunk_id": "www_flagler_edu__77ae53fb510874a9::c0005", "stable_id": "www_flagler_edu__77ae53fb510874a9", "url": "https://www.flagler.edu/news-events/news/ai-business-education-new-era-opportunity", "final_url": "https://www.flagler.edu/news-events/news/ai-business-education-new-era-opportunity", "retrieved_at": "2026-01-22T00:14:34.113914Z", "title": "AI in Business & Education: A new era of opportunity | Flagler College", "section": "How Businesses Are Using AI", "source_type": "University / Official", "content_hash": "8d9b6bc5dae7cffe831a6bdddc38cde3aaf2a2a71e592c13311fec9d214d2fb2", "chunk_index": 5, "char_start": 7029, "char_end": 8366, "text": "These are just a few straightforward ways to utilize AI that can drastically benefit your organization and workflows. While more complex applications can be extremely useful for the right organization, they require the necessary resources to create and effectively use the tools. Otherwise, the project could end up being a net negative.\n\nConsider this analogy: When trying to get a screw through a board, moving from using your hands to using a screwdriver is already a significant improvement. While an electric drill might be the best tool, it's useless if you don't know how to use it compared to what you can achieve with just a screwdriver.\n\nThis is where we, as students, have a great opportunity in front of us. If you can learn at least the basics of AI as a college student—the equivalent of using a screwdriver—you will be an asset to any organization. If you can advance to learn the “power tools” of AI and apply them effectively across situations, then you will be hard-pressed to find an organization that doesn’t want to hire you.\n\nMy take: AI is a powerful tool for businesses, from simplifying tasks to enhancing data analysis. Understanding how to integrate AI into your existing business can lead to significant productivity gains and is necessary for your business to stay competitive in the rapidly evolving market."}
{"chunk_id": "mitsloan_mit_edu__55f3ee2df411df14::c0000", "stable_id": "mitsloan_mit_edu__55f3ee2df411df14", "url": "https://mitsloan.mit.edu/ideas-made-to-matter/how-digital-business-models-are-evolving-age-agentic-ai", "final_url": "https://mitsloan.mit.edu/ideas-made-to-matter/how-digital-business-models-are-evolving-age-agentic-ai", "retrieved_at": "2026-01-22T00:14:46.067561Z", "title": "How digital business models are evolving in the age of agentic AI | MIT Sloan", "section": "How Businesses Are Using AI", "source_type": "University / Official", "content_hash": "c97b41130e3d90d4238f3fd5cca337518dc768beec64c262676c3835dd3439db", "chunk_index": 0, "char_start": 0, "char_end": 1201, "text": "Credit: Andrii Yalanskyi / Shutterstock\n\nHow digital business models are evolving in the age of agentic AI\n\nResearchers have identified four new business models for the era of agentic artificial intelligence:\n\n- Existing+. Augment an existing business model with AI.\n\n- Customer Proxy. Achieve customer outcomes through predefined processes executed by AI.\n\n- Modular Creator. Use AI to assemble reusable modules (including third parties) to assist in achieving customer outcomes, with no predetermined process.\n\n- Orchestrator. Achieve customer outcomes by using AI to assemble an ecosystem of complementary products and services, with no predetermined process.\n\n+++\n\nIf your enterprise is pivoting amid a changing technology landscape, rest assured that you’re not alone. A recent research brief from the MIT Center for Information Systems Research outlined how business models are evolving to keep pace with advances in artificial intelligence, and what it takes to successfully navigate change.\n\nThe original digital business models\n\nTo understand new business models for the AI era, it helps to unpack the old ones first. In 2013, MIT CISR researchers and identified four digital business models:"}
{"chunk_id": "mitsloan_mit_edu__55f3ee2df411df14::c0001", "stable_id": "mitsloan_mit_edu__55f3ee2df411df14", "url": "https://mitsloan.mit.edu/ideas-made-to-matter/how-digital-business-models-are-evolving-age-agentic-ai", "final_url": "https://mitsloan.mit.edu/ideas-made-to-matter/how-digital-business-models-are-evolving-age-agentic-ai", "retrieved_at": "2026-01-22T00:14:46.067561Z", "title": "How digital business models are evolving in the age of agentic AI | MIT Sloan", "section": "How Businesses Are Using AI", "source_type": "University / Official", "content_hash": "c97b41130e3d90d4238f3fd5cca337518dc768beec64c262676c3835dd3439db", "chunk_index": 1, "char_start": 1201, "char_end": 2410, "text": "- Supplier companies, which sell products through third parties, like manufacturers.\n\n- Omnichannel companies, which have a digital and physical presence, such as retailers and banks.\n\n- Modular Producers, which offer plug-and-play products or services, such as payment service providers.\n\n- Ecosystem Drivers, which offer a go-to destination in a given customer domain (e.g., housing) and connect customers with providers.\n\nThese models have seen significant shifts in the past 12 years, with companies that lead or otherwise participate in a digital ecosystem becoming far more prevalent than traditional brick-and-mortar sellers. Focusing on firms’ dominant models, supplier and omnichannel business models are much less prevalent today, while companies with ecosystem driver business models have grown from 12% of businesses in 2013 to 58% of businesses in 2025. In large part, this is because these companies were the only ones of the four to exceed industry-average revenue growth.\n\nThese shifts, coupled with rapid adoption of AI in all its forms — machine learning plus agentic, generative, and robotic AI — prompted the development of a new business model framework.\n\n4 business models for the AI era"}
{"chunk_id": "mitsloan_mit_edu__55f3ee2df411df14::c0002", "stable_id": "mitsloan_mit_edu__55f3ee2df411df14", "url": "https://mitsloan.mit.edu/ideas-made-to-matter/how-digital-business-models-are-evolving-age-agentic-ai", "final_url": "https://mitsloan.mit.edu/ideas-made-to-matter/how-digital-business-models-are-evolving-age-agentic-ai", "retrieved_at": "2026-01-22T00:14:46.067561Z", "title": "How digital business models are evolving in the age of agentic AI | MIT Sloan", "section": "How Businesses Are Using AI", "source_type": "University / Official", "content_hash": "c97b41130e3d90d4238f3fd5cca337518dc768beec64c262676c3835dd3439db", "chunk_index": 2, "char_start": 2410, "char_end": 3881, "text": "For the update, Weill, Woerner, and colleagues and Gayan Benedict used survey data obtained from 2,378 companies between 2013 and 2025 to organize business models into four new categories. They used the example of a hypothetical financial services company to describe how the business models operate in theory.\n\n- Existing+: These firms augment an existing business model with AI. Here, a financial services company could enhance the traditional advisory process by using AI to analyze customer information and provide personalized recommendations.\n\n- Customer Proxy: These firms achieve customer outcomes (within guardrails) using predefined processes now supported by AI. In this case, a financial services company could set parameters to automatically manage a customer’s investment portfolio.\n\n- Modular Creator: Much like producers of plug-and-play products, these firms use AI to assemble reusable modules (including those from third parties) into tailored service bundles. Applying this model, a financial services company could create and recommend a bundle of investment, insurance, and credit products that align with a customer’s goals.\n\n- Orchestrator: These firms achieve customer outcomes (within guardrails) by using AI to assemble an ecosystem of complementary products and services. In this case, a financial services company could provide a fully managed wealth solution that automatically and continuously optimizes the customer’s investment portfolio."}
{"chunk_id": "mitsloan_mit_edu__55f3ee2df411df14::c0003", "stable_id": "mitsloan_mit_edu__55f3ee2df411df14", "url": "https://mitsloan.mit.edu/ideas-made-to-matter/how-digital-business-models-are-evolving-age-agentic-ai", "final_url": "https://mitsloan.mit.edu/ideas-made-to-matter/how-digital-business-models-are-evolving-age-agentic-ai", "retrieved_at": "2026-01-22T00:14:46.067561Z", "title": "How digital business models are evolving in the age of agentic AI | MIT Sloan", "section": "How Businesses Are Using AI", "source_type": "University / Official", "content_hash": "c97b41130e3d90d4238f3fd5cca337518dc768beec64c262676c3835dd3439db", "chunk_index": 3, "char_start": 3881, "char_end": 5131, "text": "How One New Zealand Group has evolved its business model\n\nThe ongoing transformation of telecommunications provider One New Zealand Group illustrates these business models in action. Currently, for example, the company uses AI agents to help answer customers’ frequently asked questions and assist employees in serving customers (the Existing+ model); act on requests to upgrade plans or create service tickets (Customer Proxy); and monitor power failures, forecast demand, and recommend action during weather-related service disruptions (Modular Curator).\n\nLooking ahead, One NZ intends to bring autonomous AI agents to marketing operations (Orchestrator). Agents would be capable of creating personalized campaigns and adapting them based on how customers respond. The marketing team would set goals and guardrails for the AI agents and monitor their performance.\n\nCompanies seeking to adapt the way that One NZ has need to understand where they can create value, according to the researchers. Does your company merely assist customers, or can it represent their goals through autonomous action? Is business execution built on a structured process, or can that process be adapted, with the help of AI agents, based on a customer’s desired outcomes?"}
{"chunk_id": "mitsloan_mit_edu__55f3ee2df411df14::c0004", "stable_id": "mitsloan_mit_edu__55f3ee2df411df14", "url": "https://mitsloan.mit.edu/ideas-made-to-matter/how-digital-business-models-are-evolving-age-agentic-ai", "final_url": "https://mitsloan.mit.edu/ideas-made-to-matter/how-digital-business-models-are-evolving-age-agentic-ai", "retrieved_at": "2026-01-22T00:14:46.067561Z", "title": "How digital business models are evolving in the age of agentic AI | MIT Sloan", "section": "How Businesses Are Using AI", "source_type": "University / Official", "content_hash": "c97b41130e3d90d4238f3fd5cca337518dc768beec64c262676c3835dd3439db", "chunk_index": 4, "char_start": 5131, "char_end": 6483, "text": "Leaders looking to understand the opportunities AI offers their company can start by identifying existing AI-enabled business models that they can scale, and the corresponding AI capabilities a company needs to build.\n\nRead the research briefing: “Business models in the AI era“\n\nGenerative AI Business Sprint\n\nAttend Online\n\nREGISTER NOW\n\nThis article is based on research by Peter Weill, Ina Sebastian, Stephanie Woerner, and Gayan Benedict from the MIT Center for Information Systems Research.\n\nPeter Weill is a senior research scientist at MIT Sloan and chairman of MIT CISR. His work explores future trends, such as digital business models, IT investment portfolios, and AI maturity models, to help organizations maintain a competitive edge. Ina Sebastian is a research scientist at MIT CISR. She studies how large enterprises transform for success in the digital economy, with a focus on digital partnering, value creation, and value capture in digital models. Stephanie Woerner is a principal research scientist at MIT Sloan and the director of MIT CISR. She studies how companies use technology and data to make more effective business models, as well as how they manage associated organizational change, governance, and strategy implications. Gayan Benedict is an industry research fellow at MIT CISR and a technology partner at PwC Australia."}
{"chunk_id": "www_artificialintelligence_news_com__f00bf123dd759632::c0000", "stable_id": "www_artificialintelligence_news_com__f00bf123dd759632", "url": "https://www.artificialintelligence-news.com/news/kroger-and-lowe-test-ai-agents-without-handing-control-to-google/", "final_url": "https://www.artificialintelligence-news.com/news/kroger-and-lowe-test-ai-agents-without-handing-control-to-google/", "retrieved_at": "2026-01-22T00:14:01.176574Z", "title": "Kroger and Lowe’s test AI agents without handing control to Google", "section": "How Businesses Are Using AI", "source_type": "News / Update", "content_hash": "9418be754e6b13e77527db078bdeff00df752b030575f78309df00741662b954", "chunk_index": 0, "char_start": 0, "char_end": 1404, "text": "Retailers are starting to confront a problem that sits behind much of the hype around AI shopping: as customers turn to chatbots and automated assistants to decide what to buy, retailers risk losing control over how their products are shown, sold, and bundled.\n\nThat concern is pushing some large chains to build or support their own AI-powered shopping tools, rather than relying only on third-party platforms. The goal is not to chase novelty, but to stay close to customers as buying decisions shift toward automation.\n\nSeveral retailers, including Lowe’s, Kroger, and Papa Johns, are experimenting with AI agents that can help shoppers search for items, get support, or place orders. Many of these efforts are backed by tools from Google, which is offering retailers a way to deploy agents inside their own apps and websites instead of sending customers elsewhere.\n\nKeeping control as shopping shifts toward automation\n\nFor grocers like Kroger, the concern is not whether AI will influence shopping, but how quickly it might do so. The company is testing an AI shopping agent that can compare items, handle purchases, and adjust suggestions based on customer habits and needs.\n\n“Things are moving at a pace that if you’re not already deep into [AI agents], you’re probably creating a competitive barrier or disadvantage,” said Yael Cosset, Kroger’s chief digital officer and executive vice president."}
{"chunk_id": "www_artificialintelligence_news_com__f00bf123dd759632::c0001", "stable_id": "www_artificialintelligence_news_com__f00bf123dd759632", "url": "https://www.artificialintelligence-news.com/news/kroger-and-lowe-test-ai-agents-without-handing-control-to-google/", "final_url": "https://www.artificialintelligence-news.com/news/kroger-and-lowe-test-ai-agents-without-handing-control-to-google/", "retrieved_at": "2026-01-22T00:14:01.176574Z", "title": "Kroger and Lowe’s test AI agents without handing control to Google", "section": "How Businesses Are Using AI", "source_type": "News / Update", "content_hash": "9418be754e6b13e77527db078bdeff00df752b030575f78309df00741662b954", "chunk_index": 1, "char_start": 1404, "char_end": 2686, "text": "The agent, which sits inside Kroger’s mobile app, can take into account factors such as time limits or meal plans, while also drawing on data the retailer already has, including price sensitivity and brand preferences. The intent is to keep those decisions within Kroger’s own systems rather than handing them off to external platforms.\n\nThat approach reflects a wider tension in retail. Making products available directly inside large AI chatbots can widen reach, but it can also weaken customer loyalty, reduce add-on sales, and cut into advertising revenue. Once a third party controls the interface, retailers have less say in how choices are framed.\n\nThis is one reason some retailers are cautious about selling directly through tools built by companies like OpenAI or Microsoft. Both have rolled out features that allow users to complete purchases inside their chatbots, and last year Walmart said it would work with OpenAI to let customers buy items through ChatGPT.\n\nFor retailers, the appeal of running their own agents is control. “There’s a market shift across the spectrum of retailers who are investing in their own capabilities rather than just relying on third-parties,” said Lauren Wiener, a global leader of marketing and customer growth at Boston Consulting Group."}
{"chunk_id": "www_artificialintelligence_news_com__f00bf123dd759632::c0002", "stable_id": "www_artificialintelligence_news_com__f00bf123dd759632", "url": "https://www.artificialintelligence-news.com/news/kroger-and-lowe-test-ai-agents-without-handing-control-to-google/", "final_url": "https://www.artificialintelligence-news.com/news/kroger-and-lowe-test-ai-agents-without-handing-control-to-google/", "retrieved_at": "2026-01-22T00:14:01.176574Z", "title": "Kroger and Lowe’s test AI agents without handing control to Google", "section": "How Businesses Are Using AI", "source_type": "News / Update", "content_hash": "9418be754e6b13e77527db078bdeff00df752b030575f78309df00741662b954", "chunk_index": 2, "char_start": 2686, "char_end": 4017, "text": "Why retailers are spreading risk across vendors\n\nStill, building and maintaining these systems is not simple. The underlying models change quickly, and tools that work today may need reworking weeks later. That reality is shaping how retailers think about vendors.\n\nAt Lowe’s, Google’s shopping agent sits behind the retailer’s own virtual assistant, Mylow. When customers use Mylow online, the company says conversion rates more than double. But Lowe’s does not rely on a single provider.\n\n“The tech we build can become outdated in two weeks,” said Seemantini Godbole, Lowe’s chief digital and information officer. That pace is one reason Lowe’s works with several vendors, including OpenAI, rather than betting on one system.\n\nKroger is taking a similar approach. Alongside Google, it works with companies such as Instacart to support its agent strategy. “[AI agents] are not just top of mind, it’s a priority for us,” Cosset said. “It’s going at a remarkable pace.”\n\nTesting AI agents without overcommitting\n\nFor others, the challenge is not keeping up with the technology, but deciding how much to build at all. Papa Johns does not create its own AI models or agents. Instead, it is testing Google’s food ordering agent to handle tasks like estimating how many pizzas a group might need based on a photo uploaded by a customer."}
{"chunk_id": "www_artificialintelligence_news_com__f00bf123dd759632::c0003", "stable_id": "www_artificialintelligence_news_com__f00bf123dd759632", "url": "https://www.artificialintelligence-news.com/news/kroger-and-lowe-test-ai-agents-without-handing-control-to-google/", "final_url": "https://www.artificialintelligence-news.com/news/kroger-and-lowe-test-ai-agents-without-handing-control-to-google/", "retrieved_at": "2026-01-22T00:14:01.176574Z", "title": "Kroger and Lowe’s test AI agents without handing control to Google", "section": "How Businesses Are Using AI", "source_type": "News / Update", "content_hash": "9418be754e6b13e77527db078bdeff00df752b030575f78309df00741662b954", "chunk_index": 3, "char_start": 4017, "char_end": 5386, "text": "Customers will be able to use the agent by phone, through the company’s website, or in its app. “I don’t want to be an AI expert in terms of building the agents,” said Kevin Vasconi, Papa Johns’ chief digital and technology officer. “I want to be an AI expert in terms of, ‘How do I use the agents?’”\n\nThat focus on use rather than ownership reflects a practical view of where AI fits today. While agent-based shopping is gaining attention, it is not yet the main way people buy everyday goods.\n\n“I don’t think [AI agents] are going to totally change the industry,” Vasconi said. “People still call our stores on the phone to order pizza in this day and age.”\n\nAnalysts see Google’s tools less as a finished answer and more as a way to lower the barrier for retailers that do not want to start from scratch. “The real challenge here is application of the technologies,” said Ed Anderson, a tech analyst at Gartner. “These announcements take a step forward so that retailers don’t have to start from ground zero.”\n\nFor now, retailers are testing, mixing vendors, and holding back from firm commitments. Kroger, Lowe’s, and Papa Johns have not shared detailed results from their trials. That caution suggests many are still trying to understand how much control they are willing to give up—and how much they can afford to keep—as shopping slowly shifts toward automation."}
{"chunk_id": "www_artificialintelligence_news_com__f00bf123dd759632::c0004", "stable_id": "www_artificialintelligence_news_com__f00bf123dd759632", "url": "https://www.artificialintelligence-news.com/news/kroger-and-lowe-test-ai-agents-without-handing-control-to-google/", "final_url": "https://www.artificialintelligence-news.com/news/kroger-and-lowe-test-ai-agents-without-handing-control-to-google/", "retrieved_at": "2026-01-22T00:14:01.176574Z", "title": "Kroger and Lowe’s test AI agents without handing control to Google", "section": "How Businesses Are Using AI", "source_type": "News / Update", "content_hash": "9418be754e6b13e77527db078bdeff00df752b030575f78309df00741662b954", "chunk_index": 4, "char_start": 5386, "char_end": 5858, "text": "(Photo by Heidi Fin)\n\nSee also: Grab brings robotics in-house to manage delivery costs\n\nWant to learn more about AI and big data from industry leaders? Check outAI & Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events, click here for more information.\n\nAI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here."}
{"chunk_id": "www_ibm_com__c0f7a40a9c452b3a::c0000", "stable_id": "www_ibm_com__c0f7a40a9c452b3a", "url": "https://www.ibm.com/think/topics/artificial-intelligence-business", "final_url": "https://www.ibm.com/think/topics/artificial-intelligence-business", "retrieved_at": "2026-01-22T00:14:35.823870Z", "title": "What is Artificial Intelligence (AI) in Business? | IBM", "section": "How Businesses Are Using AI", "source_type": "Explainer / Guide", "content_hash": "e596eba2ff12588f8c4827a64ab9f4a8f5eef6214b1da302569d698eaf620ff2", "chunk_index": 0, "char_start": 0, "char_end": 1336, "text": "Artificial intelligence in business is the use of AI tools such as machine learning, natural language processing, and computer vision to optimize business functions, boost employee productivity, and drive business value.\n\nArtificial intelligence, or the development of computer systems and machine learning to mimic the problem-solving and decision-making capabilities of human intelligence, impacts an array of business processes. Organizations use artificial intelligence (AI) to strengthen data analysis and decision-making, improve customer experiences, generate content, optimize IT operations, sales, marketing and cybersecurity practices and more. As AI technologies improve and evolve, new business applications emerge.\n\nArtificial intelligence is used as a tool to support a human workforce in optimizing workflows and making business operations more efficient. These gains are made in various ways, including using AI to automate repetitive tasks, generate information based on machine learning algorithms, quickly process vast amounts of data sets and extract meaningful insights, and predict future outcomes based on data analysis. AI systems power several types of business automation, including enterprise automation and process automation, helping to reduce human error and free up human workforces for higher-level work."}
{"chunk_id": "www_ibm_com__c0f7a40a9c452b3a::c0001", "stable_id": "www_ibm_com__c0f7a40a9c452b3a", "url": "https://www.ibm.com/think/topics/artificial-intelligence-business", "final_url": "https://www.ibm.com/think/topics/artificial-intelligence-business", "retrieved_at": "2026-01-22T00:14:35.823870Z", "title": "What is Artificial Intelligence (AI) in Business? | IBM", "section": "How Businesses Are Using AI", "source_type": "Explainer / Guide", "content_hash": "e596eba2ff12588f8c4827a64ab9f4a8f5eef6214b1da302569d698eaf620ff2", "chunk_index": 1, "char_start": 1336, "char_end": 2874, "text": "According to McKinsey & Company, the use of artificial intelligence in business operations has doubled since 2017.1 This is largely because AI technology can be customized to meet an organization’s unique needs. 63% of McKinsey’s respondents expect their investment in AI technologies to increase over the next three years.2 To use AI in an effective business strategy, an organization must have a clear understanding of its business functions, how AI works and what aspects of the business can be improved through AI implementation.\n\nWhile the use of AI tools to automate repetitive tasks and increase employee productivity remains popular, businesses are also moving beyond these use cases and using AI to assist in higher-level, strategic initiatives that help drive broader business value.\n\nThink Newsletter\n\nStay up to date on the most important—and intriguing—industry trends on AI, automation, data and beyond with the Think newsletter. See the IBM Privacy Statement.\n\nYour subscription will be delivered in English. You will find an unsubscribe link in every newsletter. You can manage your subscriptions or unsubscribe here. Refer to our IBM Privacy Statement for more information.\n\nArtificial intelligence, “the science and engineering of making intelligent machines, especially intelligent computer programs,”3 uses large amounts of data and human knowledge to power computer systems with the ability to categorize data, make predictions, identify errors, have conversations, and analyze information in a similar way to humans."}
{"chunk_id": "www_ibm_com__c0f7a40a9c452b3a::c0002", "stable_id": "www_ibm_com__c0f7a40a9c452b3a", "url": "https://www.ibm.com/think/topics/artificial-intelligence-business", "final_url": "https://www.ibm.com/think/topics/artificial-intelligence-business", "retrieved_at": "2026-01-22T00:14:35.823870Z", "title": "What is Artificial Intelligence (AI) in Business? | IBM", "section": "How Businesses Are Using AI", "source_type": "Explainer / Guide", "content_hash": "e596eba2ff12588f8c4827a64ab9f4a8f5eef6214b1da302569d698eaf620ff2", "chunk_index": 2, "char_start": 2874, "char_end": 4351, "text": "One of the goals of artificial intelligence is to create computer systems that can mimic the critical thinking skills of humans. These systems rely on business data and use technologies like natural language processing (NLP), machine learning (ML), and deep learning to facilitate business operations. Integrating AI into business functions requires a baseline understanding of the following components:\n\nThese algorithms are a subset of artificial intelligence and are used to make predictions or classifications based on input data. Through training data sets, these algorithms can learn to identify patterns, discover anomalies, or make projections such as future sales revenue. Machine learning algorithms help mine large datasets for key insights that can offer real-world benefits for improved business decisions. Machine learning algorithms benefit from labeled data, which is data that a human expert categorizes before it is processed.\n\nDeep learning is a subset of machine learning that allows for the automation of tasks without human intervention. Virtual assistants, chatbots, facial recognition and fraud prevention technology all rely on deep learning. By examining data that is related to user behavior, deep learning models can make predictions about future behavior. Compared to general machine learning, deep learning models can more accurately extract information from unstructured data such as text and images and do not require as much human intervention."}
{"chunk_id": "www_ibm_com__c0f7a40a9c452b3a::c0003", "stable_id": "www_ibm_com__c0f7a40a9c452b3a", "url": "https://www.ibm.com/think/topics/artificial-intelligence-business", "final_url": "https://www.ibm.com/think/topics/artificial-intelligence-business", "retrieved_at": "2026-01-22T00:14:35.823870Z", "title": "What is Artificial Intelligence (AI) in Business? | IBM", "section": "How Businesses Are Using AI", "source_type": "Explainer / Guide", "content_hash": "e596eba2ff12588f8c4827a64ab9f4a8f5eef6214b1da302569d698eaf620ff2", "chunk_index": 3, "char_start": 4351, "char_end": 5724, "text": "Natural language processing is a branch of AI that “enables computers and digital devices to recognize, understand, and generate text and speech.”4 Customer support chatbots, digital assistants, and voice-operated technologies such as GPS systems are all powered by NLP. Used with machine learning algorithms and deep learning models, NLP allows systems to extract insights from unstructured data that are text- or voice-driven.\n\nComputer vision is a subset of AI that allows computer systems to extract information from digital images, videos and other visual inputs.5 Computer vision uses both deep learning and machine learning algorithms to learn and identify specific elements of digital imagery. Computer vision is currently applied in several ways, and applications are expanding as the technology progresses. For example, computer vision can be implemented in production lines to detect minor defects during the manufacturing process.\n\nIntegrating enterprise-grade AI can help free human workforces from repetitive manual tasks, improve data analysis, business strategy and decision-making, and optimize processes organization-wide. To do so, enterprises must have an infrastructure that properly manages data and supports AI technology. Having a strong data governance framework helps keep data available to all relevant stakeholders and secure from data breaches."}
{"chunk_id": "www_ibm_com__c0f7a40a9c452b3a::c0004", "stable_id": "www_ibm_com__c0f7a40a9c452b3a", "url": "https://www.ibm.com/think/topics/artificial-intelligence-business", "final_url": "https://www.ibm.com/think/topics/artificial-intelligence-business", "retrieved_at": "2026-01-22T00:14:35.823870Z", "title": "What is Artificial Intelligence (AI) in Business? | IBM", "section": "How Businesses Are Using AI", "source_type": "Explainer / Guide", "content_hash": "e596eba2ff12588f8c4827a64ab9f4a8f5eef6214b1da302569d698eaf620ff2", "chunk_index": 4, "char_start": 5724, "char_end": 6975, "text": "It also helps promote the use of advanced data analytics. Part of this framework involves a digital transformation and the integration of hybrid cloud and multicloud environments to help manage large volumes of data. Once these systems are in place, an organization can begin mining data for insights and building training models to instruct AI technologies.\n\nAs new technologies enter the market, and existing ones improve, the possible applications of artificial intelligence in business grow more numerous. The benefits of AI vary and require the integration of technologies and human workforces to improve operational efficiency and drive business value.\n\nSome examples that demonstrate the use of artificial intelligence in business include:\n\nAIOps—artificial intelligence for IT operations—consists of the practice of using AI, machine learning and natural language processing models to streamline IT operations and service management. AIOps allows IT teams to quickly sift through large amounts of data and reduce the amount of time it takes to detect anomalies, troubleshoot errors, and monitor the performance of IT systems. Artificial intelligence helps IT teams achieve greater observability and provides real-time insights into operations."}
{"chunk_id": "www_ibm_com__c0f7a40a9c452b3a::c0005", "stable_id": "www_ibm_com__c0f7a40a9c452b3a", "url": "https://www.ibm.com/think/topics/artificial-intelligence-business", "final_url": "https://www.ibm.com/think/topics/artificial-intelligence-business", "retrieved_at": "2026-01-22T00:14:35.823870Z", "title": "What is Artificial Intelligence (AI) in Business? | IBM", "section": "How Businesses Are Using AI", "source_type": "Explainer / Guide", "content_hash": "e596eba2ff12588f8c4827a64ab9f4a8f5eef6214b1da302569d698eaf620ff2", "chunk_index": 5, "char_start": 6975, "char_end": 8191, "text": "Customer data helps marketing teams develop marketing strategies by identify trends and spending patterns. Artificial intelligence tools help process these big data sets to forecast future spending trends and conduct competitor analysis. This helps an organization gain a deeper understanding of its place in the market.\n\nAI tools allow for marketing segmentation, a strategy that uses data to tailor marketing campaigns to specific customers based on their interests. Sales teams can use this same data to make product recommendations based on customer analytics.\n\nAI enables businesses to provide 24/7 customer service and faster response times, which help improve the customer experience. AI-powered chatbots can help customers resolve simple queries without requiring a human agent. This ability allows the human customer service workforce to address more complex issues.\n\nMcKinsey reported savings of USD 80 million for a South American telecommunications company that used conversational AI to prioritize higher-value clients.6 Powerful conversational AI tools such as IBM watsonx™ Assistant help chatbots overcome some of the pain points of earlier models, which were unable to handle many customer questions."}
{"chunk_id": "www_ibm_com__c0f7a40a9c452b3a::c0006", "stable_id": "www_ibm_com__c0f7a40a9c452b3a", "url": "https://www.ibm.com/think/topics/artificial-intelligence-business", "final_url": "https://www.ibm.com/think/topics/artificial-intelligence-business", "retrieved_at": "2026-01-22T00:14:35.823870Z", "title": "What is Artificial Intelligence (AI) in Business? | IBM", "section": "How Businesses Are Using AI", "source_type": "Explainer / Guide", "content_hash": "e596eba2ff12588f8c4827a64ab9f4a8f5eef6214b1da302569d698eaf620ff2", "chunk_index": 6, "char_start": 8191, "char_end": 9563, "text": "Generative AI (GenAI) is a growing field that helps organizations optimize content creation. Tools such as ChatGPT provide content teams with powerful tools to create original content. These tools can generate images or text based on input prompts, and designers, writers, and content leads can use these generative AI outputs to help with brainstorming, outlining, and other project tasks. Gartner estimates that by 2025 generative AI will be used to create 30% of outbound marketing content, up from 2% in 2022.7 Generative tools such as IBM watsonx™ Code Assistant can help developers by generating code.\n\nWhile AI content generation is still largely unregulated, human employees should monitor the use of AI in generating content to prevent copyright infringement, the publication of misinformation, or other unethical business practices.\n\nArtificial intelligence tools can be used to improve network security, anomaly detection, fraud detection, and help prevent data breaches. The increased use of technology in the workplace creates greater opportunities for security breaches; to thwart threats and protect organizational and customer data, organizations must be proactive in detecting anomalies. For example, deep learning models can be used to examine large sets of network traffic data and identify behavior that might signal an attempted attack on the network."}
{"chunk_id": "www_ibm_com__c0f7a40a9c452b3a::c0007", "stable_id": "www_ibm_com__c0f7a40a9c452b3a", "url": "https://www.ibm.com/think/topics/artificial-intelligence-business", "final_url": "https://www.ibm.com/think/topics/artificial-intelligence-business", "retrieved_at": "2026-01-22T00:14:35.823870Z", "title": "What is Artificial Intelligence (AI) in Business? | IBM", "section": "How Businesses Are Using AI", "source_type": "Explainer / Guide", "content_hash": "e596eba2ff12588f8c4827a64ab9f4a8f5eef6214b1da302569d698eaf620ff2", "chunk_index": 7, "char_start": 9563, "char_end": 10883, "text": "Data breaches can be costly and erode customer trust. The IBM Cost of a Data Breach Report 2023 indicates that the average savings for organizations that “use security AI and automation extensively is USD 1.76 million compared to organizations that don’t.”\n\nThe application of AI in supply chain management comes in the form of predictive analytics, which helps forecast future pricing of shipping and material costs. Predictive analytics also helps organizations maintain appropriate levels of inventory. This reduces bottlenecks, or the overstocking of products.\n\nAI technologies are rapidly evolving, and their use is expanding to meet a wider variety of business needs and strategies. New technologies and the innovation of business leaders will dictate the future of AI—understanding how AI fits into your business model is key to maintaining a competitive edge.\n\n1, 2 \"The state of AI in 2022—and a half decade in review\". McKinsey & Company. December 6, 2022\n\n3 \"What is artificial intelligence?\". IBM.com\n\n4 \"What is natural language processing?\". IBM.com\n\n5 \"What is computer vision?\". IBM.com\n\n6 \"Generative AI will first be successfully scaled in business operations\". Marie El Hoyek, Curt Mueller, Nicolai Müller. McKinsey & Company. February 5, 2024.\n\n7 \"What Generative AI Means for Business\". gartner.com."}
{"chunk_id": "www_nist_gov__590e8afe152537ff::c0000", "stable_id": "www_nist_gov__590e8afe152537ff", "url": "https://www.nist.gov/artificial-intelligence/ai-standards-federal-engagement", "final_url": "https://www.nist.gov/artificial-intelligence/ai-standards-federal-engagement", "retrieved_at": "2026-01-22T00:15:04.661266Z", "title": "AI Standards: Federal Engagement | NIST", "section": "Rules, Risks, and Ethics of AI", "source_type": "University / Official", "content_hash": "04c4b604272219096f060308763628bb69e32f72dda19c6f0bff227ee801d6a7", "chunk_index": 0, "char_start": 0, "char_end": 1443, "text": "Federal agencies engaged in developing standards for artificial intelligence (AI) either because these activities are part of their assigned responsibilities or because AI is essential to their current or evolving missions. Executive Order (EO) 13859 directed agencies to ensure that “technical standards minimize vulnerability to attacks from malicious actors and reflect federal priorities for innovation, public trust, and public confidence in systems that use AI technologies” and to “develop international standards to promote and protect those priorities.”\n\nNIST involved stakeholders from the private and public sectors in developing the U.S. Leadership in AI: A Plan for Federal Engagement in Developing Technical Standards and Related Tool, which was released in August 2019. The Plan provided guidance regarding priorities and appropriate levels of engagement in AI-standards-related matters. It also recommended that the “Federal Government should commit to deeper, consistent, long-term engagement in AI standards development activities to help the United States to speed the pace of reliable, robust, and trustworthy AI technology development.”\n\nSince then, agencies which develop or use AI have made progress in bolstering AI standards-related knowledge, leadership, and coordination; promoted focused research on the trustworthiness of AI systems; supported and expanded public-private partnerships; and engaged internationally."}
{"chunk_id": "www_nist_gov__590e8afe152537ff::c0001", "stable_id": "www_nist_gov__590e8afe152537ff", "url": "https://www.nist.gov/artificial-intelligence/ai-standards-federal-engagement", "final_url": "https://www.nist.gov/artificial-intelligence/ai-standards-federal-engagement", "retrieved_at": "2026-01-22T00:15:04.661266Z", "title": "AI Standards: Federal Engagement | NIST", "section": "Rules, Risks, and Ethics of AI", "source_type": "University / Official", "content_hash": "04c4b604272219096f060308763628bb69e32f72dda19c6f0bff227ee801d6a7", "chunk_index": 1, "char_start": 1443, "char_end": 2957, "text": "Notable steps by agencies include:\n\n- Established the role of Federal AI Standards Coordinator with responsibility to gather and share AI standards-related needs, strategics, roadmaps, terminology, use cases, and best practices in support of reliable, robust, and trustworthy AI in government operations. This responsibility resides with NIST.\n\n- Created the AI Standards Coordination Working Group (AISCWG) to facilitate agency activities related to development and use of AI standards. Working under the charter of the Interagency Committee on Standards Policy (ICSP), and aligning its activities with the Federal AI Standards Coordinator, the AISCWG is responsible for promoting effective and consistent federal policies leveraging AI standards cited in the AI Standards Plan\n\n- Multiple agencies are reviewing options to better position the Federal Government to gain access to new employees and to develop current employees to meet rapidly growing AI-capable workforce needs. That includes aiming to develop and provide a clear career development and promotion path that values and encourages participation in and expertise in AI standards and standards development.\n\n- The National Defense Authorization Action of 2021 (NDAA) explicitly authorized NIST to carry out a wide range of AI standards-related functions. Over the past two years, NIST has expanded and made noteworthy progress in carrying out research that specifically addresses standards-oriented research recommendations in the AI Standards Plan."}
{"chunk_id": "www_nist_gov__590e8afe152537ff::c0002", "stable_id": "www_nist_gov__590e8afe152537ff", "url": "https://www.nist.gov/artificial-intelligence/ai-standards-federal-engagement", "final_url": "https://www.nist.gov/artificial-intelligence/ai-standards-federal-engagement", "retrieved_at": "2026-01-22T00:15:04.661266Z", "title": "AI Standards: Federal Engagement | NIST", "section": "Rules, Risks, and Ethics of AI", "source_type": "University / Official", "content_hash": "04c4b604272219096f060308763628bb69e32f72dda19c6f0bff227ee801d6a7", "chunk_index": 2, "char_start": 2957, "char_end": 4024, "text": "- The National Science Foundation (NSF) is supporting several grant programs related to AI trustworthiness. Among them is the National AI Research Institutes program, which includes a growing number of partnerships with federal agencies and private companies.\n\n- Strategic engagement in international AI standards was the focus of the U.S. Department of State’s submission to Congress of “A Plan to Establish Exchanges and Partnerships between the United States and Its Allies to Create Standards for Artificial Intelligence Technologies.” The US championed development of the first international principles for the responsible use of AI at the Organisation for Economic Co-operation and Development (OECD). Also, the US became a founding member of the Global Partnership on AI (GPAI). Through engagement in GPAI, the United States seeks to complement the more policy-oriented work of the OECD by increasing coordination on research and development and scaling up practical projects for implementing trustworthy AI.\n\nFor background and details about the plan go here."}
{"chunk_id": "www_aaup_org__920dd7ff9e19f3c6::c0000", "stable_id": "www_aaup_org__920dd7ff9e19f3c6", "url": "https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic", "final_url": "https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic", "retrieved_at": "2026-01-22T00:14:29.028278Z", "title": "Artificial Intelligence and Academic Professions | AAUP", "section": "Rules, Risks, and Ethics of AI", "source_type": "University / Official", "content_hash": "b6d82a5ea40faba092f7a153ce0e030c344b28aef200d5b1c6a3ea04da841c00", "chunk_index": 0, "char_start": 0, "char_end": 1216, "text": "Executive Summary\n\nEducational technology, or ed-tech, including artificial intelligence (AI), continues to become more integrated into teaching and research in higher education, with minimal oversight. The AAUP’s ad hoc Committee on Artificial Intelligence and Academic Professions—composed of higher education faculty members, staff, and scholars interested in technology and its impact on academic labor—was formed under the assumption that faculty members are best positioned to understand and improve teaching and learning conditions, including the development and implementation of institutional policies around educational technology.\n\nTo learn more about the experiences and priorities of AAUP members, the committee conducted a survey with a sample of five hundred members from nearly two hundred campuses across the country, collected during a two-week time period. Respondents emphasized the importance of improving education on AI, promoting shared governance through policies and oversight, and focusing on equity, transparency, and worker protections. Based on those responses, the committee identified the five key concerns listed below and described more fully in the findings section of this report."}
{"chunk_id": "www_aaup_org__920dd7ff9e19f3c6::c0001", "stable_id": "www_aaup_org__920dd7ff9e19f3c6", "url": "https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic", "final_url": "https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic", "retrieved_at": "2026-01-22T00:14:29.028278Z", "title": "Artificial Intelligence and Academic Professions | AAUP", "section": "Rules, Risks, and Ethics of AI", "source_type": "University / Official", "content_hash": "b6d82a5ea40faba092f7a153ce0e030c344b28aef200d5b1c6a3ea04da841c00", "chunk_index": 1, "char_start": 1216, "char_end": 2532, "text": "1. Improving Professional Development Regarding AI and Technology Harms\n\n- Despite the widespread use of ed-tech, there is an overall lack of understanding about the relationship between AI and commonly used data-intensive educational technologies.\n\n- Untested and unproven technologies are adopted uncritically\n\n2. Implementing Shared Governance Policies to Promote Oversight\n\n- AI integration initiatives are spearheaded by administrations with little input from faculty members and other campus community members, including staff and students.\n\n- High levels of concern arose around AI and technology procurement, deployment, and use; dehumanized relations; and poor working and learning conditions.\n\n3. Improving Working and Learning Conditions\n\n- Preexisting work intensification and devaluation are the main reasons respondents give for using AI to assist with academic tasks.\n\n- Implementing AI in higher education adds to faculty and staff workloads and exacerbates long-standing inequities.\n\n- AI raises concerns around bias, discrimination, and accessibility because of the untested and uneven impacts on students and student learning.\n\n4. Demanding Transparency and the Ability to Opt Out\n\n- Faculty members and staff lack choice and meaningful avenues to opt out of both AI-based tools and other ed-tech."}
{"chunk_id": "www_aaup_org__920dd7ff9e19f3c6::c0002", "stable_id": "www_aaup_org__920dd7ff9e19f3c6", "url": "https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic", "final_url": "https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic", "retrieved_at": "2026-01-22T00:14:29.028278Z", "title": "Artificial Intelligence and Academic Professions | AAUP", "section": "Rules, Risks, and Ethics of AI", "source_type": "University / Official", "content_hash": "b6d82a5ea40faba092f7a153ce0e030c344b28aef200d5b1c6a3ea04da841c00", "chunk_index": 2, "char_start": 2532, "char_end": 3771, "text": "- Few institutions have created transparent, equitable policies or provided effective professional development opportunities on AI use.\n\n5. Protecting Faculty Members and Other Academic Workers\n\n- Academic workers across job categories are worried about increased reliance on contingent appointments and declining wages.\n\n- Respondents expressed concern about academic freedom and intellectual property rights.\n\nThe report provides details on the survey’s findings about these concerns and addresses them with recommendations to improve higher education—both broadly and narrowly as it relates to emerging technologies. Faculty members can work to implement these recommendations on their campuses by incorporating guidelines in faculty handbooks and collective bargaining agreements. The recommendations can inform strategy for organizing and policymaking related to AI in higher education institutions and organized labor more generally.\n\nThe ad hoc Committee on Artificial Intelligence and Academic Professions has provided a resource guide to help members implement the recommendations of this report.\n\nThe report that follows was prepared by the AAUP’s ad hoc Committee on Artificial Intelligence and Academic Professions in May 2025."}
{"chunk_id": "www_aaup_org__920dd7ff9e19f3c6::c0003", "stable_id": "www_aaup_org__920dd7ff9e19f3c6", "url": "https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic", "final_url": "https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic", "retrieved_at": "2026-01-22T00:14:29.028278Z", "title": "Artificial Intelligence and Academic Professions | AAUP", "section": "Rules, Risks, and Ethics of AI", "source_type": "University / Official", "content_hash": "b6d82a5ea40faba092f7a153ce0e030c344b28aef200d5b1c6a3ea04da841c00", "chunk_index": 3, "char_start": 3771, "char_end": 5433, "text": "Introduction\n\nFor decades, there have been significant labor issues around the use of technology in higher education.1Now, however, the uncritical adoption of artificial intelligence (AI) poses a threat to academic professions through potential work intensification and job losses and through its implications for intellectual property, economic security, and the faculty working conditions that affect student learning conditions. In its 2023 Statement on Online Education, the AAUP reaffirmed its principles with regard to the use of technology in higher education, stating that “(1) the use of new technologies in teaching should be for the purpose of advancing the basic functions of colleges and universities to preserve, augment, and transmit knowledge and to foster the abilities of students to learn and (2) as with all other curricular matters, the faculty should have primary responsibility for determining the policies and practices of the institution with regard to online education.”2 The findings of our survey of AAUP members, discussed in this report, show that many institutions diverge from these principles and that most faculty members have little input into how their colleges and universities procure and deploy AI and other educational technology (ed-tech). In their survey responses, AAUP members pleaded for guidance on how to deal with the onslaught of AI in their professional lives. Addressing their concerns, we articulate how academic communities can intervene meaning fully in response to issues related to AI and ed-tech in general, because they both promise to become far more entrenched in higher education in the coming years.3"}
{"chunk_id": "www_aaup_org__920dd7ff9e19f3c6::c0004", "stable_id": "www_aaup_org__920dd7ff9e19f3c6", "url": "https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic", "final_url": "https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic", "retrieved_at": "2026-01-22T00:14:29.028278Z", "title": "Artificial Intelligence and Academic Professions | AAUP", "section": "Rules, Risks, and Ethics of AI", "source_type": "University / Official", "content_hash": "b6d82a5ea40faba092f7a153ce0e030c344b28aef200d5b1c6a3ea04da841c00", "chunk_index": 4, "char_start": 5433, "char_end": 6695, "text": "Over the past two decades, colleges and universities have increasingly used ed-tech to implement learning management systems, offer online courses, and store and analyze large and small research datasets.4At present, legacy ed-tech platforms for course management and videoconferencing often incorporate massive data collection and analyses with predictive analytics that are similar to AI. Both new and legacy platforms alike use a number of techniques, including AI and related statistical methods applied to large language models and used to analyze, make predictions and recommendations, and, in the case of generative AI, even generate image, text, and video content.\n\nAI is both a marketing term and a usable product. Management in higher education and other sectors, the press, and technology companies often frame AI as something new, opaque, and exceedingly powerful that will replace many activities based on human intelligence, including labor. At the same time, they encourage public buy-in and network effects—that is, gains in the value of the technology as more people use it. Such framing serves to increase the power of technology firms and employers, thereby shutting down already meager avenues for critique, dissent, negotiation, and refusal."}
{"chunk_id": "www_aaup_org__920dd7ff9e19f3c6::c0005", "stable_id": "www_aaup_org__920dd7ff9e19f3c6", "url": "https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic", "final_url": "https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic", "retrieved_at": "2026-01-22T00:14:29.028278Z", "title": "Artificial Intelligence and Academic Professions | AAUP", "section": "Rules, Risks, and Ethics of AI", "source_type": "University / Official", "content_hash": "b6d82a5ea40faba092f7a153ce0e030c344b28aef200d5b1c6a3ea04da841c00", "chunk_index": 5, "char_start": 6695, "char_end": 7934, "text": "After decades of funding cuts, many colleges and universities rely on data-intensive technologies for the triage of limited resources. These technologies increasingly use AI to guide decision-making on everything from fundraising to pedagogy.5At many institutions, faculty members are expected to take on more advising, teach more students, and conduct more research—and to manage all these responsibilities with fewer resources. But rather than addressing inequity among faculty members or improving their working conditions, which are student learning conditions, administrations often choose to invest in technological interventions that they perceive as cheaper.\n\nTechnological interventions, especially those offered as one-size-fits-all solutions for educational problems, do not improve student, faculty, institutional, or research outcomes.6In many instances, their use harms students as well as faculty members and staff.7 Adding to these harms, faculty members, graduate students (including graduate student employees with teaching or research duties), and undergraduate students—who experience directly the impacts of technological triage—are largely excluded from decisions about which platforms and products to develop or use."}
{"chunk_id": "www_aaup_org__920dd7ff9e19f3c6::c0006", "stable_id": "www_aaup_org__920dd7ff9e19f3c6", "url": "https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic", "final_url": "https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic", "retrieved_at": "2026-01-22T00:14:29.028278Z", "title": "Artificial Intelligence and Academic Professions | AAUP", "section": "Rules, Risks, and Ethics of AI", "source_type": "University / Official", "content_hash": "b6d82a5ea40faba092f7a153ce0e030c344b28aef200d5b1c6a3ea04da841c00", "chunk_index": 6, "char_start": 7934, "char_end": 9270, "text": "According to the principles set forth in the AAUP’s 1966 Statement on Government of Colleges and Universities, it is “the responsibility primarily of the faculty to determine the appropriate curriculum and procedures of student instruction.”8This responsibility includes AI and other ed-tech infrastructure. However, many colleges and universities currently have no meaningful shared governance mechanisms around technology, as the findings of this survey suggest, and the explosion of AI has highlighted the need for such mechanisms among faculty members at individual institutions and across the higher education workforce.\n\nMethodology\n\nTo gain a better understanding of how AAUP members are experiencing AI and other ed-tech and what types of concerns they might have, the committee administered the national AAUP Survey on AI and the Profession in December 2024. The survey included Likert-scale items, which were ordered to measure respondents’ attitudes, such as agreement or importance, about the role of technology in higher education and at their institutions; yes-or-no items measuring whether particular tools, initiatives, or policies were in place at their institutions; and open-ended items addressing those tools, initiatives, and policies as well as general concerns regarding the use of technology in higher education."}
{"chunk_id": "www_aaup_org__920dd7ff9e19f3c6::c0007", "stable_id": "www_aaup_org__920dd7ff9e19f3c6", "url": "https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic", "final_url": "https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic", "retrieved_at": "2026-01-22T00:14:29.028278Z", "title": "Artificial Intelligence and Academic Professions | AAUP", "section": "Rules, Risks, and Ethics of AI", "source_type": "University / Official", "content_hash": "b6d82a5ea40faba092f7a153ce0e030c344b28aef200d5b1c6a3ea04da841c00", "chunk_index": 7, "char_start": 9270, "char_end": 10687, "text": "Participants were AAUP members. Five thousand members were selected from the Association’s active membership list using a random number generator and invited to participate in the online survey through a series of three email messages that provided a survey link. Approximately five hundred responses were received in two weeks and are reflected in the analysis below. Follow-up interviews were conducted in spring 2025 with thirteen respondents; however, findings from these interviews are excluded from this report.\n\nResponses collected from the Likert-scale items were analyzed and are reported at the descriptive level only (including frequencies and percentages). The open-ended items were analyzed using an open-coding process identifying generalized thematic trends. The categorical results reported in this document mainly reflect the trends emerging from the preconceptualized quantitative survey items. In some cases, the report’s presentation of survey results intersperses specific anonymous quotes pertaining to descriptive frequencies and percentages to add voice to participant perspectives conveyed in the report. Overall, the results reflect the views of the faculty members and other academic workers who took the time to respond to the online survey, but it does not necessarily represent the views of the entire AAUP membership or the overall population of academic workers in US higher education."}
{"chunk_id": "www_aaup_org__920dd7ff9e19f3c6::c0008", "stable_id": "www_aaup_org__920dd7ff9e19f3c6", "url": "https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic", "final_url": "https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic", "retrieved_at": "2026-01-22T00:14:29.028278Z", "title": "Artificial Intelligence and Academic Professions | AAUP", "section": "Rules, Risks, and Ethics of AI", "source_type": "University / Official", "content_hash": "b6d82a5ea40faba092f7a153ce0e030c344b28aef200d5b1c6a3ea04da841c00", "chunk_index": 8, "char_start": 10687, "char_end": 11920, "text": "Findings\n\nThe findings below are organized around five key concerns, along with recommendations related to those concerns.\n\n1.Improving Professional Development Regarding AI and Technology Harms\n\nDespite the widespread use of ed-tech, there is an overall lack of understanding about the relationship between AI and commonly used data-intensive educational technologies.\n\nRespondents viewed AI as having the potential to harm or to worsen many aspects of their work, while ed-tech is at least “somewhat helpful.” Eighty-one percent of respondents noted that they use some type of ed-tech, and 45 percent said they see it as at least somewhat helpful. Fifteen percent said they are required to use AI, yet nearly 81 percent reported that they are mandated to use ed-tech systems like the Canvas learning management system (LMS) or Google Suite, which have components that include predictive analytics, even when AI is “turned off.” This suggests that many faculty members and other academic workers may not realize that they are using AI-enabled tools for their work. Six percent said that they are required to use AI services like the Turnitin plagiarism detector and viewed Canvas as a data-intensive tool that is synonymous with AI."}
{"chunk_id": "www_aaup_org__920dd7ff9e19f3c6::c0009", "stable_id": "www_aaup_org__920dd7ff9e19f3c6", "url": "https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic", "final_url": "https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic", "retrieved_at": "2026-01-22T00:14:29.028278Z", "title": "Artificial Intelligence and Academic Professions | AAUP", "section": "Rules, Risks, and Ethics of AI", "source_type": "University / Official", "content_hash": "b6d82a5ea40faba092f7a153ce0e030c344b28aef200d5b1c6a3ea04da841c00", "chunk_index": 9, "char_start": 11920, "char_end": 13096, "text": "Recommendation 1: Colleges and universities should offer better and more critically informed, holistic professional development around AI, including what it is and is not and how it has been incorporated already into ed-tech business models (for example, not all users of the Canvas LMS recognize that its “Intelligent Insights” use AI and data analytics–driven recommendations, regardless of whether faculty members plan lessons using the Khan Academy’s Khanmigo “teacher tools” add-on).\n\nRecommendation 2: There is a need for discussions in academic communities that acknowledge technology as a labor concern and connect it with concerns around AI infrastructure and use in other sectors while underscoring the public service mission of higher education.\n\nRecommendation 3: While administrators set up “initiatives,” they are not doing enough to respond to day-to-day concerns; faculty members and other academic workers need localized policy solutions, including opportunities to directly participate in the development of best practices or guardrails that address deteriorating working and learning conditions.\n\nUntested and unproven technologies are adopted uncritically."}
{"chunk_id": "www_aaup_org__920dd7ff9e19f3c6::c0010", "stable_id": "www_aaup_org__920dd7ff9e19f3c6", "url": "https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic", "final_url": "https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic", "retrieved_at": "2026-01-22T00:14:29.028278Z", "title": "Artificial Intelligence and Academic Professions | AAUP", "section": "Rules, Risks, and Ethics of AI", "source_type": "University / Official", "content_hash": "b6d82a5ea40faba092f7a153ce0e030c344b28aef200d5b1c6a3ea04da841c00", "chunk_index": 10, "char_start": 13096, "char_end": 14409, "text": "Respondents articulated that AI technology is untested and unreliable in sensitive scenarios and thus questioned if it should be used at all. One respondent noted, “AI is not dependable enough for most scientific medical work. I uncover major errors. This is something that teachers and students must be made aware of.” Another highlighted how generative AI interferes with the core goals of education and learning: “Large language models like ChatGPT produce shallow, unoriginal ‘predictive text-y ideas’ and I worry that my students and others will increasingly believe that that’s okay—that there’s nothing better than that to aspire to.”\n\nRecommendation: Professional development around AI should include guidance for determining whether AI is the most appropriate solution for a given problem and for considering whether AI use is responsible, given its potential long-term impact on institutions and academic communities.\n\n2. Implementing Shared Governance Policies to Promote Oversight\n\nAI integration initiatives are spearheaded by administrations with little input from faculty members and other campus community members, including staff and students. High levels of concern arose around AI and technology procurement, deployment, and use; dehumanized relations; and poor working and learning conditions."}
{"chunk_id": "www_aaup_org__920dd7ff9e19f3c6::c0011", "stable_id": "www_aaup_org__920dd7ff9e19f3c6", "url": "https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic", "final_url": "https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic", "retrieved_at": "2026-01-22T00:14:29.028278Z", "title": "Artificial Intelligence and Academic Professions | AAUP", "section": "Rules, Risks, and Ethics of AI", "source_type": "University / Official", "content_hash": "b6d82a5ea40faba092f7a153ce0e030c344b28aef200d5b1c6a3ea04da841c00", "chunk_index": 11, "char_start": 14409, "char_end": 15843, "text": "Seventy-one percent of respondents said decision-making and AI initiatives are overwhelmingly led by college or university administrations, and many respondents described administrators exerting great effort to introduce AI into research, teaching, policy, and professional development with little meaningful input from faculty members, staff, or students. Examples include the development of institutional AI tools, workshops on teaching and detecting plagiarism, and subscribing to AI tools for students (such as Grammarly, marketed as an “AI writing partner”) without involving faculty members or students in the decision-making process. One respondent noted that “admin doesn’t seem to care about or value faculty input on this or any other topic” and hoped for “more faculty involvement in determining how AI and tech generally are used.”\n\nThis finding similarly highlights the importance of implementing AI policies created by and for faculty members, staff, and students.\n\nRecommendation: Institutions should develop meaningful shared governance policies and practices around ed-tech decision-making and use, as discussed in the AAUP’s Statement on Online Education.9A standing or ad hoc committee of faculty members, staff, and students should be elected by their respective constituencies and charged with monitoring, evaluating, and reviewing ed-tech procurement processes and policy. This ed-tech oversight committee should"}
{"chunk_id": "www_aaup_org__920dd7ff9e19f3c6::c0012", "stable_id": "www_aaup_org__920dd7ff9e19f3c6", "url": "https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic", "final_url": "https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic", "retrieved_at": "2026-01-22T00:14:29.028278Z", "title": "Artificial Intelligence and Academic Professions | AAUP", "section": "Rules, Risks, and Ethics of AI", "source_type": "University / Official", "content_hash": "b6d82a5ea40faba092f7a153ce0e030c344b28aef200d5b1c6a3ea04da841c00", "chunk_index": 12, "char_start": 15843, "char_end": 17483, "text": "- have access to and meaningful input in all parts of the procurement and deployment process;\n\n- push for an assessment of the impact of proposed ed-tech tools before decisions are made about procurement;\n\n- have the ability to meaningfully challenge decisions about ed-tech procurement and deployment;\n\n- perform ongoing evaluations of ed-tech data flows and uses at the university and vendor levels;\n\n- receive institutional funds allocated for these evaluations;\n\n- have meaningful levers of enforcement (for example, an agreement by the institution to rescind or abolish contracts for any ed-tech system or vendor that the committee finds harmful or unhelpful);\n\n- have the ability to suggest new ed-tech policies;\n\n- monitor accountability of administration members for protecting faculty, staff, and student data; and\n\n- act as a liaison with the broader campus community.\n\n3. Improving Working and Learning Conditions\n\nPreexisting work intensification and devaluation are the main reasons respondents give for using AI to assist with academic tasks.\n\nA quarter of respondents (25 percent) reported using AI tools or platforms to perform service, administrative duties, and teaching tasks that are often undervalued aspects of academic labor. For example, some respondents said that they used generative AI to write email messages, letters of recommendation, and internal reports or memos and to review grant applications and manuscripts. Respondents also reported using AI tools or platforms for detecting plagiarism and for developing course materials, which are also undervalued but time-consuming and crucial instructional duties."}
{"chunk_id": "www_aaup_org__920dd7ff9e19f3c6::c0013", "stable_id": "www_aaup_org__920dd7ff9e19f3c6", "url": "https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic", "final_url": "https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic", "retrieved_at": "2026-01-22T00:14:29.028278Z", "title": "Artificial Intelligence and Academic Professions | AAUP", "section": "Rules, Risks, and Ethics of AI", "source_type": "University / Official", "content_hash": "b6d82a5ea40faba092f7a153ce0e030c344b28aef200d5b1c6a3ea04da841c00", "chunk_index": 13, "char_start": 17483, "char_end": 19114, "text": "Respondents were overwhelmingly concerned with student plagiarism made possible by generative AI. Ninety-one percent noted that they were at least somewhat concerned about preventing academic dishonesty. However, one respondent wrote, “I am less concerned about the ‘honesty’ part than the ‘failure to learn’ part.” Another respondent noted, “It is now more difficult for [students] to develop their thoughts on a topic because they don’t have to spend time with it while they work through writing about it. . . . I am worried that they will never again get the chance to change their opinion as they expose themselves to ideas over the long term.” This distinction between honesty and failure to learn is critical because it highlights one of the core goals of higher education: to develop a well-informed and thoughtful citizenry.\n\nThis finding suggests that there is a need for higher education to refocus on the relational aspects of education and learning, as opposed to punitive measures that pit already overworked faculty members against debt-burdened students.\n\nImplementing AI in higher education adds to faculty and staff workloads and exacerbates long-standing inequities.\n\nOverall, respondents said that the rollout of AI at their colleges and universities has not made their jobs any better, but it has made some aspects of their work worse. Survey results indicate that AI has generally led to at least somewhat worse outcomes for the teaching environment (according to 62 percent of respondents), pay equity (30 percent), job enthusiasm (76 percent), academic freedom (40 percent), and student success (69 percent)."}
{"chunk_id": "www_aaup_org__920dd7ff9e19f3c6::c0014", "stable_id": "www_aaup_org__920dd7ff9e19f3c6", "url": "https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic", "final_url": "https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic", "retrieved_at": "2026-01-22T00:14:29.028278Z", "title": "Artificial Intelligence and Academic Professions | AAUP", "section": "Rules, Risks, and Ethics of AI", "source_type": "University / Official", "content_hash": "b6d82a5ea40faba092f7a153ce0e030c344b28aef200d5b1c6a3ea04da841c00", "chunk_index": 14, "char_start": 19114, "char_end": 20839, "text": "This finding is important because it emphasizes how the implementation of ed-tech, including AI, is connected to long-standing inequities in higher education. Required professional development on the use of AI in teaching and research adds to faculty and staff workloads—without evidence that AI improves productivity, pedagogy, or teaching and learning processes or outcomes. Indeed, AI may have negative effects on teaching and learning, especially in some pedagogical contexts.\n\nEighty-five percent of respondents said that they were at least somewhat concerned about how ed-tech is being implemented at their institutions. When considering areas that may be affected by increased use of AI in higher education, respondents resoundingly (at least 95 percent for each category) stressed the importance of protecting intellectual property rights and academic freedom, implementing meaningful opt-out policies, maintaining data privacy, improving job security and wages, preserving workplace autonomy, and supporting accessibility.\n\nOne respondent remarked that “there is ample evidence for the damage done to individuals and to society by many tech products, including generative AI, but not limited to it. However, it is treated as an unqualified good in almost all circumstances and one is required to learn and use certain technologies, even when non-tech options would be better for the workplace environment, student learning, and personal quality of life.” This response suggests the need for humanizing relationships in higher education communities and emphasizing that technocratic solutions (like plagiarism-detection technology) do not by themselves move us closer to caring and effective educational environments."}
{"chunk_id": "www_aaup_org__920dd7ff9e19f3c6::c0015", "stable_id": "www_aaup_org__920dd7ff9e19f3c6", "url": "https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic", "final_url": "https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic", "retrieved_at": "2026-01-22T00:14:29.028278Z", "title": "Artificial Intelligence and Academic Professions | AAUP", "section": "Rules, Risks, and Ethics of AI", "source_type": "University / Official", "content_hash": "b6d82a5ea40faba092f7a153ce0e030c344b28aef200d5b1c6a3ea04da841c00", "chunk_index": 15, "char_start": 20839, "char_end": 21844, "text": "Recommendation: Promote accountability for internally developed tools or tech company partnerships by requiring tech companies and vendors to provide proof of insurance covering liabilities related to the technology and to include in contracts indemnity clauses that transfer the responsibility for harms enacted (for example, data breaches or racial or socioeconomic discrimination) to the tech company or vendor.\n\n- Contracts should specify the penalties for any harms and the process for assessing and enforcing those penalties.\n\n- In many if not all cases the tech company or vendor should be held liable and should pay users or the institution an amount of money proportional to the harm.\n\n- Procurement should be overseen by a subcommittee of the earlier proposed ed-tech oversight committee with meaningful input from faculty members, staff, and students.\n\nAI raises concerns about bias, discrimination, and accessibility because of its untested and uneven impacts on students and student learning."}
{"chunk_id": "www_aaup_org__920dd7ff9e19f3c6::c0016", "stable_id": "www_aaup_org__920dd7ff9e19f3c6", "url": "https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic", "final_url": "https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic", "retrieved_at": "2026-01-22T00:14:29.028278Z", "title": "Artificial Intelligence and Academic Professions | AAUP", "section": "Rules, Risks, and Ethics of AI", "source_type": "University / Official", "content_hash": "b6d82a5ea40faba092f7a153ce0e030c344b28aef200d5b1c6a3ea04da841c00", "chunk_index": 16, "char_start": 21844, "char_end": 23300, "text": "Data-intensive technologies have a high likelihood of making recommendations, predictions, and analyses that are biased against historically marginalized people because the data and infrastructures these technologies use is also biased.10 Ninety-eight percent of respondents said that supporting accessibility was ranked as at least somewhat important when considering the increased use of AI in higher education. This finding is a reminder that student and faculty access to technology and learning experiences and ease of use should be core goals of any technologies introduced. However, many respondents also cautioned that these technologies can be so harmful that they should be subjected to thorough review. One respondent flatly charged that AI technology “has become a tool of surveillance by administration.”\n\nRecommendation 1: Require administrations to provide clear statements about how technology monitoring fits within the scope of administrators’ work, including specifics on why it is necessary, what this monitoring entails, and what outcomes may result for those monitored.\n\n- If monitoring faculty members, staff, or students is proven to be necessary for some educational reason—for example, when an instructor provides assessments on submitted student work using an LMS such as Canvas—any monitoring by the LMS or the institution must not continue indefinitely and should occur only within the framework necessary for a specified task."}
{"chunk_id": "www_aaup_org__920dd7ff9e19f3c6::c0017", "stable_id": "www_aaup_org__920dd7ff9e19f3c6", "url": "https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic", "final_url": "https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic", "retrieved_at": "2026-01-22T00:14:29.028278Z", "title": "Artificial Intelligence and Academic Professions | AAUP", "section": "Rules, Risks, and Ethics of AI", "source_type": "University / Official", "content_hash": "b6d82a5ea40faba092f7a153ce0e030c344b28aef200d5b1c6a3ea04da841c00", "chunk_index": 17, "char_start": 23300, "char_end": 24618, "text": "- The administration is prohibited from using electronic monitoring that results in violation of labor and employment laws; records workers off-duty or in sensitive areas; uses high-risk technologies, such as facial recognition; or identifies workers exercising their rights under employment and labor law.\n\n- Administrations that electronically monitor employees to assess their performance are required to disclose performance standards to faculty members and staff and apply these standards consistently.\n\n- An outside technology governance body should review and document productivity-monitoring and systems for setting performance quotas prior to their use.\n\n- Faculty members, staff, and students should be allowed to opt in to and out of monitoring of particular sessions.\n\n- Communications made available through any electronic dataset or system are protected under the same principles of academic freedom as print and other traditional media. As discussed in the AAUP’s report Academic Freedom and Electronic Communications, initially published in 1997 and last revised in 2013, this protection applies to email communications, websites, online bulletin boards, LMS content, blogs, list-servs, and social media—as well as to classroom recordings or videoconferencing communication on platforms such as Zoom.11"}
{"chunk_id": "www_aaup_org__920dd7ff9e19f3c6::c0018", "stable_id": "www_aaup_org__920dd7ff9e19f3c6", "url": "https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic", "final_url": "https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic", "retrieved_at": "2026-01-22T00:14:29.028278Z", "title": "Artificial Intelligence and Academic Professions | AAUP", "section": "Rules, Risks, and Ethics of AI", "source_type": "University / Official", "content_hash": "b6d82a5ea40faba092f7a153ce0e030c344b28aef200d5b1c6a3ea04da841c00", "chunk_index": 18, "char_start": 24618, "char_end": 25971, "text": "Recommendation 2: Minimize harms and bias resulting from the use of AI.\n\n- Campuses must conduct impact assessments of electronic monitoring systems, testing for bias and other harms to faculty members, staff, and students prior to use.\n\n- Technology should be accessible for the wide range of needs of faculty members, staff, and students.\n\n- Technology should be used to augment accessibility to the institutional working or learning environment where necessary.\n\n- All technologies used should be subject to regular and ongoing accessibility audits by a group of users approved by the campus AAUP chapter or another independent body, such as the ed-tech oversight committee proposed above or a subcommittee thereof.\n\n- Institutional funds should be available for these audit activities.\n\n4. Demanding Transparency and the Ability to Opt Out\n\nFaculty members, staff, and students lack choice and meaningful avenues to opt out of AI-based tools and other ed-tech.\n\nThis finding highlights the importance of not only prioritizing the needs and well-being of faculty members, staff, and students when implementing new AI and other ed-tech systems but also establishing policies that allow them to opt out of such systems. Furthermore, the unquestioned status quo of the continued expansion of AI often forecloses possibilities to negotiate the use of AI."}
{"chunk_id": "www_aaup_org__920dd7ff9e19f3c6::c0019", "stable_id": "www_aaup_org__920dd7ff9e19f3c6", "url": "https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic", "final_url": "https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic", "retrieved_at": "2026-01-22T00:14:29.028278Z", "title": "Artificial Intelligence and Academic Professions | AAUP", "section": "Rules, Risks, and Ethics of AI", "source_type": "University / Official", "content_hash": "b6d82a5ea40faba092f7a153ce0e030c344b28aef200d5b1c6a3ea04da841c00", "chunk_index": 19, "char_start": 25971, "char_end": 27653, "text": "Recommendation 1: Create meaningful opt-out policies, avoiding one-size-fits-all approaches.\n\nFaculty members, staff, and students should be able to opt out of technology use in ways that will not impose a burden on them or negatively affect their working or learning conditions.\n\nIt is the prerogative of educators to determine the best pedagogy in a given context and to decide whether AI engagement in learning is detrimental or simply inappropriate in some cases. Faculty members should be able to opt out of assessments that use AI or other ed-tech tools in classrooms and online or to require the use of other modalities to assess students’ performance, understanding, and knowledge.\n\nInstitutions should allow different constituents to explore and establish best practices and protections most appropriate to specific contexts and applications.\n\nRecommendation 2: Protect intellectual property for instructional materials.\n\nStandards should be set for how instructional materials may or may not be used in AI and other ed-tech data streams, including LMS platforms such as Canvas. While course syllabi are considered public documents at some colleges and universities, instructional materials such as lectures and original audiovisual materials constitute faculty intellectual property.12 As discussed in the AAUP’s Statement on Online Education, these principles apply to courses taught in person, online, or in a hybrid format. These principles also apply to AI and ed-tech generally, meaning that instructional materials, like other works of scholarship, must not be incorporated into AI data streams—for example, AI training datasets—without the consent of the creator.13"}
{"chunk_id": "www_aaup_org__920dd7ff9e19f3c6::c0020", "stable_id": "www_aaup_org__920dd7ff9e19f3c6", "url": "https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic", "final_url": "https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic", "retrieved_at": "2026-01-22T00:14:29.028278Z", "title": "Artificial Intelligence and Academic Professions | AAUP", "section": "Rules, Risks, and Ethics of AI", "source_type": "University / Official", "content_hash": "b6d82a5ea40faba092f7a153ce0e030c344b28aef200d5b1c6a3ea04da841c00", "chunk_index": 20, "char_start": 27653, "char_end": 29049, "text": "Recommendation 3: Protect student and instructor privacy.\n\nData, content, and information collected in AI and other ed-tech data streams should not be the property of the institution or vendors unless they identify and clearly disclose to faculty, students, and administrators a specific educational need. The Family Educational Rights and Privacy Act, a US federal law that protects the privacy of student education records, is a floor and not a ceiling for considering whether data-intensive technologies should be procured and used in a higher education setting.14\n\nFaculty members, staff, and students should be allowed to opt out of having their data, content, or information used or shared at no penalty to them or to their working or learning conditions.\n\nFew institutions have created transparent, equitable policies or provided effective professional development opportunities on AI use.\n\nRespondents noted the need for transparent and equitable policies on AI in their reflections on what they would change about the use of technology in higher education. One respondent emphasized the importance of “fair and equitable policies with clear transparency” for faculty members and students to better understand the acceptable uses of AI. Addressing student use of AI, another respondent noted that “strategies, resources, and training would be really helpful in navigating this challenge.”"}
{"chunk_id": "www_aaup_org__920dd7ff9e19f3c6::c0021", "stable_id": "www_aaup_org__920dd7ff9e19f3c6", "url": "https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic", "final_url": "https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic", "retrieved_at": "2026-01-22T00:14:29.028278Z", "title": "Artificial Intelligence and Academic Professions | AAUP", "section": "Rules, Risks, and Ethics of AI", "source_type": "University / Official", "content_hash": "b6d82a5ea40faba092f7a153ce0e030c344b28aef200d5b1c6a3ea04da841c00", "chunk_index": 21, "char_start": 29049, "char_end": 30239, "text": "Although 90 percent of respondents reported that their colleges and universities have introduced initiatives around uses of AI for teaching, research, learning, or work, these initiatives have not materialized into clear policies on AI implementation and use. This finding aligns with Inside Higher Ed’s 2024 Survey of College and University Chief Academic Officers, which found that 20 percent of colleges and universities have published a policy or policies governing the use of AI, including teaching and research.15 The lack of transparent and equitable policies seems at odds with the cross-campus AI initiatives, workshops, and expenditures spearheaded by college and university administrations and described by some respondents in terms such as “enormous,” highlighting again how faculty members, staff, and students are left out of major decisions about technology implementation and use. In open-ended responses, survey takers asked for better policies and more rigorous enforcement and accountability around technology in higher education. Some argued for guardrails, resources, and recommendations for ethical AI use, while others argued for prohibiting use in certain scenarios."}
{"chunk_id": "www_aaup_org__920dd7ff9e19f3c6::c0022", "stable_id": "www_aaup_org__920dd7ff9e19f3c6", "url": "https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic", "final_url": "https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic", "retrieved_at": "2026-01-22T00:14:29.028278Z", "title": "Artificial Intelligence and Academic Professions | AAUP", "section": "Rules, Risks, and Ethics of AI", "source_type": "University / Official", "content_hash": "b6d82a5ea40faba092f7a153ce0e030c344b28aef200d5b1c6a3ea04da841c00", "chunk_index": 22, "char_start": 30239, "char_end": 31504, "text": "Faculty members and staff need to have input in evaluating ed-tech before deployment, to have a say in how that technology is deployed and used, and to participate in ongoing evaluation of the technology and related policy over time. Ongoing communication, professional development, and cultivation of transparency with faculty members and staff will be important. Meaningful shared governance policies and practices should include access to information about the procurement and deployment process and the ability to meaningfully challenge administrations’ decision-making facilitated by data-intensive technology, as discussed earlier.\n\nRecommendation 1: Provide ongoing professional development opportunities.\n\nFaculty members, other academic workers, and students should have access to ongoing professional development—approved by the ed-tech oversight committee described above and organized and paid for by the institution—about technology uses, harms, and benefits.\n\nRecommendation 2: Ensure transparency and disclosure in ed-tech and the use of data streams.\n\nFaculty members and other academic workers should have\n\n- access to institutional technology procurement practices;\n\n- transparency regarding the cost of technologies procured and any alternatives;"}
{"chunk_id": "www_aaup_org__920dd7ff9e19f3c6::c0023", "stable_id": "www_aaup_org__920dd7ff9e19f3c6", "url": "https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic", "final_url": "https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic", "retrieved_at": "2026-01-22T00:14:29.028278Z", "title": "Artificial Intelligence and Academic Professions | AAUP", "section": "Rules, Risks, and Ethics of AI", "source_type": "University / Official", "content_hash": "b6d82a5ea40faba092f7a153ce0e030c344b28aef200d5b1c6a3ea04da841c00", "chunk_index": 23, "char_start": 31504, "char_end": 32993, "text": "- access to contracts with vendors;\n\n- access to data collected about them through ed-tech platforms or electronic monitoring systems;\n\n- the right to correct any data collected about them and to hold administrations accountable for adjusting any appointment-related decisions that were based, partially or solely, on inaccurate or biased data;\n\n- access to names of “partner companies” and vendors and clear articulations of how they use data streams; and\n\n- protection from retaliation for exercising their rights, including private rights of action.\n\n5. Protecting Faculty Members and Other Academic Workers\n\nAcademic workers across job categories are worried about increased reliance on contingent appointments and declining wages. Respondents expressed concern about academic freedom and intellectual property rights.\n\nEighty-seven percent of respondents maintained that it is important to improve job security and wages as AI is rolled out. Among part-time faculty members, there was near unanimity on this issue. Similarly, many respondents said that AI has generally led to worse outcomes for pay equity (27 percent), academic freedom (20 percent), and job enthusiasm (38 percent) at their institutions. Part-time faculty members and librarians were nearly unanimous that AI was leading to worse outcomes in most areas. Eighty-seven percent of respondents said that it is at least somewhat important to protect intellectual property rights over the products of their academic work."}
{"chunk_id": "www_aaup_org__920dd7ff9e19f3c6::c0024", "stable_id": "www_aaup_org__920dd7ff9e19f3c6", "url": "https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic", "final_url": "https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic", "retrieved_at": "2026-01-22T00:14:29.028278Z", "title": "Artificial Intelligence and Academic Professions | AAUP", "section": "Rules, Risks, and Ethics of AI", "source_type": "University / Official", "content_hash": "b6d82a5ea40faba092f7a153ce0e030c344b28aef200d5b1c6a3ea04da841c00", "chunk_index": 24, "char_start": 32993, "char_end": 34215, "text": "The path of dehumanization and automation is not the only option available. The growing adoption of data-intensive technologies in the workplace represents a critical challenge for workers across industries and job categories, highlighting the urgent need for a new set of labor standards for technology in higher education. These standards must be bold and comprehensive, keeping pace with the rapid advancements in workplace technologies and addressing the potential risks they pose to faculty members, staff, students, and society more broadly.\n\nAcademic workers are intimately familiar with the benefits, shortcomings, and harms of the technologies they use. Their engagement with technology offers insights that can drive meaningful change. It is important for faculty members and staff to participate actively in deciding which technologies are implemented, how they are used in their workplaces, and how resulting productivity gains are shared among all campus community members. Campuses can establish higher education workplace policies to harness new technologies and prioritize living-wage jobs, good working conditions that contribute to good learning conditions, and equity across job and identity categories."}
{"chunk_id": "www_aaup_org__920dd7ff9e19f3c6::c0025", "stable_id": "www_aaup_org__920dd7ff9e19f3c6", "url": "https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic", "final_url": "https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic", "retrieved_at": "2026-01-22T00:14:29.028278Z", "title": "Artificial Intelligence and Academic Professions | AAUP", "section": "Rules, Risks, and Ethics of AI", "source_type": "University / Official", "content_hash": "b6d82a5ea40faba092f7a153ce0e030c344b28aef200d5b1c6a3ea04da841c00", "chunk_index": 25, "char_start": 34215, "char_end": 35461, "text": "Recommendation 1: Maintain protections against work intensification.\n\nMembers of the institution’s ed-tech oversight committee should identify issues of work intensification, such as plagiarism checking, as well as invisible labor—unseen and often uncompensated tasks and responsibilities that are essential but frequently overlooked—related to technology implementation. Any technology found by the committee to be meaningfully causing work intensification should be prohibited or curtailed, and the committee should propose “best practices” to minimize work intensification.\n\nRecommendation 2: Provide protections against deskilling and job loss.\n\nDecisions on faculty appointments such as hiring, tenure, promotion, or termination should not rely primarily or exclusively on AI or data-intensive analytic technologies. Instead, decision-makers must independently corroborate the findings and data and provide the faculty member with full documentation, including the actual data used.\n\n- Data-intensive technologies cannot be used as a pretext for shifting faculty members holding tenure-line appointments to contingent appointments or lower-paid positions.\n\n- Data-intensive technologies cannot be used to justify decreasing wages in any way."}
{"chunk_id": "www_aaup_org__920dd7ff9e19f3c6::c0026", "stable_id": "www_aaup_org__920dd7ff9e19f3c6", "url": "https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic", "final_url": "https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic", "retrieved_at": "2026-01-22T00:14:29.028278Z", "title": "Artificial Intelligence and Academic Professions | AAUP", "section": "Rules, Risks, and Ethics of AI", "source_type": "University / Official", "content_hash": "b6d82a5ea40faba092f7a153ce0e030c344b28aef200d5b1c6a3ea04da841c00", "chunk_index": 26, "char_start": 35461, "char_end": 36898, "text": "- Data and information from these technologies cannot be the basis for decisions on faculty appointments such as hiring, reappointment, tenure, promotion, or termination.\n\n- If any of the above scenarios occur, a hearing and audit should be held to evaluate the technology and consider prohibiting it.\n\nRecommendation 3: Implement processes that allow faculty members and staff to meaningfully challenge administrative decisions on ed-tech.\n\nThere should be ongoing review of, and faculty participation in, decision-making. If reviews find that any technology contributes to deskilling, wage decreases, or job loss or to decreased academic freedom, intellectual property rights, faculty involvement in shared governance, or rights to organize for protections, there should be a process for faculty members and staff to meaningfully challenge the use of the offending technology and to reconsider, downsize, renegotiate, or void the contract for that technology.\n\nAny technology that threatens the academic freedom, role in shared governance, or economic security of faculty members should be prohibited.\n\nRecommendation 4: Protect academic freedom and the right to organize.\n\nFundamental principles of academic freedom apply as much to AI and other ed-tech data streams as they do to electronic communications in general, including communications among faculty members about their working conditions and organizing on their own behalf.16"}
{"chunk_id": "www_aaup_org__920dd7ff9e19f3c6::c0027", "stable_id": "www_aaup_org__920dd7ff9e19f3c6", "url": "https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic", "final_url": "https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic", "retrieved_at": "2026-01-22T00:14:29.028278Z", "title": "Artificial Intelligence and Academic Professions | AAUP", "section": "Rules, Risks, and Ethics of AI", "source_type": "University / Official", "content_hash": "b6d82a5ea40faba092f7a153ce0e030c344b28aef200d5b1c6a3ea04da841c00", "chunk_index": 27, "char_start": 36898, "char_end": 38167, "text": "Strategy, Targets, Outputs, and Action\n\nThe survey findings presented in this report highlight the need to establish structures of bottom-up shared governance to guide decisions around ed-tech, and especially AI, in higher education. The report also points to the importance of fostering solidaristic strategies across higher education, education more broadly, white-collar and industrial sectors, and civil society and grassroots organizations fighting on many fronts to establish bottom-up policy around generative AI.\n\nInternal and External Organizing\n\nTargets: AAUP members and the broader higher education community\n\nThere is a lot of work to do to communicate the potential harms related to uncritical deployment of AI and other ed-tech. Academic work and the learning conditions of students—and indeed higher education more broadly—are often devalued by technology. There is also a need to establish research functions within the AAUP that facilitate collaboration across associations and unions in higher education and other sectors. Together, these organizations could provide evolving best practices, guidelines, collective bargaining wish lists, ed-tech professional development, and organizing support as well as guidance on individual institutional issues."}
{"chunk_id": "www_aaup_org__920dd7ff9e19f3c6::c0028", "stable_id": "www_aaup_org__920dd7ff9e19f3c6", "url": "https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic", "final_url": "https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic", "retrieved_at": "2026-01-22T00:14:29.028278Z", "title": "Artificial Intelligence and Academic Professions | AAUP", "section": "Rules, Risks, and Ethics of AI", "source_type": "University / Official", "content_hash": "b6d82a5ea40faba092f7a153ce0e030c344b28aef200d5b1c6a3ea04da841c00", "chunk_index": 28, "char_start": 38167, "char_end": 39825, "text": "External communication strategies:\n\n- Organize and conduct workshops and develop documentation for faculty members covering ed-tech procurement processes, budget forensics, assessment of the impact of technology, and vendor practices.\n\n- Communicate with campus community members, policymakers, and the public through op-eds, AAUP member communications, conferences, meetings, and cross-union, civil society, legislative, and public conversations.\n\n- Develop web resources promoting these initiatives and other publicly available materials.\n\nInternal communications strategies:\n\n- Build out robust faculty, staff, and student educational resources on how technology is an issue that affects academic work, educational environments, and quality of life.\n\n- Work toward establishing faculty, staff, and student boards or governing bodies that can hold administrators accountable for their decision-making, with the goal of correcting technology policy failures to serve the educational mission of the institution.\n\nGuardrails and Best Policies\n\nTarget: AAUP members\n\nEach of the conceptual recommendations above points to problems and solutions to overcome them. Building on the AFT document detailing “guardrails” for using AI in primary and secondary schools17 and the findings and recommendations in this report, the AAUP should develop and promulgate a set of best practices for policymaking around the use of AI in higher education. In institutions without a bargaining unit, chapter members and leaders should attempt to adopt these practices through governance bodies, such as academic senates, and put in place mechanisms for enforcement and oversight."}
{"chunk_id": "www_aaup_org__920dd7ff9e19f3c6::c0029", "stable_id": "www_aaup_org__920dd7ff9e19f3c6", "url": "https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic", "final_url": "https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic", "retrieved_at": "2026-01-22T00:14:29.028278Z", "title": "Artificial Intelligence and Academic Professions | AAUP", "section": "Rules, Risks, and Ethics of AI", "source_type": "University / Official", "content_hash": "b6d82a5ea40faba092f7a153ce0e030c344b28aef200d5b1c6a3ea04da841c00", "chunk_index": 29, "char_start": 39825, "char_end": 41533, "text": "Bargaining\n\nTarget: AAUP collective bargaining chapters Establish a wish list developed from the recommendations in this report to be adapted by bargaining-unit legal representatives for each institutional context.\n\nAs they draft demands and negotiate agreements with administrations, bargaining units should consult with any internal ed-tech committees or teams they have established.\n\nState Policy\n\nTarget: State lawmakers\n\nCurrently in the United States, employers are introducing untested data-intensive technologies with almost no regulation or oversight, as former Federal Communications Commission Chairman Tom Wheeler documented.18 Workers largely do not have the right to know what data are being gathered about them or whether the data are being shared with others. They do not have the right to review or correct the data. Employers in many states are not required to notify workers about any electronic monitoring or algorithms they are basing decisions on, and workers do not have the right to challenge those decisions.\n\nOne of the most important strategies for state policy would be providing government agencies and employees the skills and resources necessary to research, educate others about, enhance, and enforce these protections. There should be increases in funding at state and federal levels for that purpose. However, we know that the Trump administration is currently uninterested in advancing such measures, as it has reversed even the mildest interventions to promote thoughtful, equitable advances in AI.19 At present, even state-level interventions seem unlikely. Nonetheless, we can build momentum for future policy interventions even where it appears there is no way forward."}
{"chunk_id": "www_aaup_org__920dd7ff9e19f3c6::c0030", "stable_id": "www_aaup_org__920dd7ff9e19f3c6", "url": "https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic", "final_url": "https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic", "retrieved_at": "2026-01-22T00:14:29.028278Z", "title": "Artificial Intelligence and Academic Professions | AAUP", "section": "Rules, Risks, and Ethics of AI", "source_type": "University / Official", "content_hash": "b6d82a5ea40faba092f7a153ce0e030c344b28aef200d5b1c6a3ea04da841c00", "chunk_index": 30, "char_start": 41533, "char_end": 42801, "text": "| Activity | Organizing target | Tools, outputs, and practices |\n\n| Faculty, staff, and student ed-tech oversight committee | Internal to higher education | Develop faculty, staff, and student committees and governing bodies that provide oversight on ed-tech procurement processes and policy. |\n\n| Guardrails and best practices | Internal | Develop language around AI and other ed-tech deployment to be adapted for collective bargaining contracts and faculty handbooks. |\n\n| Member outreach and education | Internal | Develop outreach materials (reports, one-pagers, FAQs, videos) to distribute to chapter leaders and members. Host and participate in events to distribute materials and discuss relevant issues. |\n\n| Structural analysis of education and technology | External to higher education | Emphasize how systemic inequalities in education combine with other concerns through external-facing outreach and communications. |\n\n| Solidarity and collective power across sectors | External | Collaborate with associations and unions in higher education and other sectors to develop best practices, guidelines, bargaining language, and professional development. Provide organizing support and advice on issues related to AI and technology deployment in the workplace. |"}
{"chunk_id": "www_aaup_org__920dd7ff9e19f3c6::c0031", "stable_id": "www_aaup_org__920dd7ff9e19f3c6", "url": "https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic", "final_url": "https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic", "retrieved_at": "2026-01-22T00:14:29.028278Z", "title": "Artificial Intelligence and Academic Professions | AAUP", "section": "Rules, Risks, and Ethics of AI", "source_type": "University / Official", "content_hash": "b6d82a5ea40faba092f7a153ce0e030c344b28aef200d5b1c6a3ea04da841c00", "chunk_index": 31, "char_start": 42801, "char_end": 44339, "text": "| State policy | External | Support state-level policies that establish guardrails and regulation on technology deployment in higher education and other sectors, building on existing policy efforts that focus on algorithmic decision-making, worker surveillance, replacing workers with technology, and protecting intellectual property. Provide guidance by organized labor to government agencies and employees through coordinated outreach and research efforts. |\n\nThe table above sums up this section’s suggestions about strategies, targets, output, and action.\n\nConclusion: Next Steps for AI in Higher Education\n\nIt is essential that higher education workers are in control of technological advancements affecting their employment. Faculty members and other academic workers are the closest to these technologies and are intimately familiar with their benefits, shortcomings, and harms. Their familiarity with ed-tech promises invaluable insights that can drive meaningful change. Faculty members should actively participate in deciding which ed-tech systems are adopted, how they are implemented in their workplaces, and how the resulting benefits are shared among all academic workers. We can establish appropriate higher education workplace policy and use our power to harness new technologies for fostering dynamic and productive institutions that prioritize economic security, good faculty working conditions and student learning conditions, and equity for all campus community members, while refusing tools that undermine these aims."}
{"chunk_id": "www_aaup_org__920dd7ff9e19f3c6::c0032", "stable_id": "www_aaup_org__920dd7ff9e19f3c6", "url": "https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic", "final_url": "https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic", "retrieved_at": "2026-01-22T00:14:29.028278Z", "title": "Artificial Intelligence and Academic Professions | AAUP", "section": "Rules, Risks, and Ethics of AI", "source_type": "University / Official", "content_hash": "b6d82a5ea40faba092f7a153ce0e030c344b28aef200d5b1c6a3ea04da841c00", "chunk_index": 32, "char_start": 44339, "char_end": 45892, "text": "BRITT S. PARIS (Information Studies)\n\nRutgers University–New Brunswick, chair\n\nCYNTHIA CONTI-COOK (Law)\n\nCollaborative Research Center for Resilience\n\nDANIEL GREENE (Information)\n\nUniversity of Maryland\n\nKYLE M. L. JONES (Library and Information Science)\n\nIndiana University Indianapolis\n\nBRIAN JUSTIE (Information Studies)\n\nUniversity of California, Los Angeles\n\nMATTHEW KIRSCHENBAUM (English)\n\nUniversity of Maryland\n\nLISA KRESGE (Technology and Work)\n\nUniversity of California, Berkeley\n\nEMMA MAY (Library and Information Science)\n\nRutgers University–New Brunswick\n\nAIHA NGUYEN (Urban Planning)\n\nData & Society\n\nREBECCA REYNOLDS (Library and Information Science)\n\nRutgers University–New Brunswick\n\nSERITA SARGENT (Library and Information Science)\n\nRutgers University–New Brunswick\n\nLINDSAY WEINBERG (Science and Technology Studies)\n\nPurdue University\n\nSARAH MYERS WEST (Communication)\n\nAI Now Institute\n\nDAVID GRAY WIDDER (Software Engineering)\n\nCornell University\n\nThe committee\n\nNotes\n\n1. Howard Besser and Maria Bonn, “Impact of Distance Independent Education,” Journal of the American Society for Information Science 47, no. 11 (1996): 880–83, https://doi.org/10.1002/(SICI)1097- 4571(199611)47:11<880::AID-ASI14>3.0.CO;2-Z; Christopher Newfield, The Great Mistake: How We Wrecked Public Universities and How We Can Fix Them (Johns Hopkins University Press, 2016); and Andrew Feenberg, “The Online Education Controversy and the Future of the University,” Foundations of Science 22, no. 2 (2017): 363–71, https://doi.org/10.1007/s10699-015-9444-9."}
{"chunk_id": "www_aaup_org__920dd7ff9e19f3c6::c0033", "stable_id": "www_aaup_org__920dd7ff9e19f3c6", "url": "https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic", "final_url": "https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic", "retrieved_at": "2026-01-22T00:14:29.028278Z", "title": "Artificial Intelligence and Academic Professions | AAUP", "section": "Rules, Risks, and Ethics of AI", "source_type": "University / Official", "content_hash": "b6d82a5ea40faba092f7a153ce0e030c344b28aef200d5b1c6a3ea04da841c00", "chunk_index": 33, "char_start": 45892, "char_end": 47411, "text": "2. AAUP, Policy Documents and Reports, 12th ed. (Johns Hopkins University Press, 2025), 245.\n\n3. Arizona State University, “Arizona State University Collaboration with OpenAI Charts the Future of AI in Higher Education,” PR Newswire, January 18, 2024, https://www.prnewswire.com/news-releases/arizona-state-university-collaboration-with-openai-charts-the-future-of-ai-in-higher-education-302038869.html; Kathryn Palmer, “Tech Giants Partner with Cal State System to Advance ‘Equitable’ AI Training,” Inside Higher Ed, February 5, 2025, https://www.insidehighered.com/news/tech-innovation/artificial-intelligence/2025/02/05/cal-state-system-tech-giants-partner.\n\n4. Britt Paris, Rebecca Reynolds, and Catherine McGowan, “Sins of Omission: Critical Informatics Perspectives on Privacy in E-learning Systems in Higher Education,” Journal of the Association for Information Science and Technology 73, no. 5 (2022): 708–25, https://doi.org/10.1002/asi.24575.\n\n5. Kelli Bird, Benjamin Castelman, Yifeng Song, and Zachary Mabel, “Big Data on Campus,” Education Next 12, no. 4 (2021), https://www.educationnext.org/big-data-on-campus-putting-predictive-analytics-to-the-test/.\n\n6. Paris, Reynolds, and McGowan, “Sins of Omission”; Kyle M. L. Jones, “Learning Analytics and Higher Education: A Proposed Model for Establishing Informed Consent Mechanisms to Promote Student Privacy and Autonomy,” International Journal of Educational Technology in Higher Education 16, no. 1 (2019): 24, https://doi.org/10.1186/s41239-019-0155-0."}
{"chunk_id": "www_aaup_org__920dd7ff9e19f3c6::c0034", "stable_id": "www_aaup_org__920dd7ff9e19f3c6", "url": "https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic", "final_url": "https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic", "retrieved_at": "2026-01-22T00:14:29.028278Z", "title": "Artificial Intelligence and Academic Professions | AAUP", "section": "Rules, Risks, and Ethics of AI", "source_type": "University / Official", "content_hash": "b6d82a5ea40faba092f7a153ce0e030c344b28aef200d5b1c6a3ea04da841c00", "chunk_index": 34, "char_start": 47411, "char_end": 48642, "text": "7. Hao-Ping (Hank) Lee, Advait Sarkar, Lev Tankelevitch, Ian Drosos, Sean Rintel, Richard Banks, and Nicholas Wilson, “The Impact of Generative AI on Critical Thinking: Self-Reported Reductions in Cognitive Effort and Confidence Effects from a Survey of Knowledge Workers,” in Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems, Association for Computing Machinery Digital Library, April 25, 2025, https://doi.org/10.1145/3706598.3713778.\n\n8. AAUP, Policy Documents and Reports, 12th ed. (Johns Hopkins University Press, 2025), 120.\n\n9. Policy Documents and Reports, 12th ed., 245–46.\n\n10. See Paris, Reynolds, and McGowan, “Sins of Omission”; Bird, Castelman, Song, and Mabel, “Big Data on Campus”; Joy Buolamwini, Unmasking AI: My Mission to Protect What Is Human in a World of Machines (Random House, 2023); and Safiya Noble, Algorithms of Oppression: How Search Engines Reinforce Racism (New York University Press, 2018).\n\n11. AAUP, “Academic Freedom and Electronic Communications,” Policy Documents and Reports, 12th ed. (Johns Hopkins University Press, 2025), 48.\n\n12. AAUP, “Statement on Intellectual Property,” Policy Documents and Reports, 11th ed. (Johns Hopkins University Press, 2015), 261–63."}
{"chunk_id": "www_aaup_org__920dd7ff9e19f3c6::c0035", "stable_id": "www_aaup_org__920dd7ff9e19f3c6", "url": "https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic", "final_url": "https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic", "retrieved_at": "2026-01-22T00:14:29.028278Z", "title": "Artificial Intelligence and Academic Professions | AAUP", "section": "Rules, Risks, and Ethics of AI", "source_type": "University / Official", "content_hash": "b6d82a5ea40faba092f7a153ce0e030c344b28aef200d5b1c6a3ea04da841c00", "chunk_index": 35, "char_start": 48642, "char_end": 50040, "text": "13. Policy Documents and Reports, 12th ed., 246.\n\n14. See Kyle M. L. Jones and Amy VanScoy, “The Syllabus as a Student Privacy Document in an Age of Learning Analytics,” Journal of Documentation 75, no. 6 (January 1, 2019): 1333–55, https://doi.org/10.1108/JD-12-2018-0202; Elana Zeide, “The Limits of Education Purpose Limitations,” University of Miami Law Review 71, no. 2 (March 1, 2017): 494; and Paris, Reynolds, and McGowan, “Sins of Omission.”\n\n15. “2024 Survey of College and University Chief Academic Officers,” Inside Higher Ed, https://www.insidehighered.com/reports/2024/04/15/2024-survey-college-and-university-chief-academic-officers.\n\n16. See AAUP, “Academic Freedom and Electronic Communications,” Policy Documents and Reports, 12th ed., 48–63.\n\n17. American Federation of Teachers, “Commonsense Guardrails for Using Advanced Technology in Schools,” published June 18, 2024; updated March 2025, https://www.aft.org/press-release/aft-announces-new-guardrails-artificial-intelligence-nations-classrooms.\n\n18. Tom Wheeler, “The Three Challenges of AI Regulation,” Brookings Institution TechTank blog, June 15, 2023, https://www.brookings.edu/articles/the-three-challenges-of-ai-regulation/.\n\n19. Exec. Order 14179, 90 Fed. Reg. 8741 (January 31, 2025), https://www.federalregister.gov/documents/2025/01/31/2025-02172/removing-barriers-to-american-leadership-in-artificial-intelligence."}
{"chunk_id": "www_nea_org__9eff2ae51d184c5d::c0000", "stable_id": "www_nea_org__9eff2ae51d184c5d", "url": "https://www.nea.org/professional-excellence/student-engagement/tools-tips/dos-and-donts-ai-classroom", "final_url": "https://www.nea.org/professional-excellence/student-engagement/tools-tips/dos-and-donts-ai-classroom", "retrieved_at": "2026-01-22T00:15:03.038648Z", "title": "Dos and Don'ts of AI in the Classroom | NEA", "section": "Rules, Risks, and Ethics of AI", "source_type": "University / Official", "content_hash": "835b3a33ec7a5e14830d562a9bee1658eb07c656df127e6823cc111a25dbf9bf", "chunk_index": 0, "char_start": 0, "char_end": 1295, "text": "Published: June 20, 2025\n\nBy integrating artificial intelligence (AI) in the classroom, educators can offer students powerful educational opportunities. Here’s a quick guide highlighting essential best practices (“Dos”) and common pitfalls to avoid (“Don’ts”), ensuring a safe, effective, and equitable AI-enhanced learning environment.\n\nDo…\n\n- Do Keep Humans Central: Always prioritize tools that complement human interactions and enhance educator-student relationships.\n\n- Do Ensure Evidence-Based Use: Choose AI tools proven effective through independent research and aligned with educational objectives.\n\n- Do Provide Professional Development: Offer thorough training for educators on AI literacy, enabling confident and informed use of AI tools.\n\n- Do Maintain Transparency: Select tools with clear information on data practices, ensuring understanding of data collection, usage, and storage.\n\n- Do Promote Accessibility: Opt for AI solutions accessible to diverse learners, including emergent multilingual learners and individuals with disabilities.\n\n- Do Adhere to Privacy Standards: Ensure strict compliance with privacy laws (e.g., FERPA) and robust data security practices.\n\n- Do Evaluate for Bias: Regularly assess AI tools for algorithmic biases to safeguard against unfair outcomes."}
{"chunk_id": "www_nea_org__9eff2ae51d184c5d::c0001", "stable_id": "www_nea_org__9eff2ae51d184c5d", "url": "https://www.nea.org/professional-excellence/student-engagement/tools-tips/dos-and-donts-ai-classroom", "final_url": "https://www.nea.org/professional-excellence/student-engagement/tools-tips/dos-and-donts-ai-classroom", "retrieved_at": "2026-01-22T00:15:03.038648Z", "title": "Dos and Don'ts of AI in the Classroom | NEA", "section": "Rules, Risks, and Ethics of AI", "source_type": "University / Official", "content_hash": "835b3a33ec7a5e14830d562a9bee1658eb07c656df127e6823cc111a25dbf9bf", "chunk_index": 1, "char_start": 1295, "char_end": 2585, "text": "- Do Set High Standards for Surveillance: Only adopt AI-powered surveillance tools with clear educational justification, community transparency, and safeguards to prevent misuse.\n\nDon't…\n\n- Don’t Rely Solely on AI: Avoid replacing essential human interactions or educator judgment entirely with technology.\n\n- Don’t Ignore Equity: Refrain from adopting AI tools that disadvantage or fail to accommodate diverse student populations.\n\n- Don’t Overlook Ethics: Never compromise ethical considerations, such as data privacy, student autonomy, or transparency, for convenience.\n\n- Don’t Skip Training: Avoid implementing AI tools without providing adequate training and resources for educators.\n\n- Don’t Trust Unverified Claims: Beware of AI tools lacking independent research backing or credible evaluations.\n\n- Don’t Underestimate Risks: Avoid neglecting potential issues, such as biases, data security, and privacy violations.\n\n- Don’t Forget Accountability: Never implement AI without clear accountability measures to address any errors, biases, or negative outcomes swiftly.\n\n- Don’t Use Surveillance Tools for High-Stakes Decisions: Do not use AI monitoring tools to make disciplinary, evaluative, or grading decisions. These tools should not substitute for due process or human oversight."}
{"chunk_id": "www_nea_org__9eff2ae51d184c5d::c0002", "stable_id": "www_nea_org__9eff2ae51d184c5d", "url": "https://www.nea.org/professional-excellence/student-engagement/tools-tips/dos-and-donts-ai-classroom", "final_url": "https://www.nea.org/professional-excellence/student-engagement/tools-tips/dos-and-donts-ai-classroom", "retrieved_at": "2026-01-22T00:15:03.038648Z", "title": "Dos and Don'ts of AI in the Classroom | NEA", "section": "Rules, Risks, and Ethics of AI", "source_type": "University / Official", "content_hash": "835b3a33ec7a5e14830d562a9bee1658eb07c656df127e6823cc111a25dbf9bf", "chunk_index": 2, "char_start": 2585, "char_end": 3537, "text": "Successfully integrating AI into the classroom hinges on mindful, ethical, and informed practices. By embracing these dos and steering clear of the don'ts, educators can set their classroom up for success in an increasingly AI-driven world.\n\nDownloads\n\nJoin Our Movement\n\nWe ask only what is right: equal opportunity for every student, every educator, every family. At home, in school, online, in Washington–there’s a right place for all of us to make a difference.\n\nEducation News Relevant to You\n\nWe're here to help you succeed in your career, advocate for public school students, and stay up to date on the latest education news and trends. Browse stories by topic, access the latest issue of NEA Today magazine, and celebrate educators and public schools.\n\nStay Informed We'll come to you\n\nWe're here to help you succeed in your career, advocate for public school students, and stay up to date on the latest education news. Sign up to stay informed"}
{"chunk_id": "infoguides_gmu_edu__9691dd62a09a1480::c0000", "stable_id": "infoguides_gmu_edu__9691dd62a09a1480", "url": "https://infoguides.gmu.edu/GenArtificial-Intelligence/ethics", "final_url": "https://infoguides.gmu.edu/GenArtificial-Intelligence/ethics", "retrieved_at": "2026-01-22T00:14:56.295414Z", "title": "Ethical Use - Generative Artificial Intelligence (AI) - InfoGuides at George Mason University", "section": "Rules, Risks, and Ethics of AI", "source_type": "University / Official", "content_hash": "50d5bd5a78685dba2fd29da35a0268db51cff34da9691bd5173814052b730e6e", "chunk_index": 0, "char_start": 0, "char_end": 12694, "text": "Ethical Use - Generative Artificial Intelligence (AI) - InfoGuides at George Mason University Skip to Main Content | University Libraries See Updates and FAQss for the latest library services updates. University Libraries InfoGuides Generative Artificial Intelligence (AI) Ethical Use Search this Guide Search Generative Artificial Intelligence (AI) Generative Artificial Intelligence Begin your research Books and databases Mason AI Tools Academic Integrity and AI AI and Personal Choice: Opting Out Citing AI Content AI and Copyright AI Tools for Literature Reviews AI and Data AI Literacy Skills Ethical Use AI and Ethics Mason Specific Guidelines for Ethics Key Ethical Considerations Broader Ethical Impacts Graduate Student-Specific Considerations Professional Development Implications \"Can I use AI for this task?\" Flowchart I need help AI and Ethics What is AI Ethics? AI ethics is the study of how to optimize artificial intelligence's beneficial impact while reducing risks and harmful outcomes. As AI becomes increasingly integrated into research, education, and daily life, understanding ethical considerations is essential for responsible use. Practical Steps for Ethical AI Use Before You Start Review your instructor's AI policy for the course Check George Mason's current AI guidelines Understand the privacy policy of any AI tool you plan to use During Use Keep records of your AI interactions for citation purposes Fact-check all AI-generated information Ensure AI enhances your work rather than replacing your thinking In Your Final Work Clearly disclose all AI assistance Cite AI tools appropriately according to your style guide Take responsibility for the accuracy of your final product This guide follows George Mason University's AI Guidelines and is regularly updated to reflect evolving best practices and policies. Claude.ai was used for formatting the information and content was reviewed by Dr. Heidi Blackburn. GRADReCon 2025 - Smart AI Use at George Mason: \u000bResearch Ethics and Guidelines You Need to Know Slide deck for \"Smart AI Use at George Mason: Research Ethics and Guidelines You Need to Know\" presented by Dr. Heidi Blackburn GRADReCON 2025 - Handout Handout for \"Smart AI Use at George Mason: Research Ethics and Guidelines You Need to Know\" presented by Dr. Heidi Blackburn Mason Specific Guidelines for Ethics Mason-Specific Guidelines George Mason University's AI Guidelines emphasize six key principles: Human Oversight - You remain responsible for all AI-assisted work Transparency - Clearly disclose AI use in your work Compliance - Follow all university policies and legal requirements Data Privacy - Protect confidential and sensitive information Critical Thinking - Develop AI literacy and question AI outputs Accuracy - Verify all AI-generated content before use What's Prohibited at George Mason ⚠️ Uploading confidential data or personally identifiable information (sensitive research data, student information, proprietary content) Using AI to create deceptive or misleading content Violating copyright through AI-generated materials Academic dishonesty involving undisclosed AI use George Mason University AI Guidelines for Students These guidelines provide a framework for helping students understand when and how to responsibly and ethically use generative AI applications, especially within their courses and degree programs. This guide will help you make informed decisions about AI use while prioritizing your learning outcomes and maintaining academic integrity. Key Ethical Considerations Key Ethical Considerations Bias and Fairness AI systems can perpetuate or amplify existing biases present in their training data. AI output depends entirely on its input, including the dataset used for training, which can result in explicit and implicit bias. This occurs because generative AI ingests enormous amounts of training data from across the internet, which means it can replicate the biases, stereotypes, and hate speech found on the web. What this means for you: Be aware that AI responses may reflect societal biases Critically evaluate AI outputs, especially on sensitive topics Consider diverse perspectives when using AI for research Privacy and Data Security There are ongoing privacy concerns about how AI systems harvest personal data from users, including information you may not realize you're sharing. Personal or sensitive user-submitted data can become part of the material used to train AI without explicit consent. Best practices: Never input confidential or sensitive data into AI tools Read privacy policies before using new AI tools Be especially cautious with research data, personal information, or proprietary content Follow George Mason's AI Guidelines regarding data security IRB Protocols: Consult with Information Technology Services' (ITS) IT Security Office Academic Integrity Using AI in academic work raises important questions about originality, citation, and honest representation of your work. Key principles: Disclosure : Always indicate when and how you've used AI assistance Verification : AI tools are known for producing \"hallucinations\" - false information created by the AI system, including partially or fully fabricated citations Original thinking : Ensure AI enhances rather than replaces your critical thinking Accuracy and Misinformation AI-generated content may contain factual errors, outdated information, or completely fabricated details presented convincingly. Critical evaluation tips: Always fact-check AI-generated information Cross-reference with reliable, primary sources Be aware that AI training data may not include recent developments Understand that AI information may lack currency as systems may have been trained on past datasets Broader Ethical Impacts Broader Ethical Impacts Environmental Impact AI technologies rely on vast physical infrastructures that require tremendous amounts of natural resources, including energy, water, and rare earth minerals. Labor and Consent Academic publishers have struck deals with AI companies to provide access to books and scholarly journals, without necessarily giving notice to authors. This raises questions about consent and fair compensation for intellectual property use. Graduate Student-Specific Considerations Graduate Student-Specific Considerations Discipline-Specific Guidelines While George Mason's AI guidelines apply universally, individual academic units may have additional requirements or interpretations. Different fields may have varying levels of AI acceptance based on disciplinary norms and methodological traditions. Action steps: Check with your specific college, school, or department for additional guidance Consult with your advisor about field-specific AI practices and expectations Review professional organizations' emerging AI policies in your discipline Collaborative Research and Co-Authorship Working with others adds complexity to AI disclosure and decision-making. Team members may have different comfort levels and institutional requirements for AI use. Best practices: Establish team agreements about AI use early in collaborative projects Ensure all collaborators understand disclosure requirements Document AI use decisions for shared reference When working with advisors, discuss AI policies upfront to avoid conflicts Research Applications and IRB Considerations AI use in research involving human subjects requires special attention to ethics and IRB compliance. Key areas: IRB protocols : Researchers should fully and explicitly include any use of AI in research processes in their Institutional Review Board (IRB) protocols and consult with Information Technology Services (ITS) IT Security Office to understand levels of risk Using AI for data analysis raises confidentiality and replicability concerns : with qualitative research (transcripts, interviews content) with quantitative research (data sets with sensitive content) with reproducability and replicability of research For secondary data, consult terms of service or data use agreements Literature reviews : AI assistance in systematic reviews may affect methodology reporting Grant writing : Some funding agencies have specific AI disclosure requirements or prohibit the use of AI 🚨 Important: Research involving or using sensitive data requires an approved protected AI environment which does not share information outside of the project. Consult ITS data classification guidance . University-Approved Tools for Graduate Students George Mason provides specific AI tools that meet university security and privacy standards. Using approved tools helps ensure compliance with institutional policies. Enterprise-approved tools (no restrictions): Adobe AI Microsoft Copilot Chat LinkedIn Learning AI Career Coach PatriotAI (university-managed access to large language models) Zoom AI Companion Approved but not supported (public data only): ChatGPT (with specific privacy settings - see AI Toolkit for details) Getting tools reviewed: Contact the Architectural Standards Review Board (ASRB) to request evaluation of tools not on the approved list. Professional Development Implications Professional Development Implications Skill Development and Learning Consider how AI use affects your academic and professional growth as a student. Balance considerations: Use AI to enhance learning without replacing critical thinking skills Develop AI literacy as a professional competency Understand when human expertise is irreplaceable Build skills that complement rather than compete with AI Academic Publishing and Career Preparation The academic publishing landscape is rapidly evolving regarding AI use, with different journals and conferences developing varying policies. Evolving standards: Journal policies on AI use are still developing across disciplines Conference submission guidelines increasingly address AI disclosure Publishers are creating new standards for AI-generated content Peer review processes may prohibit or restrict AI use Career implications: AI literacy is becoming an expected skill in many fields Demonstrable human expertise remains valuable Understanding ethical AI use is a professional asset \"Can I use AI for this task?\" Flowchart \"Can I use AI for this task?\" Flowchart **These tools are meant as guides for the thought process. Always consult with your instructor, advisor, or publisher for the most current guidelines. Academic Writing: Is this a high-stakes assignment (thesis, dissertation, publication)? ➡️ Proceed with extra caution, consult advisor Does your instructor/program have specific AI policies? ➡️ Follow those requirements first Are you using AI to replace your own thinking? ➡️ Not recommended Are you using AI to enhance organization, grammar, or brainstorming? ➡️ Generally acceptable with disclosure Research Tasks: Does your task involve confidential or sensitive data? ➡️ Do NOT use public AI tools. Use tools approved for the task. Understand the sensitivity level of your data. Are you using university-approved tools only? ➡️ Check the AI Toolkit Is this part of IRB-approved research? ➡️ Must be disclosed in your IRB protocol Can you verify all AI outputs independently? ➡️ Required for research integrity Data and Privacy: Is this information you would share publicly? ➡️ If no, don't share with AI Does this contain student information, research participant data, or proprietary content? ➡️ Prohibited Are you using an enterprise-approved tool? ➡️ Safer choice but check guidelines for IRB Can you complete your work without sharing sensitive information? ➡️ Best practice Template Disclosure Statement Examples For course assignments: \"I used [AI tool name] on [date] to help with [specific task, e.g., brainstorming ideas, organizing content, checking grammar]. All final content was reviewed, revised, and verified by me.\" For research papers: \"AI assistance was used in this research for [specific tasks]. [AI tool name] was used on [dates] to [specific description]. All AI-generated content was verified through independent sources and analysis.\" For dissertation/thesis acknowledgments: \"I acknowledge the use of [AI tool name] for [specific assistance] during the completion of this work. All analysis, interpretations, and conclusions remain my own.\" << Previous: AI Literacy Skills Next: I need help >> Last Updated: Nov 19, 2025 11:33 AM URL: https://infoguides.gmu.edu/GenArtificial-Intelligence Print Page Login to LibApps Report a problem Subjects: Computer Science & IT Tags: computer science , computer_science , information technology , information_technoogy Ask a Librarian | Hours & Directions | Mason Libraries Home Copyright © George Mason University"}
{"chunk_id": "professional_dce_harvard_edu__b348c4377dbfa8f0::c0000", "stable_id": "professional_dce_harvard_edu__b348c4377dbfa8f0", "url": "https://professional.dce.harvard.edu/blog/ethics-in-ai-why-it-matters/", "final_url": "https://professional.dce.harvard.edu/blog/ethics-in-ai-why-it-matters/", "retrieved_at": "2026-01-22T00:14:54.549493Z", "title": "Ethics in AI: Why It Matters - Professional & Executive Development | Harvard DCE", "section": "Rules, Risks, and Ethics of AI", "source_type": "Explainer / Guide", "content_hash": "7e3e01d93b23a86c1b34eba480308a5f0deec669d83f5a2b9de2489af5ed56f0", "chunk_index": 0, "char_start": 0, "char_end": 1445, "text": "On this page\n\nWhile the uses of AI tools can seem unlimited, it’s critical that their expertise does not go unquestioned; AI tools are only as reliable as the data they’re trained on — and the people who build them.\n\nIssues related to privacy, biases, and transparency remain paramount for building AI systems that are both ethical and accurate. As corporations continue to embed AI into their day-to-day processes, establishing frameworks ensuring AI applications are within legal and ethical bounds is increasingly important.\n\nMeet Our Expert\n\nThe Importance of AI Ethics\n\nUnderstanding the ethical implications of AI is critical for leaders:\n\nFirst, AI ethical literacy gives leaders an understanding of the potential issues AI could cause, allowing them to protect their companies from lawsuits and reputational damage.\n\nSecond, understanding AI ethics helps leaders build a holistic picture of the coming AI-Age, and the concomitant risks and opportunities.\n\nAI ethics examines the societal implications of widespread AI usage around issues like fairness and privacy. It also explores how AI affects the environment and its potential impact on the workforce: AI data centers require more water resources than traditional data centers, and even AI innovators like Anthropic CEO Dario Amodei foresee widespread white-collar job loss, projecting that AI could replace 50 percent of all entry-level white-collar jobs within the next five years."}
{"chunk_id": "professional_dce_harvard_edu__b348c4377dbfa8f0::c0001", "stable_id": "professional_dce_harvard_edu__b348c4377dbfa8f0", "url": "https://professional.dce.harvard.edu/blog/ethics-in-ai-why-it-matters/", "final_url": "https://professional.dce.harvard.edu/blog/ethics-in-ai-why-it-matters/", "retrieved_at": "2026-01-22T00:14:54.549493Z", "title": "Ethics in AI: Why It Matters - Professional & Executive Development | Harvard DCE", "section": "Rules, Risks, and Ethics of AI", "source_type": "Explainer / Guide", "content_hash": "7e3e01d93b23a86c1b34eba480308a5f0deec669d83f5a2b9de2489af5ed56f0", "chunk_index": 1, "char_start": 1445, "char_end": 2758, "text": "Understanding how ethical issues affect a business’ day-to-day operations, such as privacy-related issues, in addition to the broader implications of AI on the economy, the workforce, and the environment, will enable leaders to make informed and balanced decisions.\n\nEthical Challenges in AI\n\nLeaders are facing a host of challenges when it comes to managing AI data and privacy, biases, transparency issues, and more.\n\nMichael Impink, instructor of AI Ethics in Business at Harvard DCE’s Professional and Executive Development division, weighs in on how executives and business leaders can meet these challenges head-on.\n\n“For leaders, awareness is the number one step,” Impink says. Once leaders know where ethical AI issues might exist, they can begin to generate solutions.”\n\nBut because AI is moving so quickly, there’s no clearly defined step-by-step process for resolving all ethical issues.\n\n“AI is idiosyncratic to what you want it to do, so there’s no one-size-fits-all approach,” he adds.\n\nUnderstanding the main ethical challenges AI presents — and creatively generating solutions — is mandatory for the leaders of tomorrow.\n\nAI Data & Privacy\n\nData privacy is paramount for most companies. Ensuring their customers’, patients’, or business’ proprietary information remains secure is mission-critical."}
{"chunk_id": "professional_dce_harvard_edu__b348c4377dbfa8f0::c0002", "stable_id": "professional_dce_harvard_edu__b348c4377dbfa8f0", "url": "https://professional.dce.harvard.edu/blog/ethics-in-ai-why-it-matters/", "final_url": "https://professional.dce.harvard.edu/blog/ethics-in-ai-why-it-matters/", "retrieved_at": "2026-01-22T00:14:54.549493Z", "title": "Ethics in AI: Why It Matters - Professional & Executive Development | Harvard DCE", "section": "Rules, Risks, and Ethics of AI", "source_type": "Explainer / Guide", "content_hash": "7e3e01d93b23a86c1b34eba480308a5f0deec669d83f5a2b9de2489af5ed56f0", "chunk_index": 2, "char_start": 2758, "char_end": 4158, "text": "But AI creates other avenues for bad actors to gain access to a company’s sensitive information, potentially exposing the business to litigation. For example, businesses that collect Personal Identifying Information (PII) have a legal responsibility to keep that information secure. If PII is given to an AI tool, the tool needs to be secure from both external cyberattacks and any internal manipulation. It’s critical that leaders understand the risks posed by AI malware and develop internal cybersecurity systems to detect and mitigate AI threats.\n\nPossible Bias in AI\n\nBiases are one of the biggest ethical challenges for AI systems, but most business leaders miss the implications of biases on business outcomes.\n\nJust like working with an inaccurate business model, a biased AI tool can lead to a host of poor outcomes: inaccurate predictions, litigation (if the biases are found to negatively impact a protected class, for example), and wrongheaded conclusions.\n\nAccording to Impink, there are three main sources for biases in AI: The programmers, the algorithm, or the training data.\n\n“The programmers creating the AI could be biased,” Impink says. “The algorithm could be biased, it could be overweighting something, or the bias could be something inherent in the algorithm. The training data itself could be biased, or these learning materials could be limiting some important information.”"}
{"chunk_id": "professional_dce_harvard_edu__b348c4377dbfa8f0::c0003", "stable_id": "professional_dce_harvard_edu__b348c4377dbfa8f0", "url": "https://professional.dce.harvard.edu/blog/ethics-in-ai-why-it-matters/", "final_url": "https://professional.dce.harvard.edu/blog/ethics-in-ai-why-it-matters/", "retrieved_at": "2026-01-22T00:14:54.549493Z", "title": "Ethics in AI: Why It Matters - Professional & Executive Development | Harvard DCE", "section": "Rules, Risks, and Ethics of AI", "source_type": "Explainer / Guide", "content_hash": "7e3e01d93b23a86c1b34eba480308a5f0deec669d83f5a2b9de2489af5ed56f0", "chunk_index": 3, "char_start": 4158, "char_end": 5474, "text": "AI Modeling Transparency and Explainability\n\nIn an era where AI systems increasingly shape business decisions in sectors like finance, healthcare, and education, humans need to understand the algorithmic priorities and rationales that drive AI decision-making. If an AI model denies a loan, flags a tumor, or prioritizes a job applicant, humans must be able to trace the reasoning behind that decision. An ethical application of AI tools necessitates deep human understanding to ensure that decisions are made fairly.\n\n“If it becomes commonplace to use AI, the firms who use it ethically and responsibly will gain a competitive advantage,” Impink says. “The ones who don’t might have a harder time winning contracts or accessing data.”\n\nWithout a clear window into how AI arrives at its conclusions, businesses run the risk of creating so-called “black-box” systems where the algorithms automating decisions are inscrutable to employees managing the systems.\n\nWhen decisions become biased or limit opportunities for recourse, current inequalities are likely to be reinforced and public trust in businesses or AI tools could degrade rapidly. If people begin to feel that AI algorithms are making life-altering decisions in ways even experts can’t explain, they’re likely to lose trust in those institutions and tools."}
{"chunk_id": "professional_dce_harvard_edu__b348c4377dbfa8f0::c0004", "stable_id": "professional_dce_harvard_edu__b348c4377dbfa8f0", "url": "https://professional.dce.harvard.edu/blog/ethics-in-ai-why-it-matters/", "final_url": "https://professional.dce.harvard.edu/blog/ethics-in-ai-why-it-matters/", "retrieved_at": "2026-01-22T00:14:54.549493Z", "title": "Ethics in AI: Why It Matters - Professional & Executive Development | Harvard DCE", "section": "Rules, Risks, and Ethics of AI", "source_type": "Explainer / Guide", "content_hash": "7e3e01d93b23a86c1b34eba480308a5f0deec669d83f5a2b9de2489af5ed56f0", "chunk_index": 4, "char_start": 5474, "char_end": 6702, "text": "If it becomes commonplace to use AI, the firms who use it ethically and responsibly will gain a competitive advantage.\n\nMichael Impink\n\nThe Impact of AI on Employment\n\nThe rise of AI in the workplace presents a complex ethical challenge for the global job market. On one hand, AI promises to automate repetitive tasks, boost productivity, and unlock entirely new industries. On the other, the speed of AI adoption threatens white-collar jobs across industries and roles, from software engineers to copywriters, sales reps to HR associates.\n\nHistorically, big waves of technological change have led to increased economic productivity. Proponents of AI argue that these periods freed workers from low-wage, rote work and delivered them to higher-value, more interesting work — and the AI revolution will do the same.\n\nHowever, it’s unclear what new roles will be created, and what new training or upskilling opportunities will be available to prepare workers for new positions. As the speed of AI development and adoption increases, there’s a risk that development will outpace retraining.\n\nGovernments may need to step in to support this major transition within the workforce through a provision for Universal Basic Income (UBI)."}
{"chunk_id": "professional_dce_harvard_edu__b348c4377dbfa8f0::c0005", "stable_id": "professional_dce_harvard_edu__b348c4377dbfa8f0", "url": "https://professional.dce.harvard.edu/blog/ethics-in-ai-why-it-matters/", "final_url": "https://professional.dce.harvard.edu/blog/ethics-in-ai-why-it-matters/", "retrieved_at": "2026-01-22T00:14:54.549493Z", "title": "Ethics in AI: Why It Matters - Professional & Executive Development | Harvard DCE", "section": "Rules, Risks, and Ethics of AI", "source_type": "Explainer / Guide", "content_hash": "7e3e01d93b23a86c1b34eba480308a5f0deec669d83f5a2b9de2489af5ed56f0", "chunk_index": 5, "char_start": 6702, "char_end": 8250, "text": "“We might see people working 30 hours a week and receiving some form of UBI. But we’re not there yet — we haven’t seen the productivity gains,” Impink says.\n\nBut widespread AI use could also create new jobs. AI-experts in different fields will likely be in high-demand in the coming years.\n\nAI Governance\n\nAs AI technology becomes more widespread, international bodies are recognizing the need for global coordination to address challenges and risks while also distributing and maximizing benefits. For example, the Organization for Economic Co-operation and Development (OECD) issued its OECD AI Principles, designed to promote an innovative yet trustworthy use of AI that respects democratic norms.\n\nSimilarly, the United Nations Secretary-General has established a board of 39 experts from various disciplines to act as a High-Level Advisory Body on AI. By engaging stakeholders that include governments, the private sector, and society at-large, the board will recommend strategies for international AI governance that respects human rights and helps meet sustainable development goals.\n\nToday’s Regulatory Landscape for AI\n\nThe AI regulatory landscape is rapidly evolving in the United States. The Trump Administration’s sweeping tax and spending package includes an unusual and hotly debated proviso — no state AI regulations for 10 years. In the most recent version of the bill, states that pass regulations on AI models and systems wouldn’t be able to access the $500 million in federal funds earmarked for AI infrastructure and deployment."}
{"chunk_id": "professional_dce_harvard_edu__b348c4377dbfa8f0::c0006", "stable_id": "professional_dce_harvard_edu__b348c4377dbfa8f0", "url": "https://professional.dce.harvard.edu/blog/ethics-in-ai-why-it-matters/", "final_url": "https://professional.dce.harvard.edu/blog/ethics-in-ai-why-it-matters/", "retrieved_at": "2026-01-22T00:14:54.549493Z", "title": "Ethics in AI: Why It Matters - Professional & Executive Development | Harvard DCE", "section": "Rules, Risks, and Ethics of AI", "source_type": "Explainer / Guide", "content_hash": "7e3e01d93b23a86c1b34eba480308a5f0deec669d83f5a2b9de2489af5ed56f0", "chunk_index": 6, "char_start": 8250, "char_end": 9723, "text": "Further afield, the European Union passed the EU AI Act, presenting a comprehensive framework for classifying AI tools based on the risks they present to users. Regulations are applied based on risk level, with AI applications deemed high risk subject to greater scrutiny and more regulations. High risk AI applications include those that impact safety (such as AI used in aviation or medical devices) or fundamental rights (such as AI used in law enforcement, educational training, and more).\n\nWhile AI regulation in the US is up for debate, US firms with a global footprint using AI may still find themselves facing regulations:\n\n“The E.U. regulates the developed world,” Impink says. “U.S. firms adhere to E.U. regulations if they want to work internationally, so everyone really follows Europe.”\n\nThe Future of AI Ethics\n\nTo some, AI seems like a fad. Free, public-facing LLMs like ChatGPT and Claude regularly make mistakes and aren’t up-to-date on the latest news and trends, leading some to underestimate the power of this technology.\n\nHowever, internal AI systems are more powerful than publicly available LLMs and can become ever-more refined through specific prompting. As development continues — to the point where AI systems are developing themselves and other systems — reliability concerns will drop. However, more sophisticated AI systems raise other concerns, like the possibility of superintelligence and what that could mean for the workforce and society."}
{"chunk_id": "professional_dce_harvard_edu__b348c4377dbfa8f0::c0007", "stable_id": "professional_dce_harvard_edu__b348c4377dbfa8f0", "url": "https://professional.dce.harvard.edu/blog/ethics-in-ai-why-it-matters/", "final_url": "https://professional.dce.harvard.edu/blog/ethics-in-ai-why-it-matters/", "retrieved_at": "2026-01-22T00:14:54.549493Z", "title": "Ethics in AI: Why It Matters - Professional & Executive Development | Harvard DCE", "section": "Rules, Risks, and Ethics of AI", "source_type": "Explainer / Guide", "content_hash": "7e3e01d93b23a86c1b34eba480308a5f0deec669d83f5a2b9de2489af5ed56f0", "chunk_index": 7, "char_start": 9723, "char_end": 10969, "text": "Is Superintelligence A Possibility?\n\nA.I. superintelligence — an AI system whose intelligence surpasses that of humans — is a hotly debated subject. Some experts believe we’ll see superintelligence by the end of the decade, and others say that it will probably never happen. But what exactly is AI superintelligence?\n\nAI superintelligence refers to systems that exceed human intelligence and can autonomously learn and innovate beyond their initial programming.\n\n“Personally, I don’t think it’s possible,” Impink says. Because A.I. tools are trained on human materials and iterated upon by human prompting, “there’s some level of creativity it won’t be able to reach.”\n\nReady to dive deeper into AI?\n\nAs AI transforms the workforce, leaders need tailored training to stay up-to-date. Harvard’s Department of Continuing Education offers several courses related to artificial intelligence to support business leaders as they use AI to drive business success.\n\nAs businesses implement AI into their workflows, systems, and teams, establishing ethical processes around data privacy, fairness, and transparency is critical for success. Companies that pursue an ethical approach to AI are likely to avoid litigation and preserve their brand reputation."}
{"chunk_id": "www_unesco_org__831d1af606b537dd::c0000", "stable_id": "www_unesco_org__831d1af606b537dd", "url": "https://www.unesco.org/en/artificial-intelligence/recommendation-ethics", "final_url": "https://www.unesco.org/en/artificial-intelligence/recommendation-ethics", "retrieved_at": "2026-01-22T00:15:01.228259Z", "title": "Ethics of Artificial Intelligence - AI | UNESCO", "section": "Rules, Risks, and Ethics of AI", "source_type": "University / Official", "content_hash": "d7af3aa55fdf3acc256ac82b93c24ab206f5d7d24bffe728ed0e91d1265ed936", "chunk_index": 0, "char_start": 0, "char_end": 1216, "text": "Ethics of Artificial Intelligence\n\nWith its unique mandate, UNESCO has led the international effort to ensure that science and technology develop with strong ethical guardrails for decades.\n\nBe it on genetic research, climate change, or scientific research, UNESCO has delivered global standards to maximize the benefits of the scientific discoveries, while minimizing the downside risks, ensuring they contribute to a more inclusive, sustainable, and peaceful world. It has also identified frontier challenges in areas such as the ethics of neurotechnology, on climate engineering, and the internet of things.\n\nThe rapid rise in artificial intelligence (AI) has created many opportunities globally, from facilitating healthcare diagnoses to enabling human connections through social media and creating labour efficiencies through automated tasks.\n\nHowever, these rapid changes also raise profound ethical concerns. These arise from the potential AI systems have to embed biases, contribute to climate degradation, threaten human rights and more. Such risks associated with AI have already begun to compound on top of existing inequalities, resulting in further harm to already marginalised groups.\n\nFour core values"}
{"chunk_id": "www_unesco_org__831d1af606b537dd::c0001", "stable_id": "www_unesco_org__831d1af606b537dd", "url": "https://www.unesco.org/en/artificial-intelligence/recommendation-ethics", "final_url": "https://www.unesco.org/en/artificial-intelligence/recommendation-ethics", "retrieved_at": "2026-01-22T00:15:01.228259Z", "title": "Ethics of Artificial Intelligence - AI | UNESCO", "section": "Rules, Risks, and Ethics of AI", "source_type": "University / Official", "content_hash": "d7af3aa55fdf3acc256ac82b93c24ab206f5d7d24bffe728ed0e91d1265ed936", "chunk_index": 1, "char_start": 1216, "char_end": 2433, "text": "A dynamic understanding of AI\n\nThe Recommendation interprets AI broadly as systems with the ability to process data in a way which resembles intelligent behaviour.\n\nThis is crucial as the rapid pace of technological change would quickly render any fixed, narrow definition outdated, and make future-proof policies infeasible.\n\nA human rights approach to AI\n\nActionable policies\n\nKey policy areas make clear arenas where Member States can make strides towards responsible developments in AI\n\nWhile values and principles are crucial to establishing a basis for any ethical AI framework, recent movements in AI ethics have emphasised the need to move beyond high-level principles and toward practical strategies.\n\nThe Recommendation does just this by setting out eleven key areas for policy actions.\n\nImplementing the Recommendation\n\nWomen4Ethical AI expert platform to advance gender equality\n\nUNESCO's Women4Ethical AI is a new collaborative platform to support governments and companies’ efforts to ensure that women are represented equally in both the design and deployment of AI. The platform’s members will also contribute to the advancement of all the ethical provisions in the Recommendation on the Ethics of AI."}
{"chunk_id": "www_unesco_org__831d1af606b537dd::c0002", "stable_id": "www_unesco_org__831d1af606b537dd", "url": "https://www.unesco.org/en/artificial-intelligence/recommendation-ethics", "final_url": "https://www.unesco.org/en/artificial-intelligence/recommendation-ethics", "retrieved_at": "2026-01-22T00:15:01.228259Z", "title": "Ethics of Artificial Intelligence - AI | UNESCO", "section": "Rules, Risks, and Ethics of AI", "source_type": "University / Official", "content_hash": "d7af3aa55fdf3acc256ac82b93c24ab206f5d7d24bffe728ed0e91d1265ed936", "chunk_index": 2, "char_start": 2433, "char_end": 3836, "text": "The platform unites 17 leading female experts from academia, civil society, the private sector and regulatory bodies, from around the world. They will share research and contribute to a repository of good practices. The platform will drive progress on non-discriminatory algorithms and data sources, and incentivize girls, women and under-represented groups to participate in AI.\n\nBusiness Council for Ethics of AI\n\nThe Business Council for Ethics of AI is a collaborative initiative between UNESCO and companies operating in Latin America that are involved in the development or use of artificial intelligence (AI) in various sectors.\n\nThe Council serves as a platform for companies to come together, exchange experiences, and promote ethical practices within the AI industry. By working closely with UNESCO, it aims to ensure that AI is developed and utilized in a manner that respects human rights and upholds ethical standards.\n\nCurrently co-chaired by Microsoft and Telefonica, the Council is committed to strengthening technical capacities in ethics and AI, designing and implementing the Ethical Impact Assessment tool mandated by the Recommendation on the Ethics of AI, and contributing to the development of intelligent regional regulations. Through these efforts, it strives to create a competitive environment that benefits all stakeholders and promotes the responsible and ethical use of AI."}
{"chunk_id": "iep_utm_edu__b4724b6f15ae5245::c0000", "stable_id": "iep_utm_edu__b4724b6f15ae5245", "url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "final_url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "retrieved_at": "2026-01-22T00:14:58.390095Z", "title": "Ethics of Artificial Intelligence | Internet Encyclopedia of Philosophy", "section": "Rules, Risks, and Ethics of AI", "source_type": "Research / Academic", "content_hash": "9f8ed6c1d97f4e21d96dafbd4fa1143bda57020eae4b1780c3debc92d1ab75a2", "chunk_index": 0, "char_start": 0, "char_end": 1743, "text": "Ethics of Artificial Intelligence\n\nThis article provides a comprehensive overview of the main ethical issues related to the impact of Artificial Intelligence (AI) on human society. AI is the use of machines to do things that would normally require human intelligence. In many areas of human life, AI has rapidly and significantly affected human society and the ways we interact with each other. It will continue to do so. Along the way, AI has presented substantial ethical and socio-political challenges that call for a thorough philosophical and ethical analysis. Its social impact should be studied so as to avoid any negative repercussions. AI systems are becoming more and more autonomous, apparently rational, and intelligent. This comprehensive development gives rise to numerous issues. In addition to the potential harm and impact of AI technologies on our privacy, other concerns include their moral and legal status (including moral and legal rights), their possible moral agency and patienthood, and issues related to their possible personhood and even dignity. It is common, however, to distinguish the following issues as of utmost significance with respect to AI and its relation to human society, according to three different time periods: (1) short-term (early 21st century): autonomous systems (transportation, weapons), machine bias in law, privacy and surveillance, the black box problem and AI decision-making; (2) mid-term (from the 2040s to the end of the century): AI governance, confirming the moral and legal status of intelligent machines (artificial moral agents), human-machine interaction, mass automation; (3) long-term (starting with the 2100s): technological singularity, mass unemployment, space colonisation."}
{"chunk_id": "iep_utm_edu__b4724b6f15ae5245::c0001", "stable_id": "iep_utm_edu__b4724b6f15ae5245", "url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "final_url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "retrieved_at": "2026-01-22T00:14:58.390095Z", "title": "Ethics of Artificial Intelligence | Internet Encyclopedia of Philosophy", "section": "Rules, Risks, and Ethics of AI", "source_type": "Research / Academic", "content_hash": "9f8ed6c1d97f4e21d96dafbd4fa1143bda57020eae4b1780c3debc92d1ab75a2", "chunk_index": 1, "char_start": 1743, "char_end": 3113, "text": "Table of Contents\n\n- The Relevance of AI for Ethics\n\n- Main Debates\n\n- Ethical Guidelines for AI\n\n- Conclusion\n\n- References and Further Reading\n\n1. The Relevance of AI for Ethics\n\nThis section discusses why AI is of utmost importance for our systems of ethics and morality, given the increasing human-machine interaction.\n\na. What is AI?\n\nAI may mean several different things and it is defined in many different ways. When Alan Turing introduced the so-called Turing test (which he called an ‘imitation game’) in his famous 1950 essay about whether machines can think, the term ‘artificial intelligence’ had not yet been introduced. Turing considered whether machines can think, and suggested that it would be clearer to replace that question with the question of whether it might be possible to build machines that could imitate humans so convincingly that people would find it difficult to tell whether, for example, a written message comes from a computer or from a human (Turing 1950).\n\nThe term ‘AI’ was coined in 1955 by a group of researchers—John McCarthy, Marvin L. Minsky, Nathaniel Rochester and Claude E. Shannon—who organised a famous two-month summer workshop at Dartmouth College on the ‘Study of Artificial Intelligence’ in 1956. This event is widely recognised as the very beginning of the study of AI. The organisers described the workshop as follows:"}
{"chunk_id": "iep_utm_edu__b4724b6f15ae5245::c0002", "stable_id": "iep_utm_edu__b4724b6f15ae5245", "url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "final_url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "retrieved_at": "2026-01-22T00:14:58.390095Z", "title": "Ethics of Artificial Intelligence | Internet Encyclopedia of Philosophy", "section": "Rules, Risks, and Ethics of AI", "source_type": "Research / Academic", "content_hash": "9f8ed6c1d97f4e21d96dafbd4fa1143bda57020eae4b1780c3debc92d1ab75a2", "chunk_index": 2, "char_start": 3113, "char_end": 4248, "text": "We propose that a 2-month, 10-man study of artificial intelligence be carried out during the summer of 1956 at Dartmouth College in Hanover, New Hampshire. The study is to proceed on the basis of the conjecture that every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it. An attempt will be made to find how to make machines use language, form abstractions and concepts, solve kinds of problems now reserved for humans, and improve themselves. We think that a significant advance can be made in one or more of these problems if a carefully selected group of scientists work on it together for a summer. (Proposal 1955: 2)\n\nAnother, later scholarly definition describes AI as:\n\nthe ability of a digital computer or computer-controlled robot to perform tasks commonly associated with intelligent beings. The term is frequently applied to the project of developing systems endowed with the intellectual processes characteristic of humans, such as the ability to reason, discover meaning, generalize, or learn from past experience. (Copeland 2020)"}
{"chunk_id": "iep_utm_edu__b4724b6f15ae5245::c0003", "stable_id": "iep_utm_edu__b4724b6f15ae5245", "url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "final_url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "retrieved_at": "2026-01-22T00:14:58.390095Z", "title": "Ethics of Artificial Intelligence | Internet Encyclopedia of Philosophy", "section": "Rules, Risks, and Ethics of AI", "source_type": "Research / Academic", "content_hash": "9f8ed6c1d97f4e21d96dafbd4fa1143bda57020eae4b1780c3debc92d1ab75a2", "chunk_index": 3, "char_start": 4248, "char_end": 5700, "text": "In the early twenty-first century, the ultimate goal of many computer specialists and engineers has been to build a robust AI system which would not differ from human intelligence in any aspect other than its machine origin. Whether this is at all possible has been a matter of lively debate for several decades. The prominent American philosopher John Searle (1980) introduced the so-called Chinese room argument to contend that strong or general AI (AGI)—that is, building AI systems which could deal with many different and complex tasks that require human-like intelligence—is in principle impossible. In doing so, he sparked a long-standing general debate on the possibility of AGI. Current AI systems are narrowly focused (that is, weak AI) and can only solve one particular task, such as playing chess or the Chinese game of Go. Searle’s general thesis was that no matter how complex and sophisticated a machine is, it will nonetheless have no ‘consciousness’ or ‘mind’, which is a prerequisite for the ability to understand, in contrast to the capability to compute (see section 2.e.).\n\nSearle’s argument has been critically evaluated against the counterclaims of functionalism and computationalism. It is generally argued that intelligence does not require a particular substratum, such as carbon-based beings, but that it will also evolve in silicon-based environments, if the system is complex enough (for example, Chalmers 1996, chapter 9)."}
{"chunk_id": "iep_utm_edu__b4724b6f15ae5245::c0004", "stable_id": "iep_utm_edu__b4724b6f15ae5245", "url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "final_url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "retrieved_at": "2026-01-22T00:14:58.390095Z", "title": "Ethics of Artificial Intelligence | Internet Encyclopedia of Philosophy", "section": "Rules, Risks, and Ethics of AI", "source_type": "Research / Academic", "content_hash": "9f8ed6c1d97f4e21d96dafbd4fa1143bda57020eae4b1780c3debc92d1ab75a2", "chunk_index": 4, "char_start": 5700, "char_end": 6479, "text": "In the early years of the twenty-first century, many researchers working on AI development associated AI primarily with different forms of the so-called machine learning—that is, technologies that identify patterns in data. Simpler forms of such systems are said to engage in ‘supervised learning’—which nonetheless still requires considerable human input and supervision—but the aim of many researchers, perhaps most prominently Yann LeCun, had been set to develop the so-called self-supervised learning systems. These days, some researchers began to discuss AI in a way that seems to equate the concept with machine learning. This article, however, uses the term ‘AI’ in a wider sense that includes—but is not limited to—machine learning technologies.\n\nb. Its Ethical Relevance"}
{"chunk_id": "iep_utm_edu__b4724b6f15ae5245::c0005", "stable_id": "iep_utm_edu__b4724b6f15ae5245", "url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "final_url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "retrieved_at": "2026-01-22T00:14:58.390095Z", "title": "Ethics of Artificial Intelligence | Internet Encyclopedia of Philosophy", "section": "Rules, Risks, and Ethics of AI", "source_type": "Research / Academic", "content_hash": "9f8ed6c1d97f4e21d96dafbd4fa1143bda57020eae4b1780c3debc92d1ab75a2", "chunk_index": 5, "char_start": 6479, "char_end": 8135, "text": "The major ethical challenges for human societies AI poses are presented well in the excellent introductions by Vincent Müller (2020), Mark Coeckelbergh (2020), Janina Loh (2019), Catrin Misselhorn (2018) and David Gunkel (2012). Regardless of the possibility of construing AGI, autonomous AI systems already raise substantial ethical issues: for example, the machine bias in law, making hiring decisions by means of smart algorithms, racist and sexist chatbots, or non-gender-neutral language translations (see section 2.c.). The very idea of a machine ‘imitating’ human intelligence—which is one common definition of AI—gives rise to worries about deception, especially if the AI is built into robots designed to look or act like human beings (Boden et al. 2017; Nyholm and Frank 2019). Moreover, Rosalind Picard rightly claims that ‘the greater the freedom of a machine, the more it will need moral standards’ (1997: 19). This substantiates the claim that all interactions between AI systems and human beings necessarily entail an ethical dimension, for example, in the context of autonomous transportation (see section 2.d.).\n\nThe idea of implementing ethics within a machine is one of the main research goals in the field of machine ethics (for example, Lin et al. 2012; Anderson and Anderson 2011; Wallach and Allen 2009). More and more responsibility has been shifted from human beings to autonomous AI systems which are able to work much faster than human beings without taking any breaks and with no need for constant supervision, as illustrated by the excellent performance of many systems (once they have successfully passed the debugging phase)."}
{"chunk_id": "iep_utm_edu__b4724b6f15ae5245::c0006", "stable_id": "iep_utm_edu__b4724b6f15ae5245", "url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "final_url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "retrieved_at": "2026-01-22T00:14:58.390095Z", "title": "Ethics of Artificial Intelligence | Internet Encyclopedia of Philosophy", "section": "Rules, Risks, and Ethics of AI", "source_type": "Research / Academic", "content_hash": "9f8ed6c1d97f4e21d96dafbd4fa1143bda57020eae4b1780c3debc92d1ab75a2", "chunk_index": 6, "char_start": 8135, "char_end": 9565, "text": "It has been suggested that humanity’s future existence may depend on the implementation of solid moral standards in AI systems, given the possibility that these systems may, at some point, either match or supersede human capabilities (see section 2.g.). This point in time was called ‘technological singularity’ by Vernon Vinge in 1983 (see also: Vinge 1993; Kurzweil 2005; Chalmers 2010). The famous playwright Karl Čapek (1920), the renowned astrophysicist Stephen Hawking and the influential philosopher Nick Bostrom (2016, 2018) have all warned about the possible dangers of technological singularity should intelligent machines turn against their creators, that is, human beings. Therefore, according to Nick Bostrom, it is of utmost importance to build friendly AI (see the alignment problem, discussed in section 2.g.).\n\nIn conclusion, the implementation of ethics is crucial for AI systems for multiple reasons: to provide safety guidelines that can prevent existential risks for humanity, to solve any issues related to bias, to build friendly AI systems that will adopt our ethical standards, and to help humanity flourish.\n\n2. Main Debates\n\nThe following debates are of utmost significance in the context of AI and ethics. They are not the only important debates in the field, but they provide a good overview of topics that will likely remain of great importance for many decades (for a similar list, see Müller 2020)."}
{"chunk_id": "iep_utm_edu__b4724b6f15ae5245::c0007", "stable_id": "iep_utm_edu__b4724b6f15ae5245", "url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "final_url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "retrieved_at": "2026-01-22T00:14:58.390095Z", "title": "Ethics of Artificial Intelligence | Internet Encyclopedia of Philosophy", "section": "Rules, Risks, and Ethics of AI", "source_type": "Research / Academic", "content_hash": "9f8ed6c1d97f4e21d96dafbd4fa1143bda57020eae4b1780c3debc92d1ab75a2", "chunk_index": 7, "char_start": 9565, "char_end": 10938, "text": "a. Machine Ethics\n\nSusan Anderson, a pioneer of machine ethics, defines the goal of machine ethics as:\n\nto create a machine that follows an ideal ethical principle or set of principles in guiding its behaviour; in other words, it is guided by this principle, or these principles, in the decisions it makes about possible courses of action it could take. We can say, more simply, that this involves “adding an ethical dimension” to the machine. (2011: 22)\n\nIn addition, the study of machine ethics examines issues regarding the moral status of intelligent machines and asks whether they should be entitled to moral and legal rights (Gordon 2020a, 2020b; Richardson 2019; Gunkel and Bryson 2014; Gunkel 2012; Anderson and Anderson 2011; Wallach and Allen 2010). In general, machine ethics is an interdisciplinary sub-discipline of the ethics of technology, which is in turn a discipline within applied ethics. The ethics of technology also contains the sub-disciplines of robot ethics (see, for example, Lin et al. 2011, 2017; Gunkel 2018; Nyholm 2020), which is concerned with questions of how human beings design, construct and use robots; and computer ethics (for example, Johnson 1985/2009; Johnson and Nissenbaum 1995; Himma and Tavani 2008), which is concerned with =commercial behaviour involving computers and information (for example, data security, privacy issues)."}
{"chunk_id": "iep_utm_edu__b4724b6f15ae5245::c0008", "stable_id": "iep_utm_edu__b4724b6f15ae5245", "url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "final_url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "retrieved_at": "2026-01-22T00:14:58.390095Z", "title": "Ethics of Artificial Intelligence | Internet Encyclopedia of Philosophy", "section": "Rules, Risks, and Ethics of AI", "source_type": "Research / Academic", "content_hash": "9f8ed6c1d97f4e21d96dafbd4fa1143bda57020eae4b1780c3debc92d1ab75a2", "chunk_index": 8, "char_start": 10938, "char_end": 12192, "text": "The first ethical code for AI systems was introduced by the famed science fiction writer Isaac Asimov, who presented his Three Laws of Robotics in Runaround (Asimov 1942). These three were later supplemented by a fourth law, called the Zeroth Law of Robotics, in Robots and Empire (Asimov 1986). The four laws are as follows:\n\n- A robot may not injure a human being or, through inaction, allow a human being to be harmed;\n\n- A robot must obey the orders given it by human beings except where such orders would conflict with the first law;\n\n- A robot must protect its own existence as long as such protection does not conflict with the first or second law;\n\n- A robot may not harm humanity or, by inaction, allow humanity to suffer harm.\n\nAsimov’s four laws have played a major role in machine ethics for many decades and have been widely discussed by experts. The standard view regarding the four laws is that they are important but insufficient to deal with all the complexities related to moral machines. This seems to be a fair evaluation, since Asimov never claimed that his laws could cope with all issues. If that was really the case, then Asimov would perhaps not have written his fascinating stories about problems caused partly by the four laws."}
{"chunk_id": "iep_utm_edu__b4724b6f15ae5245::c0009", "stable_id": "iep_utm_edu__b4724b6f15ae5245", "url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "final_url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "retrieved_at": "2026-01-22T00:14:58.390095Z", "title": "Ethics of Artificial Intelligence | Internet Encyclopedia of Philosophy", "section": "Rules, Risks, and Ethics of AI", "source_type": "Research / Academic", "content_hash": "9f8ed6c1d97f4e21d96dafbd4fa1143bda57020eae4b1780c3debc92d1ab75a2", "chunk_index": 9, "char_start": 12192, "char_end": 13783, "text": "The early years of the twenty-first century saw the proposal of numerous approaches to implementing ethics within machines, to provide AI systems with ethical principles that the machines could use in making moral decisions (Gordon 2020a). We can distinguish at least three types of approaches: bottom-up, top-down, and mixed. An example of each type is provided below (see also Gordon 2020a: 147).\n\ni. Bottom-up Approaches: Casuistry\n\nGuarini’s (2006) system is an example of a bottom-up approach. It uses a neural network which bases its ethical decisions on a learning process in which the neural network is presented with known correct answers to ethical dilemmas. After the initial learning process, the system is supposed to be able to solve new ethical dilemmas on its own. However, Guarini’s system generates problems concerning the reclassification of cases, caused by the lack of adequate reflection and exact representation of the situation. Guarini himself admits that casuistry alone is insufficient for machine ethics.\n\nii. Top-down Approaches: The MoralDM Approach\n\nThe system conceived by Dehghani et al. (2011) combines two main ethical theories, utilitarianism and deontology, along with analogical reasoning. Utilitarian reasoning applies until ‘sacred values’ are concerned, at which point the system operates in a deontological mode and becomes less sensitive to the utility of actions and consequences. To align the system with human moral decisions, Dehghani et al. evaluate it against psychological studies of how the majority of human beings decide particular cases."}
{"chunk_id": "iep_utm_edu__b4724b6f15ae5245::c0010", "stable_id": "iep_utm_edu__b4724b6f15ae5245", "url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "final_url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "retrieved_at": "2026-01-22T00:14:58.390095Z", "title": "Ethics of Artificial Intelligence | Internet Encyclopedia of Philosophy", "section": "Rules, Risks, and Ethics of AI", "source_type": "Research / Academic", "content_hash": "9f8ed6c1d97f4e21d96dafbd4fa1143bda57020eae4b1780c3debc92d1ab75a2", "chunk_index": 10, "char_start": 13783, "char_end": 14361, "text": "The MoralDM approach is particularly successful in that it pays proper respect to the two main ethical theories (deontology and utilitarianism) and combines them in a fruitful and promising way. However, their additional strategy of using empirical studies to mirror human moral decisions by considering as correct only those decisions that align with the majority view is misleading and seriously flawed. Rather, their system should be seen as a model of a descriptive study of ethical behaviour but not a model for normative ethics.\n\niii. Mixed Approaches: The Hybrid Approach"}
{"chunk_id": "iep_utm_edu__b4724b6f15ae5245::c0011", "stable_id": "iep_utm_edu__b4724b6f15ae5245", "url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "final_url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "retrieved_at": "2026-01-22T00:14:58.390095Z", "title": "Ethics of Artificial Intelligence | Internet Encyclopedia of Philosophy", "section": "Rules, Risks, and Ethics of AI", "source_type": "Research / Academic", "content_hash": "9f8ed6c1d97f4e21d96dafbd4fa1143bda57020eae4b1780c3debc92d1ab75a2", "chunk_index": 11, "char_start": 14361, "char_end": 15707, "text": "The hybrid model of human cognition (Wallach et al. 2010; Wallach and Allen 2010) combines a top-down component (theory-driven reasoning) and a bottom-up (shaped by evolution and learning) component that are considered the basis of both moral reasoning and decision-making. The result thus far is LIDA, an AGI software offering a comprehensive conceptual and computational model that models a large portion of human cognition. The hybrid model of moral reasoning attempts to re-create human decision-making by appealing to a complex combination of top-down and bottom-up approaches leading eventually to a descriptive but not a normative model of ethics. In addition, its somewhat idiosyncratic understanding of both approaches from moral philosophy does not in fact match how moral philosophers understand and use them in normative ethics. The model presented by Wallach et al. is not necessarily inaccurate with respect to how moral decision-making works in an empirical sense, but their approach is descriptive rather than normative in nature. Therefore, their empirical model does not solve the normative problem of how moral machines should act. Descriptive ethics and normative ethics are two different things. The former tells us how human beings make moral decisions; the latter is concerned with how we should act.\n\nb. Autonomous Systems"}
{"chunk_id": "iep_utm_edu__b4724b6f15ae5245::c0012", "stable_id": "iep_utm_edu__b4724b6f15ae5245", "url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "final_url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "retrieved_at": "2026-01-22T00:14:58.390095Z", "title": "Ethics of Artificial Intelligence | Internet Encyclopedia of Philosophy", "section": "Rules, Risks, and Ethics of AI", "source_type": "Research / Academic", "content_hash": "9f8ed6c1d97f4e21d96dafbd4fa1143bda57020eae4b1780c3debc92d1ab75a2", "chunk_index": 12, "char_start": 15707, "char_end": 17064, "text": "The proposals for a system of machine ethics discussed in section 2.a. are increasingly being discussed in relation to autonomous systems the operation of which poses a risk of harm to human life. The two most-often discussed examples—which are at times discussed together and contrasted and compared with each other—are autonomous vehicles (also known as self-driving cars) and autonomous weapons systems (sometimes dubbed ‘killer robots’) (Purves et al. 2015; Danaher 2016; Nyholm 2018a).\n\nSome authors think that autonomous weapons might be a good replacement for human soldiers (Müller and Simpson 2014). For example, Arkin (2009, 2010) argues that having machines fight our wars for us instead of human soldiers could lead to a decrease in war crimes if the machines were equipped with an ‘ethical governor’ system that would consistently follow the rules of war and engagement. However, others worry about the widespread availability of AI-driven autonomous weapons systems, because they think the availability of such systems might tempt people to go to war more often, or because they are sceptical about the possibility of an AI system that could interpret and apply the ethical and legal principles of war (see, for example, Royakkers and van Est 2015; Strawser 2010). There are also worries that ‘killer robots’ might be hacked (Klincewicz 2015)."}
{"chunk_id": "iep_utm_edu__b4724b6f15ae5245::c0013", "stable_id": "iep_utm_edu__b4724b6f15ae5245", "url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "final_url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "retrieved_at": "2026-01-22T00:14:58.390095Z", "title": "Ethics of Artificial Intelligence | Internet Encyclopedia of Philosophy", "section": "Rules, Risks, and Ethics of AI", "source_type": "Research / Academic", "content_hash": "9f8ed6c1d97f4e21d96dafbd4fa1143bda57020eae4b1780c3debc92d1ab75a2", "chunk_index": 13, "char_start": 17064, "char_end": 18669, "text": "Similarly, while acknowledging the possible benefits of self-driving cars—such as increased traffic safety, more efficient use of fuel and better-coordinated traffic—many authors have also noted the possible accidents that could occur (Goodall 2014; Lin 2015; Gurney 2016; Nyholm 2018b, 2018c; Keeling 2020). The underlying idea is that autonomous vehicles should be equipped with ‘ethics settings’ that would help to determine how they should react to accident scenarios where people’s lives and safety are at stake (Gogoll and Müller 2017). This is considered another real-life application of machine ethics that society urgently needs to grapple with.\n\nThe concern for self-driving cars being involved in deadly accidents for which the AI system may not have been adequately prepared has already been realised, tragically, as some people have died in such accidents (Nyholm 2018b). The first instance of death while riding in an autonomous vehicle—a Tesla Model S car in ‘autopilot’ mode—occurred in May 2016. The first pedestrian was hit and killed by an experimental self-driving car, operated by the ride-hailing company Uber, in March 2018. In the latter case, part of the problem was that the AI system in the car had difficulty classifying the object that suddenly appeared in its path. It initially classified the victim as ‘unknown’, then as a ‘vehicle’, and finally as a ‘bicycle’. Just moments before the crash, the system decided to apply the brakes, but by then it was too late (Keeling 2020: 146). Whether the AI system in the car functions properly can thus be a matter of life and death."}
{"chunk_id": "iep_utm_edu__b4724b6f15ae5245::c0014", "stable_id": "iep_utm_edu__b4724b6f15ae5245", "url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "final_url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "retrieved_at": "2026-01-22T00:14:58.390095Z", "title": "Ethics of Artificial Intelligence | Internet Encyclopedia of Philosophy", "section": "Rules, Risks, and Ethics of AI", "source_type": "Research / Academic", "content_hash": "9f8ed6c1d97f4e21d96dafbd4fa1143bda57020eae4b1780c3debc92d1ab75a2", "chunk_index": 14, "char_start": 18669, "char_end": 20397, "text": "Philosophers discussing such cases may propose that, even when it cannot brake in time, the car might swerve to one side (for example, Goodall 2014; Lin 2015). But what if five people were on the only side of the road the car could swerve onto? Or what if five people appeared on the road and one person was on the curb where the car might swerve? These scenarios are similar to the much-discussed ‘trolley problem’: the choice would involve killing one person to save five, and the question would become under what sorts of circumstances that decision would or would not be permissible. Several papers have discussed relevant similarities and differences between the ethics of crashes involving self-driving cars, on the one hand, and the philosophy of the trolley problem, on the other (Lin 2015; Nyholm and Smids 2016; Goodall 2016; Himmelreich 2018; Keeling 2020; Kamm 2020).\n\nOne question that has occupied ethicists discussing autonomous systems is what ethical principles should govern their decision-making process in situations that might involve harm to human beings. A related issue is whether it is ever acceptable for autonomous machines to kill or harm human beings, particularly if they do so in a manner governed by certain principles that have been programmed into or made part of the machines in another way. Here, a distinction is made between deaths caused by self-driving cars—which are generally considered a deeply regrettable but foreseeable side effect of their use—and killing by autonomous weapons systems, which some consider always morally unacceptable (Purves et al. 2015). Even a campaign has been launched to ‘stop killer robots’, backed by many AI ethicists such as Noel Sharkey and Peter Asaro."}
{"chunk_id": "iep_utm_edu__b4724b6f15ae5245::c0015", "stable_id": "iep_utm_edu__b4724b6f15ae5245", "url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "final_url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "retrieved_at": "2026-01-22T00:14:58.390095Z", "title": "Ethics of Artificial Intelligence | Internet Encyclopedia of Philosophy", "section": "Rules, Risks, and Ethics of AI", "source_type": "Research / Academic", "content_hash": "9f8ed6c1d97f4e21d96dafbd4fa1143bda57020eae4b1780c3debc92d1ab75a2", "chunk_index": 15, "char_start": 20397, "char_end": 21979, "text": "One reason for arguing that autonomous weapons systems should be banned the campaign puts forward is that what they call ‘meaningful human control’ must be retained. This concept is also discussed in relation to self-driving cars (Santoni de Sio and van den Hoven 2018). Many authors have worried about the risk of creating ‘responsibility gaps’, or cases in which it is unclear who should be held responsible for harm that has occurred due to the decisions made by an autonomous AI system (Matthias 2004; Sparrow 2007; Danaher 2016). The key challenge here is to come up with a way of understanding moral responsibility in the context of autonomous systems that would allow us to secure the benefits of such systems and at the same time appropriately attribute responsibility for any undesirable consequences. If a machine causes harm, the human beings involved in the machine’s action may try to evade responsibility; indeed, in some cases it might seem unfair to blame people for what a machine has done. Of course, if an autonomous system produces a good outcome, which some human beings, if any, claim to deserve praise for, the result might be equally unclear. In general, people may be more willing to take responsibility for good outcomes produced by autonomous systems than for bad ones. But in both situations, responsibility gaps can arise. Accordingly, philosophers need to formulate a theory of how to allocate responsibility for outcomes produced by functionally autonomous AI technologies, whether good or bad (Nyholm 2018a; Dignum 2019; Danaher 2019a; Tigard 2020a)."}
{"chunk_id": "iep_utm_edu__b4724b6f15ae5245::c0016", "stable_id": "iep_utm_edu__b4724b6f15ae5245", "url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "final_url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "retrieved_at": "2026-01-22T00:14:58.390095Z", "title": "Ethics of Artificial Intelligence | Internet Encyclopedia of Philosophy", "section": "Rules, Risks, and Ethics of AI", "source_type": "Research / Academic", "content_hash": "9f8ed6c1d97f4e21d96dafbd4fa1143bda57020eae4b1780c3debc92d1ab75a2", "chunk_index": 16, "char_start": 21979, "char_end": 23216, "text": "c. Machine Bias\n\nMany people believe that the use of smart technologies would put an end to human bias because of the supposed ‘neutrality’ of machines. However, we have come to realise that machines may maintain and even substantiate human bias towards women, different ethnicities, the elderly, people with medical impairments, or other groups (Kraemer et al. 2011; Mittelstadt et al. 2016). As a consequence, one of the most urgent questions in the context of machine learning is how to avoid machine bias (Daniels et al. 2019). The idea of using AI systems to support human decision-making is, in general, an excellent objective in view of AI’s ‘increased efficiency, accuracy, scale and speed in making decisions and finding the best answers’ (World Economic Forum 2018: 6). However, machine bias can undermine this seemingly positive situation in various ways. Some striking cases of machine bias are as follows:\n\n- Gender bias in hiring (Dastin 2018);\n\n- Racial bias, in that certain racial groups are offered only particular types of jobs (Sweeney 2013);\n\n- Racial bias in decisions on the creditworthiness of loan applicants (Ludwig 2015);\n\n- Racial bias in decisions whether to release prisoners on parole (Angwin et al. 2016);"}
{"chunk_id": "iep_utm_edu__b4724b6f15ae5245::c0017", "stable_id": "iep_utm_edu__b4724b6f15ae5245", "url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "final_url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "retrieved_at": "2026-01-22T00:14:58.390095Z", "title": "Ethics of Artificial Intelligence | Internet Encyclopedia of Philosophy", "section": "Rules, Risks, and Ethics of AI", "source_type": "Research / Academic", "content_hash": "9f8ed6c1d97f4e21d96dafbd4fa1143bda57020eae4b1780c3debc92d1ab75a2", "chunk_index": 17, "char_start": 23216, "char_end": 24870, "text": "- Racial bias in predicting criminal activities in urban areas (O’Neil 2016);\n\n- Sexual bias when identifying a person’s sexual orientation (Wang and Kosinski 2018);\n\n- Racial bias in facial recognition systems that prefer lighter skin colours (Buolamwini and Gebru 2018);\n\n- Racial and social bias in using the geographic location of a person’s residence as a proxy for ethnicity or socio-economic status (Veale and Binns 2017).\n\nWe can recognise at least three reasons for machine bias: (1) data bias, (2) computational/algorithmic bias and (3) outcome bias (Springer et al. 2018: 451). First, a machine learning system that is trained using data that contain implicit or explicit imbalances reinforces the distortion in the data with respect to any future decision-making, thereby making the bias systematic. Second, a programme may suffer from algorithmic bias due to the developer’s implicit or explicit biases. The design of a programme relies on the developer’s understanding of the normative and non-normative values of other people, including the users and stakeholders affected by it (Dobbe et al. 2018). Third, outcome bias could be based on the use of historical records, for example, to predict criminal activities in certain particular urban areas; the system may allocate more police to a particular area, resulting in an increase in reported cases which would have been unnoticed before. This logic would substantiate the AI system’s decision to allocate the police to this area, even though other urban areas may have similar or even greater numbers of crimes, more of which would go unreported due to the lack of policing (O’Neil 2016)."}
{"chunk_id": "iep_utm_edu__b4724b6f15ae5245::c0018", "stable_id": "iep_utm_edu__b4724b6f15ae5245", "url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "final_url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "retrieved_at": "2026-01-22T00:14:58.390095Z", "title": "Ethics of Artificial Intelligence | Internet Encyclopedia of Philosophy", "section": "Rules, Risks, and Ethics of AI", "source_type": "Research / Academic", "content_hash": "9f8ed6c1d97f4e21d96dafbd4fa1143bda57020eae4b1780c3debc92d1ab75a2", "chunk_index": 18, "char_start": 24870, "char_end": 26409, "text": "Most AI researchers, programmers and developers as well as scholars working in the field of technology believe that we will never be able to design a fully unbiased system. Therefore, the focus is on reducing machine bias and minimising its detrimental effects on human beings. Nevertheless, various questions remain. What type of bias cannot be filtered out and when should we be satisfied with the remaining bias? What does it mean for a person in court to be subject not only to human bias but also to machine bias, with both forms of injustice potentially helping to determine the person’s sentence? Is one type of bias not enough? Should we not rather aim to eliminate human bias instead of introducing a new one?\n\nd. The Problem of Opacity\n\nAI systems are used to make many sorts of decisions that significantly impact people’s lives. AI can be used to make decisions about who gets a loan, who is admitted to a university, who gets an advertised job, who is likely to reoffend, and so on. Since these decisions have major impacts on people, we must be able to understand the underlying reasons for them. In other words, AI and its decision-making need to be explainable. In fact, many authors discussing the ethics of AI propose explainability (also referred to as explicability) as a basic ethical criterion, among others, for the acceptability of AI decision-making (Floridi et al. 2018). However, many decisions made by an autonomous AI system are not readily explainable to people. This came to be called the problem of opacity."}
{"chunk_id": "iep_utm_edu__b4724b6f15ae5245::c0019", "stable_id": "iep_utm_edu__b4724b6f15ae5245", "url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "final_url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "retrieved_at": "2026-01-22T00:14:58.390095Z", "title": "Ethics of Artificial Intelligence | Internet Encyclopedia of Philosophy", "section": "Rules, Risks, and Ethics of AI", "source_type": "Research / Academic", "content_hash": "9f8ed6c1d97f4e21d96dafbd4fa1143bda57020eae4b1780c3debc92d1ab75a2", "chunk_index": 19, "char_start": 26409, "char_end": 28107, "text": "The opacity of AI decision-making can be of different kinds, depending on relevant factors. Some AI decisions are opaque to those who are affected by them because the algorithms behind the decisions, though quite easy to understand, are protected trade secrets which the companies using them do not want to share with anyone outside the company. Another reason for AI opacity is that most people lack the technical expertise to understand how an AI-based system works, even if there is nothing intrinsically opaque about the technology in question. With some forms of AI, not even the experts can understand the decision-making processes used. This has been dubbed the ‘black box’ problem (Wachter, Mittelstadt and Russell 2018).\n\nOn the individual level, it can seem to be an affront to a person’s dignity and autonomy when decisions about important aspects of their lives are made by machines if it is unclear—or perhaps even impossible to know—why machines made these decisions. On the societal level, the increasing prominence of algorithmic decision-making could become a threat to our democratic processes. Henry Kissinger, the former U.S. Secretary of State, once stated, ‘We may have created a dominating technology in search of a guiding philosophy’ (Kissinger 2018; quoted in Müller 2020). John Danaher, commenting on this idea, worries that people might be led to act in superstitious and irrational ways, like those in earlier times who believed that they could affect natural phenomena through rain dances or similar behaviour. Danaher has called this situation ‘the threat of algocracy’—that is, of rule by algorithms that we do not understand but have to obey (Danaher 2016b, 2019b)."}
{"chunk_id": "iep_utm_edu__b4724b6f15ae5245::c0020", "stable_id": "iep_utm_edu__b4724b6f15ae5245", "url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "final_url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "retrieved_at": "2026-01-22T00:14:58.390095Z", "title": "Ethics of Artificial Intelligence | Internet Encyclopedia of Philosophy", "section": "Rules, Risks, and Ethics of AI", "source_type": "Research / Academic", "content_hash": "9f8ed6c1d97f4e21d96dafbd4fa1143bda57020eae4b1780c3debc92d1ab75a2", "chunk_index": 20, "char_start": 28107, "char_end": 29023, "text": "But is AI opacity always, and necessarily, a problem? Is it equally problematic across all contexts? Should there be an absolute requirement that AI must in all cases be explainable? Scott Robbins (2019) has provided some interesting and noteworthy arguments in opposition to this idea. Robbins argues, among other things, that a hard requirement for explicability could prevent us from reaping all the possible benefits of AI. For example, he points out that if an AI system could reliably detect or predict some form of cancer in a way that we cannot explain or understand, the value of knowing the information would outweigh any concerns about not knowing how the AI system would have reached this conclusion. In general, it is also possible to distinguish between contexts where the procedure behind a decision matters in itself and those where only the quality of the outcome matters (Danaher and Robbins 2020)."}
{"chunk_id": "iep_utm_edu__b4724b6f15ae5245::c0021", "stable_id": "iep_utm_edu__b4724b6f15ae5245", "url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "final_url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "retrieved_at": "2026-01-22T00:14:58.390095Z", "title": "Ethics of Artificial Intelligence | Internet Encyclopedia of Philosophy", "section": "Rules, Risks, and Ethics of AI", "source_type": "Research / Academic", "content_hash": "9f8ed6c1d97f4e21d96dafbd4fa1143bda57020eae4b1780c3debc92d1ab75a2", "chunk_index": 21, "char_start": 29023, "char_end": 30039, "text": "Another promising response to the problem of opacity is to try to construct alternative modes of explaining AI decisions that would take into account their opacity but would nevertheless offer some form of explanation that people could act on. Sandra Wachter, Brent Mittelstadt, and Chris Russell (2019) have developed the idea of a ‘counterfactual explanation’ of such decisions, one designed to offer practical guidance for people wishing to respond rationally to AI decisions they do not understand. They state that ‘counterfactual explanations do not attempt to clarify how [AI] decisions are made internally. Instead, they provide insight into which external facts could be different in order to arrive at a desired outcome’ (Wachter et al. 2018: 880). Such an external, counterfactual way of explaining AI decisions might be a promising alternative in cases where AI decision-making is highly valuable but functions according to an internal logic that is opaque to most or all people.\n\ne. Machine Consciousness"}
{"chunk_id": "iep_utm_edu__b4724b6f15ae5245::c0022", "stable_id": "iep_utm_edu__b4724b6f15ae5245", "url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "final_url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "retrieved_at": "2026-01-22T00:14:58.390095Z", "title": "Ethics of Artificial Intelligence | Internet Encyclopedia of Philosophy", "section": "Rules, Risks, and Ethics of AI", "source_type": "Research / Academic", "content_hash": "9f8ed6c1d97f4e21d96dafbd4fa1143bda57020eae4b1780c3debc92d1ab75a2", "chunk_index": 22, "char_start": 30039, "char_end": 31432, "text": "Some researchers think that when machines become more and more sophisticated and intelligent, they might at some point become spontaneously conscious as well (compare Russell 2019). This would be a sort of puzzling—but potentially highly significant from an ethical standpoint—side effect of the development of advanced AI. Some people are intentionally seeking to create machines with artificial consciousness. Kunihiro Asada, a successful engineer, set his goal as to create a robot that can experience pleasure and pain, on the basis that such a robot could engage in the kind of pre-linguistic learning that a human baby is capable of before it acquires language (Marchese 2020). Another example is Sophia the robot, whose developers at Hanson Robotics say that they wish to create a ‘super-intelligent benevolent being’ that will eventually become a ‘conscious, living machine’.\n\nOthers, such as Joanna Bryson, note that depending on how we define consciousness, some machines might already have some form of consciousness. Bryson argues that if we take consciousness to mean the presence of internal states and the ability to report on these states to other agents, then some machines might fulfil these criteria even now (Bryson 2012). In addition, Aïda Elamrani-Raoult and Roman Yampolskiy (2018) have identified as many as twenty-one different possible tests of machine consciousness."}
{"chunk_id": "iep_utm_edu__b4724b6f15ae5245::c0023", "stable_id": "iep_utm_edu__b4724b6f15ae5245", "url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "final_url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "retrieved_at": "2026-01-22T00:14:58.390095Z", "title": "Ethics of Artificial Intelligence | Internet Encyclopedia of Philosophy", "section": "Rules, Risks, and Ethics of AI", "source_type": "Research / Academic", "content_hash": "9f8ed6c1d97f4e21d96dafbd4fa1143bda57020eae4b1780c3debc92d1ab75a2", "chunk_index": 23, "char_start": 31432, "char_end": 33197, "text": "Moreover, similar claims could be made about the issue of whether machines can have minds. If mind is defined, at least in part, in a functional way, as the internal processing of inputs from the external environment that generates seemingly intelligent responses to that environment, then machines could possess minds (Nyholm 2020: 145–46). Of course, even if machines can be said to have minds or consciousness in some sense, they would still not necessarily be anything like human minds. After all, the particular consciousness and subjectivity of any being will depend on what kinds of ‘hardware’ (such as brains, sense organs, and nervous systems) the being in question has (Nagel 1974).\n\nWhether or not we think some AI machines are already conscious or that they could (either by accident or by design) become conscious, this issue is a key source of ethical controversy. Thomas Metzinger (2013), for example, argues that society should adopt, as a basic principle of AI ethics, a rule against creating machines that are capable of suffering. His argument is simple: suffering is bad, it is immoral to cause suffering, and therefore it would be immoral to create machines that suffer. Joanna Bryson contends similarly that although it is possible to create machines that would have a significant moral status, it is best to avoid doing so; in her view, we are morally obligated not to create machines to which we would have obligations (Bryson 2010, 2019). Again, this might all depend on what we understand by consciousness. Accordingly, Eric Schwitzgebel and Mara Garza (2015: 114–15) comment, ‘If society continues on the path towards developing more sophisticated artificial intelligence, developing a good theory of consciousness is a moral imperative’."}
{"chunk_id": "iep_utm_edu__b4724b6f15ae5245::c0024", "stable_id": "iep_utm_edu__b4724b6f15ae5245", "url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "final_url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "retrieved_at": "2026-01-22T00:14:58.390095Z", "title": "Ethics of Artificial Intelligence | Internet Encyclopedia of Philosophy", "section": "Rules, Risks, and Ethics of AI", "source_type": "Research / Academic", "content_hash": "9f8ed6c1d97f4e21d96dafbd4fa1143bda57020eae4b1780c3debc92d1ab75a2", "chunk_index": 24, "char_start": 33197, "char_end": 34342, "text": "Another interesting perspective is provided by Nicholas Agar (2019), who suggests that if there are arguments both in favour of and against the possibility that certain advanced machines have minds and consciousness, we should err on the side of caution and proceed on the assumption that machines do have minds. On this basis, we should then avoid any actions that might conceivably cause them to suffer. In contrast, John Danaher (2020) states that we can never be sure as to whether a machine has conscious experience, but that this uncertainty does not matter; if a machine behaves similarly to how conscious beings with moral status behave, this is sufficient moral reason, according to Danaher’s ‘ethical behaviourism’, to treat the machine with the same moral considerations with which we would treat a conscious being. The standard approach considers whether machines do actually have conscious minds and then how this answer should influence the question of whether to grant machines moral status (see, for example, Schwitzgebel and Garza 2015; Mosakas 2020; Nyholm 2020: 115–16).\n\nf. The Moral Status of Artificial Intelligent Machines"}
{"chunk_id": "iep_utm_edu__b4724b6f15ae5245::c0025", "stable_id": "iep_utm_edu__b4724b6f15ae5245", "url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "final_url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "retrieved_at": "2026-01-22T00:14:58.390095Z", "title": "Ethics of Artificial Intelligence | Internet Encyclopedia of Philosophy", "section": "Rules, Risks, and Ethics of AI", "source_type": "Research / Academic", "content_hash": "9f8ed6c1d97f4e21d96dafbd4fa1143bda57020eae4b1780c3debc92d1ab75a2", "chunk_index": 25, "char_start": 34342, "char_end": 35569, "text": "Traditionally, the concept of moral status has been of utmost importance in ethics and moral philosophy because entities that have a moral status are considered part of the moral community and are entitled to moral protection. Not all members of a moral community have the same moral status, and therefore they differ with respect to their claims to moral protection. For example, dogs and cats are part of our moral community, but they do not enjoy the same moral status as a typical adult human being. If a being has a moral status, then it has certain moral (and legal) rights as well. The twentieth century saw a growth in the recognition of the rights of ethnic minorities, women, and the LGBTQ+ community, and even the rights of animals and the environment. This expanding moral circle may eventually grow further to include artificial intelligent machines once they exist (as advocated by the robot rights movement).\n\nThe notion of personhood (whatever that may mean) has become relevant in determining whether an entity has full moral status and whether, depending on its moral status, it should enjoy the full set of moral rights. One prominent definition of moral status has been provided by Frances Kamm (2007: 229):"}
{"chunk_id": "iep_utm_edu__b4724b6f15ae5245::c0026", "stable_id": "iep_utm_edu__b4724b6f15ae5245", "url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "final_url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "retrieved_at": "2026-01-22T00:14:58.390095Z", "title": "Ethics of Artificial Intelligence | Internet Encyclopedia of Philosophy", "section": "Rules, Risks, and Ethics of AI", "source_type": "Research / Academic", "content_hash": "9f8ed6c1d97f4e21d96dafbd4fa1143bda57020eae4b1780c3debc92d1ab75a2", "chunk_index": 26, "char_start": 35569, "char_end": 36988, "text": "So, we see that within the class of entities that count in their own right, there are those entities that in their own right and for their own sake could give us reason to act. I think that it is this that people have in mind when they ordinarily attribute moral status to an entity. So, henceforth, I shall distinguish between an entity’s counting morally in its own right and its having moral status. I shall say that an entity has moral status when, in its own right and for its own sake, it can give us reason to do things such as not destroy it or help it.\n\nThings can be done for X’s own sake, according to Kamm, if X is either conscious and/or able to feel pain. This definition usually includes human beings and most animals, whereas non-living parts of nature are mainly excluded on the basis of their lack of consciousness and inability to feel pain. However, there are good reasons why one should broaden their moral reasoning and decision-making to encompass the environment as well (Stone 1972, 2010; Atapattu 2015). For example, the Grand Canyon could be taken into moral account in human decision-making, given its unique form and great aesthetic value, even though it lacks personhood and therefore moral status. Furthermore, some experts have treated sentient animals such as great apes and elephants as persons even though they are not human (for example, Singer 1975; Cavalieri 2001; Francione 2009)."}
{"chunk_id": "iep_utm_edu__b4724b6f15ae5245::c0027", "stable_id": "iep_utm_edu__b4724b6f15ae5245", "url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "final_url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "retrieved_at": "2026-01-22T00:14:58.390095Z", "title": "Ethics of Artificial Intelligence | Internet Encyclopedia of Philosophy", "section": "Rules, Risks, and Ethics of AI", "source_type": "Research / Academic", "content_hash": "9f8ed6c1d97f4e21d96dafbd4fa1143bda57020eae4b1780c3debc92d1ab75a2", "chunk_index": 27, "char_start": 36988, "char_end": 38220, "text": "In addition, we can raise the important question of whether (a) current robots used in social situations or (b) artificial intelligent machines, once they are created, might have a moral status and be entitled to moral rights as well, comparable to the moral status and rights of human beings. The following three main approaches provide a brief overview of the discussion.\n\ni. The Autonomy Approach\n\nKant and his followers place great emphasis on the notion of autonomy in the context of moral status and rights. A moral person is defined as a rational and autonomous being. Against this background, it has been suggested that one might be able to ascribe personhood to artificial intelligent machines once they have reached a certain level of autonomy in making moral decisions. Current machines are becoming increasingly autonomous, so it seems only a matter of time until they meet this moral threshold. A Kantian line of argument in support of granting moral status to machines based on autonomy could be framed as follows:\n\n- Rational agents have the capability to decide whether to act (or not act) in accordance with the demands of morality.\n\n- The ability to make decisions and to determine what is good has absolute value."}
{"chunk_id": "iep_utm_edu__b4724b6f15ae5245::c0028", "stable_id": "iep_utm_edu__b4724b6f15ae5245", "url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "final_url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "retrieved_at": "2026-01-22T00:14:58.390095Z", "title": "Ethics of Artificial Intelligence | Internet Encyclopedia of Philosophy", "section": "Rules, Risks, and Ethics of AI", "source_type": "Research / Academic", "content_hash": "9f8ed6c1d97f4e21d96dafbd4fa1143bda57020eae4b1780c3debc92d1ab75a2", "chunk_index": 28, "char_start": 38220, "char_end": 39463, "text": "- The ability to make such decisions gives rational persons absolute value.\n\n- A rational agent can act autonomously, including acting with respect to moral principles.\n\n- Rational agents have dignity insofar as they act autonomously.\n\n- Acting autonomously makes persons morally responsible.\n\n- Such a being—that is, a rational agent—has moral personhood.\n\nIt might be objected that machines—no matter how autonomous and rational—are not human beings and therefore should not be entitled to a moral status and the accompanying rights under a Kantian line of reasoning. But this objection is misleading, since Kant himself clearly states in his Groundwork (2009) that human beings should be considered as moral agents not because they are human beings, but because they are autonomous agents (Altman 2011; Timmermann 2020: 94). Kant has been criticised by his opponents for his logocentrism, even though this very claim has helped him avoid the more severe objection of speciesism—of holding that a particular species is morally superior simply because of the empirical features of the species itself (in the case of human beings, the particular DNA). This has been widely viewed as the equivalent of racism at the species level (Singer 2009)."}
{"chunk_id": "iep_utm_edu__b4724b6f15ae5245::c0029", "stable_id": "iep_utm_edu__b4724b6f15ae5245", "url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "final_url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "retrieved_at": "2026-01-22T00:14:58.390095Z", "title": "Ethics of Artificial Intelligence | Internet Encyclopedia of Philosophy", "section": "Rules, Risks, and Ethics of AI", "source_type": "Research / Academic", "content_hash": "9f8ed6c1d97f4e21d96dafbd4fa1143bda57020eae4b1780c3debc92d1ab75a2", "chunk_index": 29, "char_start": 39463, "char_end": 40801, "text": "ii. The Indirect Duties Approach\n\nThe indirect duties approach is based on Kant’s analysis of our behaviour towards animals. In general, Kant argues in his Lectures on Ethics (1980: 239–41) that even though human beings do not have direct duties towards animals (because they are not persons), they still have indirect duties towards them. The underlying reason is that human beings may start to treat their fellow humans badly if they develop bad habits by mistreating and abusing animals as they see fit. In other words, abusing animals may have a detrimental, brutalising impact on human character.\n\nKate Darling (2016) has applied the Kantian line of reasoning to show that even current social robots should be entitled to moral and legal protection. She argues that one should protect lifelike beings such as robots that interact with human beings when society cares deeply enough about them, even though they do not have a right to life. Darling offers two arguments why one should treat social robots in this way. Her first argument concerns people who witness cases of abuse and mistreatment of robots, pointing out that they might become ‘traumatized’ and ‘desensitized’. Second, she contends that abusing robots may have a detrimental impact on the abuser’s character, causing her to start treating fellow humans poorly as well."}
{"chunk_id": "iep_utm_edu__b4724b6f15ae5245::c0030", "stable_id": "iep_utm_edu__b4724b6f15ae5245", "url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "final_url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "retrieved_at": "2026-01-22T00:14:58.390095Z", "title": "Ethics of Artificial Intelligence | Internet Encyclopedia of Philosophy", "section": "Rules, Risks, and Ethics of AI", "source_type": "Research / Academic", "content_hash": "9f8ed6c1d97f4e21d96dafbd4fa1143bda57020eae4b1780c3debc92d1ab75a2", "chunk_index": 30, "char_start": 40801, "char_end": 42138, "text": "Indeed, current social robots may be best protected by the indirect duties approach, but the idea that exactly the same arguments should also be applied to future robots of greater sophistication that either match or supersede human capabilities is somewhat troublesome. Usually, one would expect that these future robots—unlike Darling’s social robots of today—will be not only moral patients but rather proper moral agents. In addition, the view that one should protect lifelike beings ‘when society cares deeply enough’ (2016: 230) about them opens the door to social exclusion based purely on people’s unwillingness to accept them as members of the moral community. Morally speaking, this is not acceptable. The next approach attempts to deal with this situation.\n\niii. The Relational Approach\n\nMark Coeckelbergh (2014) and David Gunkel (2012), the pioneers of the relational approach to moral status, believe that robots have a moral status based on their social relation with human beings. In other words, moral status or personhood emerges through social relations between different entities, such as human beings and robots, instead of depending on criteria inherent in the being such as sentience and consciousness. The general idea behind this approach comes to the fore in the following key passage (Coeckelbergh 2014: 69–70):"}
{"chunk_id": "iep_utm_edu__b4724b6f15ae5245::c0031", "stable_id": "iep_utm_edu__b4724b6f15ae5245", "url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "final_url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "retrieved_at": "2026-01-22T00:14:58.390095Z", "title": "Ethics of Artificial Intelligence | Internet Encyclopedia of Philosophy", "section": "Rules, Risks, and Ethics of AI", "source_type": "Research / Academic", "content_hash": "9f8ed6c1d97f4e21d96dafbd4fa1143bda57020eae4b1780c3debc92d1ab75a2", "chunk_index": 31, "char_start": 42138, "char_end": 43460, "text": "We may wonder if robots will remain “machines” or if they can become companions. Will people start saying, as they tend to say of people who have “met their dog” … , that someone has “met her robot”? Would such a person, having that kind of relation with that robot, still feel shame at all in front of the robot? And is there, at that point of personal engagement, still a need to talk about the “moral standing” of the robot? Is not moral quality already implied in the very relation that has emerged here? For example, if an elderly person is already very attached to her Paro robot and regards it as a pet or baby, then what needs to be discussed is that relation, rather than the “moral standing” of the robot.\n\nThe personal experience with the Other, that is, the robot, is the key component of this relational and phenomenological approach. The relational concept of personhood can be fleshed out in the following way:\n\n- A social model of autonomy, under which autonomy is not defined individually but stands in the context of social relations;\n\n- Personhood is absolute and inherent in every entity as a social being; it does not come in degrees;\n\n- An interactionist model of personhood, according to which personhood is relational by nature (but not necessarily reciprocal) and defined in non-cognitivist terms."}
{"chunk_id": "iep_utm_edu__b4724b6f15ae5245::c0032", "stable_id": "iep_utm_edu__b4724b6f15ae5245", "url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "final_url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "retrieved_at": "2026-01-22T00:14:58.390095Z", "title": "Ethics of Artificial Intelligence | Internet Encyclopedia of Philosophy", "section": "Rules, Risks, and Ethics of AI", "source_type": "Research / Academic", "content_hash": "9f8ed6c1d97f4e21d96dafbd4fa1143bda57020eae4b1780c3debc92d1ab75a2", "chunk_index": 32, "char_start": 43460, "char_end": 44995, "text": "The above claims are not intended as steps in a conclusive argument; rather, they portray the general line of reasoning regarding the moral importance of social relations. The relational approach does not require the robot to be rational, intelligent or autonomous as an individual entity; instead, the social encounter with the robot is morally decisive. The moral standing of the robot is based on exactly this social encounter.\n\nThe problem with the relational approach is that the moral status of robots is thus based completely on human beings’ willingness to enter into social relations with a robot. In other words, if human beings (for whatever reasons) do not want to enter into such relations, they could deny robots a moral status to which the robots might be entitled on more objective criteria such as rationality and sentience. Thus, the relational approach does not actually provide a strong foundation for robot rights; rather, it supports a pragmatic perspective that would make it easier to welcome robots (who already have moral status) in the moral community (Gordon 2020c).\n\niv. The Upshot\n\nThe three approaches discussed in sections 2.f.i-iii. all attempt to show how one can make sense of the idea of ascribing moral status and rights to robots. The most important observation is, however, that robots are entitled to moral status and rights independently of our opinion, once they have fulfilled the relevant criteria. Whether human beings will actually recognise their status and rights are a different matter."}
{"chunk_id": "iep_utm_edu__b4724b6f15ae5245::c0033", "stable_id": "iep_utm_edu__b4724b6f15ae5245", "url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "final_url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "retrieved_at": "2026-01-22T00:14:58.390095Z", "title": "Ethics of Artificial Intelligence | Internet Encyclopedia of Philosophy", "section": "Rules, Risks, and Ethics of AI", "source_type": "Research / Academic", "content_hash": "9f8ed6c1d97f4e21d96dafbd4fa1143bda57020eae4b1780c3debc92d1ab75a2", "chunk_index": 33, "char_start": 44995, "char_end": 46543, "text": "g. Singularity and Value Alignment\n\nSome of the theories of the potential moral status of artificial intelligent agents discussed in section 2.f. have struck some authors as belonging to science fiction. The same can be said about the next topic to be considered: singularity. The underlying argument regarding technological singularity was introduced by statistician I. J. Good in ‘Speculations Concerning the First Ultraintelligent Machine’ (1965):\n\nLet an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever. Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an “intelligence explosion”, and the intelligence of man would be left far behind. Thus, the first ultraintelligent machine is the last invention that man need ever make.\n\nThe idea of an intelligence explosion involving self-replicating, super-intelligent AI machines seems inconceivable to many; some commentators dismiss such claims as a myth about the future development of AI (for example, Floridi 2016). However, prominent voices both inside and outside academia are taking this idea very seriously—in fact, so seriously that they fear the possible consequence of the so-called ‘existential risks’ such as the risk of human extinction. Among those voicing such fears are philosophers like Nick Bostrom and Toby Ord, but also prominent figures like Elon Musk and the late Stephen Hawking."}
{"chunk_id": "iep_utm_edu__b4724b6f15ae5245::c0034", "stable_id": "iep_utm_edu__b4724b6f15ae5245", "url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "final_url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "retrieved_at": "2026-01-22T00:14:58.390095Z", "title": "Ethics of Artificial Intelligence | Internet Encyclopedia of Philosophy", "section": "Rules, Risks, and Ethics of AI", "source_type": "Research / Academic", "content_hash": "9f8ed6c1d97f4e21d96dafbd4fa1143bda57020eae4b1780c3debc92d1ab75a2", "chunk_index": 34, "char_start": 46543, "char_end": 47492, "text": "Authors discussing the idea of technological singularity differ in their views about what might lead to it. The famous futurist Ray Kurzweil is well-known for advocating the idea of singularity with exponentially increasing computing power, associated with ‘Moore’s law’, which points out that the computing power of transistors, at the time of writing, had been doubling every two years since the 1970s and could reasonably be expected to continue to do so in future (Kurzweil 2005). This approach sees the path to superintelligence as likely to proceed through a continuing improvement of the hardware Another take on what might lead to superintelligence—favoured by the well-known AI researcher Stuart Russell—focuses instead on algorithms. From Russell’s (2019) point of view, what is needed for singularity to occur are conceptual breakthroughs in such areas as the studies of language and common-sense processing as well as learning processes."}
{"chunk_id": "iep_utm_edu__b4724b6f15ae5245::c0035", "stable_id": "iep_utm_edu__b4724b6f15ae5245", "url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "final_url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "retrieved_at": "2026-01-22T00:14:58.390095Z", "title": "Ethics of Artificial Intelligence | Internet Encyclopedia of Philosophy", "section": "Rules, Risks, and Ethics of AI", "source_type": "Research / Academic", "content_hash": "9f8ed6c1d97f4e21d96dafbd4fa1143bda57020eae4b1780c3debc92d1ab75a2", "chunk_index": 35, "char_start": 47492, "char_end": 48745, "text": "Researchers concerned with singularity approach the issue of what to do to guard humanity against such existential risks in several different ways, depending in part on what they think these existential risks depend on. Bostrom, for example, understands superintelligence as consisting of a maximally powerful capacity to achieve whatever aims might be associated with artificial intelligent systems. In his much-discussed example (Bostrom 2014), a super-intelligent machine threatens the future of human life by becoming optimally efficient at maximising the number of paper clips in the world, a goal whose achievement might be facilitated by removing human beings so as to make more space for paper clips. From this point of view, it is crucial to equip super-intelligent AI machines with the right goals, so that when they pursue these goals in maximally efficient ways, there is no risk that they will extinguish the human race along the way. This is one way to think about how to create a beneficial super-intelligence.\n\nRussell (2019) presents an alternative picture, formulating three rules for AI design, which might perhaps be viewed as an updated version of or suggested replacement for Asimov’s fictional laws of robotics (see section 2.a.):"}
{"chunk_id": "iep_utm_edu__b4724b6f15ae5245::c0036", "stable_id": "iep_utm_edu__b4724b6f15ae5245", "url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "final_url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "retrieved_at": "2026-01-22T00:14:58.390095Z", "title": "Ethics of Artificial Intelligence | Internet Encyclopedia of Philosophy", "section": "Rules, Risks, and Ethics of AI", "source_type": "Research / Academic", "content_hash": "9f8ed6c1d97f4e21d96dafbd4fa1143bda57020eae4b1780c3debc92d1ab75a2", "chunk_index": 36, "char_start": 48745, "char_end": 49981, "text": "- The machine’s only objective is to maximise the realisation of human preferences.\n\n- The machine is initially uncertain about what those preferences are.\n\n- The ultimate source of information about human preferences is human behaviour.\n\nThe theories discussed in this section represent different ideas about what is sometimes called ‘value alignment’—that is, the concept that the goals and functioning of AI systems, especially super-intelligent future AI systems, should be properly aligned with human values. AI should be tracking human interests and values, and its functioning should benefit us and not lead to any existential risks, according to the ideal of value alignment. As noted in the beginning of this section, to some commentators, the idea that AI could become super-intelligent and pose existential threats is simply a myth that needs to be busted. But according to others, thinkers such as Toby Ord, AI is among the main reasons why humanity is in a critical period where its very future is at stake. According to such assessments, AI should be treated on a par with nuclear weapons and other potentially highly destructive technologies that put us all at great risk unless proper value alignment happens (Ord 2020)."}
{"chunk_id": "iep_utm_edu__b4724b6f15ae5245::c0037", "stable_id": "iep_utm_edu__b4724b6f15ae5245", "url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "final_url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "retrieved_at": "2026-01-22T00:14:58.390095Z", "title": "Ethics of Artificial Intelligence | Internet Encyclopedia of Philosophy", "section": "Rules, Risks, and Ethics of AI", "source_type": "Research / Academic", "content_hash": "9f8ed6c1d97f4e21d96dafbd4fa1143bda57020eae4b1780c3debc92d1ab75a2", "chunk_index": 37, "char_start": 49981, "char_end": 51652, "text": "A key problem concerning value alignment—especially if understood along the lines of Russell’s three principles—is whose values or preferences AI should be aligned with. As Iason Gabriel (2020) notes, reasonable people may disagree on what values and interests are the right ones with which to align the functioning of AI (whether super-intelligent or not). Gabriel’s suggestion for solving this problem is inspired by John Rawls’ (1999, 2001) work on ‘reasonable pluralism’. Rawls proposes that society should seek to identify ‘fair principles’ that could generate an overlapping consensus or widespread agreement despite the existence of more specific, reasonable disagreements about values among members of society. But how likely is it that this kind of convergence in general principles would find widespread support? (See section 3.)\n\nh. Other Debates\n\nIn addition to the topics highlighted above, other issues that have not received as much attention are beginning to be discussed within AI ethics. Five such issues are discussed briefly below.\n\ni. AI as a form of Moral Enhancement or a Moral Advisor\n\nAI systems tend to be used as ‘recommender systems’ in online shopping, online entertainment (for example, music and movie streaming), and other realms. Some ethicists have discussed the advantages and disadvantages of AI systems whose recommendations could help us to make better choices and ones more consistent with our basic values. Perhaps AI systems could even, at some point, help us improve our values. Works on these and related questions include Borenstein and Arkin (2016), Giubilini et al. (2015, 2018), Klincewicz (2016), and O’Neill et al. (2021)."}
{"chunk_id": "iep_utm_edu__b4724b6f15ae5245::c0038", "stable_id": "iep_utm_edu__b4724b6f15ae5245", "url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "final_url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "retrieved_at": "2026-01-22T00:14:58.390095Z", "title": "Ethics of Artificial Intelligence | Internet Encyclopedia of Philosophy", "section": "Rules, Risks, and Ethics of AI", "source_type": "Research / Academic", "content_hash": "9f8ed6c1d97f4e21d96dafbd4fa1143bda57020eae4b1780c3debc92d1ab75a2", "chunk_index": 38, "char_start": 51652, "char_end": 52966, "text": "ii. AI and the Future of Work\n\nMuch discussion about AI and the future of work concerns the vital issue of whether AI and other forms of automation will cause widespread ‘technological unemployment’ by eliminating large numbers of human jobs that would be taken over by automated machines (Danaher 2019a). This is often presented as a negative prospect, where the question is how and whether a world without work would offer people any prospects for fulfilling and meaningful activities, since certain goods achieved through work (other than income) are hard to achieve in other contexts (Gheaus and Herzog 2016). However, some authors have argued that work in the modern world exposes many people to various kinds of harm (Anderson 2017). Danaher (2019a) examines the important question of whether a world with less work might actually be preferable. Some argue that existential boredom would proliferate if human beings can no longer find a meaningful purpose in their work (or even their life) because machines have replaced them (Bloch 1954). In contrast, Jonas (1984) criticises Bloch, arguing that boredom will not be a substantial issue at all. Another related issue—perhaps more relevant in the short and medium-term—is how we can make increasingly technologised work remain meaningful (Smids et al. 2020)."}
{"chunk_id": "iep_utm_edu__b4724b6f15ae5245::c0039", "stable_id": "iep_utm_edu__b4724b6f15ae5245", "url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "final_url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "retrieved_at": "2026-01-22T00:14:58.390095Z", "title": "Ethics of Artificial Intelligence | Internet Encyclopedia of Philosophy", "section": "Rules, Risks, and Ethics of AI", "source_type": "Research / Academic", "content_hash": "9f8ed6c1d97f4e21d96dafbd4fa1143bda57020eae4b1780c3debc92d1ab75a2", "chunk_index": 39, "char_start": 52966, "char_end": 54062, "text": "iii. AI and the Future of Personal Relationships\n\nVarious AI-driven technologies affect the nature of friendships, romances and other interpersonal relationships and could impact them even more in future. Online ‘friendships’ arranged through social media have been investigated by philosophers who disagree as to whether relationships that are partly curated by AI algorithms, could be true friendships (Cocking et al. 2012; McFall 2012; Kaliarnta 2016; Elder 2017). Some philosophers have sharply criticised AI-driven dating apps, which they think might reinforce negative stereotypes and negative gender expectations (Frank and Klincewicz 2018). In more science-fiction-like philosophising, which might nevertheless become increasingly present in real life, there has also been discussion about whether human beings could have true friendships or romantic relationships with robots and other artificial agents equipped with advanced AI (Levy 2008; Sullins 2012; Elder 2017; Hauskeller 2017; Nyholm and Frank 2017; Danaher 2019c; Nyholm 2020).\n\niv. AI and the Concern About Human ‘Enfeeblement’"}
{"chunk_id": "iep_utm_edu__b4724b6f15ae5245::c0040", "stable_id": "iep_utm_edu__b4724b6f15ae5245", "url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "final_url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "retrieved_at": "2026-01-22T00:14:58.390095Z", "title": "Ethics of Artificial Intelligence | Internet Encyclopedia of Philosophy", "section": "Rules, Risks, and Ethics of AI", "source_type": "Research / Academic", "content_hash": "9f8ed6c1d97f4e21d96dafbd4fa1143bda57020eae4b1780c3debc92d1ab75a2", "chunk_index": 40, "char_start": 54062, "char_end": 54935, "text": "If more and more aspects of our lives are driven by the recommendations of AI systems (since we do not understand its functioning and we might question the propriety of its functioning), the results could include ‘a crisis in moral agency’ (Danaher 2019d), human ‘enfeeblement’ (Russell 2019), or ‘de-skilling’ in different areas of human life (Vallor 2015, 2016). This scenario becomes even more likely should technological singularity be attained, because at that point all work, including all research and engineering, could be done by intelligent machines. After some generations, human beings might indeed be completely dependent on machines in all areas of life and unable to turn the clock back. This situation is very dangerous; hence it is of utmost importance that human beings remain skilful and knowledgeable while developing AI capacities.\n\nv. Anthropomorphism"}
{"chunk_id": "iep_utm_edu__b4724b6f15ae5245::c0041", "stable_id": "iep_utm_edu__b4724b6f15ae5245", "url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "final_url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "retrieved_at": "2026-01-22T00:14:58.390095Z", "title": "Ethics of Artificial Intelligence | Internet Encyclopedia of Philosophy", "section": "Rules, Risks, and Ethics of AI", "source_type": "Research / Academic", "content_hash": "9f8ed6c1d97f4e21d96dafbd4fa1143bda57020eae4b1780c3debc92d1ab75a2", "chunk_index": 41, "char_start": 54935, "char_end": 56346, "text": "The very idea of artificial intelligent machines that imitate human thinking and behaviour might incorporate, according to some, a form of anthropomorphising that ought to be avoided. In other words, attributing humanlike qualities to machines that are not human might pose a problem. A common worry about many forms of AI technologies (or about how they are presented to the general public) is that they are deceptive (for example, Boden et al. 2017). Many have objected that companies tend to exaggerate the extent to which their products are based on AI technology. For example, several prominent AI researchers and ethicists have criticised the makers of Sophia the robot for falsely presenting her as much more humanlike than she really is (for example, Sharkey 2018; Bryson 2010, 2019), and as being designed to prompt anthropomorphising responses in human beings that are somehow problematic or unfitting. The related question of whether anthropomorphising responses to AI technologies are always problematic requires further consideration, which it is increasingly receiving (for example, Coeckelbergh 2010; Darling 2016, 2017; Gunkel 2018; Danaher 2020; Nyholm 2020; Smids 2020).\n\nThis list of emerging topics within AI ethics is not exhaustive, as the field is very fertile, with new issues arising constantly. This is perhaps the fastest-growing field within the study of ethics and moral philosophy."}
{"chunk_id": "iep_utm_edu__b4724b6f15ae5245::c0042", "stable_id": "iep_utm_edu__b4724b6f15ae5245", "url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "final_url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "retrieved_at": "2026-01-22T00:14:58.390095Z", "title": "Ethics of Artificial Intelligence | Internet Encyclopedia of Philosophy", "section": "Rules, Risks, and Ethics of AI", "source_type": "Research / Academic", "content_hash": "9f8ed6c1d97f4e21d96dafbd4fa1143bda57020eae4b1780c3debc92d1ab75a2", "chunk_index": 42, "char_start": 56346, "char_end": 57608, "text": "3. Ethical Guidelines for AI\n\nAs a result of widespread awareness of and interest in the ethical issues related to AI, several influential institutions (including governments, the European Union, large companies and other associations) have already tasked expert panels with drafting policy documents and ethical guidelines for AI. Such documents have proliferated to the point at which it is very difficult to keep track of all the latest AI ethical guidelines being released. Additionally, AI ethics is receiving substantial funding from various public and private sources, and multiple research centres for AI ethics have been established. These developments have mostly received positive responses, but there have also been some worries about the so-called ‘ethics washing’—that is, giving an ethical stamp of approval to something that might be, from a more critical point of view, ethically problematic (compare Tigard 2020b)—along with concerns that some efforts may be relatively toothless or too centred on the West, ignoring non-Western perspectives on AI ethics. This section, before discussing such criticisms, reviews examples of already published ethical guidelines and considers whether any consensus can emerge between these differing guidelines."}
{"chunk_id": "iep_utm_edu__b4724b6f15ae5245::c0043", "stable_id": "iep_utm_edu__b4724b6f15ae5245", "url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "final_url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "retrieved_at": "2026-01-22T00:14:58.390095Z", "title": "Ethics of Artificial Intelligence | Internet Encyclopedia of Philosophy", "section": "Rules, Risks, and Ethics of AI", "source_type": "Research / Academic", "content_hash": "9f8ed6c1d97f4e21d96dafbd4fa1143bda57020eae4b1780c3debc92d1ab75a2", "chunk_index": 43, "char_start": 57608, "char_end": 58840, "text": "An excellent resource in this context is the overview by Jobin et al. (2019), who conducted a substantial comparative review of 84 sets of ethical guidelines issued by national or international organisations from various countries. Jobin et al. found strong convergence around five key principles—transparency, justice and fairness, non-maleficence, responsibility, and privacy, among many. Their findings are reported here to illustrate the extent of this convergence on some (but not all) of the principles discussed in the original paper. The number on the left indicates the number of ethical guideline documents, among the 84 examined, in which a particular principle was prominently featured. The codes Jobin et al. used are included so that readers can see the basis for their classification.\n\n| Ethical principle | Number of documents (N = 84) | Codes included |\n\n| Transparency | 73 | Transparency, explainability, explicability, understandability, interpretability, communication, disclosure |\n\n| Justice and fairness | 68 | Justice, fairness, consistency, inclusion, equality, equity, (non-)bias, (non-)discrimination, diversity, plurality, accessibility, reversibility, remedy, redress, challenge, access, distribution |"}
{"chunk_id": "iep_utm_edu__b4724b6f15ae5245::c0044", "stable_id": "iep_utm_edu__b4724b6f15ae5245", "url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "final_url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "retrieved_at": "2026-01-22T00:14:58.390095Z", "title": "Ethics of Artificial Intelligence | Internet Encyclopedia of Philosophy", "section": "Rules, Risks, and Ethics of AI", "source_type": "Research / Academic", "content_hash": "9f8ed6c1d97f4e21d96dafbd4fa1143bda57020eae4b1780c3debc92d1ab75a2", "chunk_index": 44, "char_start": 58840, "char_end": 60190, "text": "| Non-maleficence | 60 | Non-maleficence, security, safety, harm, protection, precaution, integrity (bodily or mental), non-subversion |\n\n| Responsibility | 60 | Responsibility, accountability, liability, acting with integrity |\n\n| Privacy | 47 | Privacy, personal or private information |\n\n| Beneficence | 41 | Benefits, beneficence, well-being, peace, social good, common good |\n\n| Freedom and autonomy | 34 | Freedom, autonomy, consent, choice, self-determination, liberty, empowerment |\n\n| Trust | 28 | Trust |\n\n| Sustainability | 14 | Sustainability, environment (nature), energy, resources (energy) |\n\n| Dignity | 13 | Dignity |\n\n| Solidarity | 6 | Solidarity, social security, cohesion |\n\nThe review conducted by Jobin et al. (2019) reveals, at least with respect to the first five principles on the list, a significant degree of overlap in these attempts to create ethical guidelines for AI (see Gabriel 2020). On the other hand, the last six items on the list (beginning with beneficence) appeared as key principles in fewer than half of the documents studied. Relatedly, researchers working on the ‘moral machine’ research project, which examined people’s attitudes as to what self-driving cars should be programmed to do in various crash dilemma scenarios, also found great variation, including cross-cultural variation (Awad et al. 2018)."}
{"chunk_id": "iep_utm_edu__b4724b6f15ae5245::c0045", "stable_id": "iep_utm_edu__b4724b6f15ae5245", "url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "final_url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "retrieved_at": "2026-01-22T00:14:58.390095Z", "title": "Ethics of Artificial Intelligence | Internet Encyclopedia of Philosophy", "section": "Rules, Risks, and Ethics of AI", "source_type": "Research / Academic", "content_hash": "9f8ed6c1d97f4e21d96dafbd4fa1143bda57020eae4b1780c3debc92d1ab75a2", "chunk_index": 45, "char_start": 60190, "char_end": 61892, "text": "These ethical guidelines have received a fair amount of criticism—both in terms of their content and with respect to how they were created (for example, Metzinger 2019). For Metzinger, the very idea of ‘trustworthy AI’ is ‘nonsense’ since only human beings and not machines can be, or fail to be, trustworthy. Furthermore, the EU high-level expert group on AI had very few experts from the field of ethics but numerous industry representatives, who had an interest in toning down any ethical worries about the AI industry. In addition, the EU document ‘Ethical Guidelines for Trustworthy AI’ uses vague and non-confrontational language. It is, to use the term favoured by Resseguier and Rodrigues (2020), a mostly ‘toothless’ document. The EU ethical guidelines that industry representatives have supposedly made toothless illustrate the concerns raised about the possible ‘ethics washing’.\n\nAnother point of criticism regarding these kinds of ethical guidelines is that many of the expert panels drafting them are non-inclusive and fail to take non-Western (for example, African and Asian) perspectives on AI and ethics into account. Therefore, it would be important for future versions of such guidelines—or new ethical guidelines—to include non-Western contributions. Notably, in academic journals that focus on the ethics of technology, there has been modest progress towards publishing more non-Western perspectives on AI ethics—for example, applying Dao (Wong 2012), Confucian virtue-ethics perspectives (Jing and Doorn 2020), and southern African relational and communitarian ethics perspectives including the ‘ubuntu’ philosophy of personhood and interpersonal relationships (see Wareham 2020)."}
{"chunk_id": "iep_utm_edu__b4724b6f15ae5245::c0046", "stable_id": "iep_utm_edu__b4724b6f15ae5245", "url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "final_url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "retrieved_at": "2026-01-22T00:14:58.390095Z", "title": "Ethics of Artificial Intelligence | Internet Encyclopedia of Philosophy", "section": "Rules, Risks, and Ethics of AI", "source_type": "Research / Academic", "content_hash": "9f8ed6c1d97f4e21d96dafbd4fa1143bda57020eae4b1780c3debc92d1ab75a2", "chunk_index": 46, "char_start": 61892, "char_end": 63106, "text": "4. Conclusion\n\nThe ethics of AI has become one of the liveliest topics in philosophy of technology. AI has the potential to redefine our traditional moral concepts, ethical approaches and moral theories. The advent of artificial intelligent machines that may either match or supersede human capabilities poses a big challenge to humanity’s traditional self-understanding as the only beings with the highest moral status in the world. Accordingly, the future of AI ethics is unpredictable but likely to offer considerable excitement and surprise.\n\n5. References and Further Reading\n\n- Agar, N. (2020). How to Treat Machines That Might Have Minds. Philosophy & Technology, 33(2): 269–82.\n\n- Altman, M. C. (2011). Kant and Applied Ethics: The Uses and Limits of Kant’s Practical Philosophy. Malden, NJ: Wiley-Blackwell.\n\n- Anderson, E. (2017). Private Government: How Employers Rule Our Lives (and Why We Don’t Talk about It). Princeton, NJ: Princeton University Press.\n\n- Anderson, M., and Anderson, S. (2011). Machine Ethics. Cambridge: Cambridge University Press.\n\n- Anderson, S. L. (2011). Machine Metaethics. In M. Anderson and S. L. Anderson (Eds.), Machine Ethics, 21–27. Cambridge: Cambridge University Press."}
{"chunk_id": "iep_utm_edu__b4724b6f15ae5245::c0047", "stable_id": "iep_utm_edu__b4724b6f15ae5245", "url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "final_url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "retrieved_at": "2026-01-22T00:14:58.390095Z", "title": "Ethics of Artificial Intelligence | Internet Encyclopedia of Philosophy", "section": "Rules, Risks, and Ethics of AI", "source_type": "Research / Academic", "content_hash": "9f8ed6c1d97f4e21d96dafbd4fa1143bda57020eae4b1780c3debc92d1ab75a2", "chunk_index": 47, "char_start": 63106, "char_end": 64320, "text": "- Angwin, J., Larson, J., Mattu, S., and Kirchner, L. (2016). Machine Bias. In ProPublica, May 23. https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing.\n\n- Arkin, R. (2009). Governing Lethal Behavior in Autonomous Robots. Boca Raton, FL: CRC Press.\n\n- Arkin, R. (2010). The Case for Ethical Autonomy in Unmanned Systems. Journal of Military Ethics, 9(4), 332–41.\n\n- Asimov, I. (1942). Runaround: A Short Story. New York: Street and Smith.\n\n- Asimov, I. (1986). Robots and Empire: The Classic Robot Novel. New York: HarperCollins.\n\n- Atapattu, S. (2015). Human Rights Approaches to Climate Change: Challenges and Opportunities. New York: Routledge.\n\n- Awad, E., Dsouza, S., Kim, R., Schulz, J., Henrich, J., Shariff, A., Bonnefon, J. F., and Rahwan, I. (2018). The Moral Machine Experiment. Nature, 563, 59–64.\n\n- Bloch, E. (1985/1954). Das Prinzip Hoffnung, 3 vols. Frankfurt am Main: Suhrkamp.\n\n- Boden, M., Bryson, J., Caldwell, D., Dautenhahn, K., Edwards, L., Kember, S., Newman, P., Parry, V., Pegman, G., Rodden, T., Sorell, T., Wallis, M., Whitby, B., and Winfield, A. (2017). Principles of Robotics: Regulating Robots in the Real World. Connection Science, 29(2), 124–29."}
{"chunk_id": "iep_utm_edu__b4724b6f15ae5245::c0048", "stable_id": "iep_utm_edu__b4724b6f15ae5245", "url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "final_url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "retrieved_at": "2026-01-22T00:14:58.390095Z", "title": "Ethics of Artificial Intelligence | Internet Encyclopedia of Philosophy", "section": "Rules, Risks, and Ethics of AI", "source_type": "Research / Academic", "content_hash": "9f8ed6c1d97f4e21d96dafbd4fa1143bda57020eae4b1780c3debc92d1ab75a2", "chunk_index": 48, "char_start": 64320, "char_end": 65570, "text": "- Borenstein, J. and Arkin, R. (2016). Robotic Nudges: The Ethics of Engineering a More Socially Just Human Being. Science and Engineering Ethics, 22, 31–46.\n\n- Bostrom, N. (2014). Superintelligence: Paths, Dangers, Strategies. Oxford: Oxford University Press.\n\n- Bryson, J. (2010). Robots Should Be Slaves. In Y. Wilks (Ed.), Close Engagements with Artificial Companions, 63–74. Amsterdam: John Benjamins.\n\n- Bryson, J. (2012). A Role for Consciousness in Action Selection. International Journal of Machine Consciousness, 4(2), 471–82.\n\n- Bryson, J. (2019). Patiency Is Not a Virtue: The Design of Intelligent Systems and Systems of Ethics. Ethics and Information Technology, 20(1), 15–26.\n\n- Buolamwini, J., and Gebru, T. (2018). Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification. In Proceedings of the 1st Conference on Fairness, Accountability, and Transparency. PMLR, 81, 77–91.\n\n- Čapek, K. (1920). Rossum’s Universal Robots. Adelaide: The University of Adelaide.\n\n- Cavalieri, P. (2001). The Animal Question: Why Non-Human Animals Deserve Human Rights. Oxford: Oxford University Press.\n\n- Chalmers, D. (1996). The Conscious Mind: In Search of a Fundamental Theory. New York/Oxford: Oxford University Press."}
{"chunk_id": "iep_utm_edu__b4724b6f15ae5245::c0049", "stable_id": "iep_utm_edu__b4724b6f15ae5245", "url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "final_url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "retrieved_at": "2026-01-22T00:14:58.390095Z", "title": "Ethics of Artificial Intelligence | Internet Encyclopedia of Philosophy", "section": "Rules, Risks, and Ethics of AI", "source_type": "Research / Academic", "content_hash": "9f8ed6c1d97f4e21d96dafbd4fa1143bda57020eae4b1780c3debc92d1ab75a2", "chunk_index": 49, "char_start": 65570, "char_end": 66919, "text": "- Chalmers, D. (2010). The Singularity: A Philosophical Analysis. Journal of Consciousness Studies, 17, 7–65.\n\n- Cocking, D., Van Den Hoven, J., and Timmermans, J. (2012). Introduction: One Thousand Friends. Ethics and Information Technology, 14, 179–84.\n\n- Coeckelbergh, M. (2010). Robot Rights? Towards a Social-Relational Justification of Moral Consideration. Ethics and Information Technology, 12(3), 209–21.\n\n- Coeckelbergh, M. (2014). The Moral Standing of Machines: Towards a Relational and Non- Cartesian Moral Hermeneutics. Philosophy & Technology, 27(1), 61–77.\n\n- Coeckelbergh, M. (2020). AI Ethics. Cambridge, MA and London: MIT Press.\n\n- Copeland, B. J. (2020). Artificial Intelligence. Britannica.com. Retrieved from https://www.britannica.com/technology/artificial-intelligence.\n\n- Danaher, J. (2016a). Robots, Law, and the Retribution Gap. Ethics and Information Technology, 18(4), 299–309.\n\n- Danaher, J. (2016b). The Threat of Algocracy: Reality, Resistance and Accommodation. Philosophy & Technology, 29(3), 245–68.\n\n- Danaher, J. (2019a). Automation and Utopia. Cambridge, MA: Harvard University Press.\n\n- Danaher, J. (2019b). Escaping Skinner’s Box: AI and the New Era of Techno-Superstition. Philosophical Disquisitions blog: https://philosophicaldisquisitions.blogspot.com/2019/10/escaping-skinners-box-ai-and-new-era-of.html."}
{"chunk_id": "iep_utm_edu__b4724b6f15ae5245::c0050", "stable_id": "iep_utm_edu__b4724b6f15ae5245", "url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "final_url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "retrieved_at": "2026-01-22T00:14:58.390095Z", "title": "Ethics of Artificial Intelligence | Internet Encyclopedia of Philosophy", "section": "Rules, Risks, and Ethics of AI", "source_type": "Research / Academic", "content_hash": "9f8ed6c1d97f4e21d96dafbd4fa1143bda57020eae4b1780c3debc92d1ab75a2", "chunk_index": 50, "char_start": 66919, "char_end": 68161, "text": "- Danaher, J. (2019c). The Philosophical Case for Robot Friendship. Journal of Posthuman Studies, 3(1), 5–24.\n\n- Danaher, J. (2019d). The Rise of the Robots and the Crises of Moral Patiency. AI & Society, 34(1), 129–36.\n\n- Danaher, J. (2020). Welcoming Robots into the Moral Circle? A Defence of Ethical Behaviourism. Science and Engineering Ethics, 26(4), 2023–49.\n\n- Danaher, J., and Robbins, S. (2020). Should AI Be Explainable? Episode 77 of the Philosophical Disquisitions Podcast: https://philosophicaldisquisitions.blogspot.com/2020/07/77-should-ai-be-explainable.html.\n\n- Daniels, J., Nkonde, M. and Mir, D. (2019). Advancing Racial Literacy in Tech. https://datasociety.net/output/advancing-racial-literacy-in-tech/.\n\n- Darling, K. (2016). Extending Legal Protection to Social Robots: The Effects of Anthro- pomorphism, Empathy, and Violent Behavior towards Robotic Objects. In R. Calo, A. M. Froomkin and I. Kerr (eds.), Robot Law, 213–34. Cheltenham: Edward Elgar.\n\n- Darling, K. (2017). “Who’s Johnny?” Anthropological Framing in Human-Robot Interaction, Integration, and Policy. In P. Lin, K. Abney and R. Jenkins (Eds.), Robot Ethics 2.0: From Autonomous Cars to Artificial Intelligence, 173–92. Oxford: Oxford University Press."}
{"chunk_id": "iep_utm_edu__b4724b6f15ae5245::c0051", "stable_id": "iep_utm_edu__b4724b6f15ae5245", "url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "final_url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "retrieved_at": "2026-01-22T00:14:58.390095Z", "title": "Ethics of Artificial Intelligence | Internet Encyclopedia of Philosophy", "section": "Rules, Risks, and Ethics of AI", "source_type": "Research / Academic", "content_hash": "9f8ed6c1d97f4e21d96dafbd4fa1143bda57020eae4b1780c3debc92d1ab75a2", "chunk_index": 51, "char_start": 68161, "char_end": 69614, "text": "- Dastin, J. (2018). Amazon Scraps Secret AI Recruiting Tool That Showed Bias Against Women. Reuters, October 10. https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G.\n\n- Dehghani, M., Forbus, K., Tomai, E., and Klenk, M. (2011). An Integrated Reasoning Approach to Moral Decision Making. In M. Anderson and S. L. Anderson (Eds.), Machine Ethics, 422–41. Cambridge: Cambridge University Press.\n\n- Dignum, V. (2019). Responsible Artificial Intelligence: How to Develop and Use AI in a Responsible Way. Berlin: Springer.\n\n- Dobbe, R., Dean, S., Gilbert, T., and Kohli, N. (2018). A Broader View on Bias in Automated Decision-Making: Reflecting on Epistemology and Dynamics. In 2018 Workshop on Fairness, Accountability and Transparency in Machine Learning during ICMI, Stockholm, Sweden (July 18 version). https://arxiv.org/abs/1807.00553.\n\n- Elamrani, A., and Yampolskiy, R. (2018). Reviewing Tests for Machine Consciousness. Journal of Consciousness Studies, 26(5–6), 35–64.\n\n- Elder, A. (2017). Friendship, Robots, and Social Media: False Friends and Second Selves. London: Routledge.\n\n- Floridi, L. (2016). Should We Be Afraid of AI? Machines Seem to Be Getting Smarter and Smarter and Much Better at Human Jobs, yet True AI Is Utterly Implausible. Why? Aeon, May 9. https://aeon.co/essays/true-ai-is-both-logically-possible-and-utterly-implausible."}
{"chunk_id": "iep_utm_edu__b4724b6f15ae5245::c0052", "stable_id": "iep_utm_edu__b4724b6f15ae5245", "url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "final_url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "retrieved_at": "2026-01-22T00:14:58.390095Z", "title": "Ethics of Artificial Intelligence | Internet Encyclopedia of Philosophy", "section": "Rules, Risks, and Ethics of AI", "source_type": "Research / Academic", "content_hash": "9f8ed6c1d97f4e21d96dafbd4fa1143bda57020eae4b1780c3debc92d1ab75a2", "chunk_index": 52, "char_start": 69614, "char_end": 70839, "text": "- Floridi, L., Cowls, J., Beltrametti, M., Chatila, R., Chazerand, P., Dignum, V., Vayena, E. (2018). AI4People—An Ethical Framework for a Good AI Society: Opportunities, Risks, Principles, and Recommendations. Minds and Machines, 28(4), 689–707. https://doi.org/10.1007/s11023-018-9482-5\n\n- Francione, G. L. (2009). Animals as Persons. Essay on the Abolition of Animal Exploitation. New York: Columbia University Press.\n\n- Frank, L., and Klincewicz, M. (2018): Swiping Left on the Quantified Relationship: Exploring the Potential Soft Impacts. American Journal of Bioethics, 18(2), 27–28.\n\n- Gabriel, I. (2020). Artificial Intelligence, Values, and Alignment. Minds and Machines, available online at https://link.springer.com/article/10.1007/s11023-020-09539-2.\n\n- Gheaus, A., and Herzog, L. (2016). Goods of Work (Other than Money!). Journal of Social Philosophy, 47(1), 70–89.\n\n- Giubilini, A., and Savulescu, J. (2018). The Artificial Moral Advisor: The “Ideal Observer” Meets Artificial Intelligence. Philosophy & Technology, 1–20. https://doi.org/10.1007/s13347-017-0285-z.\n\n- Gogoll, J., and Müller, J. F. (2017). Autonomous Cars: In Favor of a Mandatory Ethics Setting. Science and Engineering Ethics, 23(3), 681–700."}
{"chunk_id": "iep_utm_edu__b4724b6f15ae5245::c0053", "stable_id": "iep_utm_edu__b4724b6f15ae5245", "url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "final_url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "retrieved_at": "2026-01-22T00:14:58.390095Z", "title": "Ethics of Artificial Intelligence | Internet Encyclopedia of Philosophy", "section": "Rules, Risks, and Ethics of AI", "source_type": "Research / Academic", "content_hash": "9f8ed6c1d97f4e21d96dafbd4fa1143bda57020eae4b1780c3debc92d1ab75a2", "chunk_index": 53, "char_start": 70839, "char_end": 72080, "text": "- Good, I. J. (1965). Speculations Concerning the First Ultraintelligent Machine. In F. Alt and M. Rubinoff (Eds.), Advances in Computers, vol. 6, 31–88. Cambridge, MA: Academic Press.\n\n- Goodall, N. J. (2014). Ethical Decision Making during Automated Vehicle Crashes. Transportation Research Record: Journal of the Transportation Research Board, 2424, 58–65.\n\n- Goodall, N. J. (2016). Away from Trolley Problems and Toward Risk Management. Applied Artificial Intelligence, 30(8), 810–21.\n\n- Gordon, J.-S. (2020a). Building Moral Machines: Ethical Pitfalls and Challenges. Science and Engineering Ethics, 26, 141–57.\n\n- Gordon, J.-S. (2020b). What Do We Owe to Intelligent Robots? AI & Society, 35, 209–23.\n\n- Gordon, J.-S. (2020c). Artificial Moral and Legal Personhood. AI & Society, online first at https://link.springer.com/article/10.1007%2Fs00146-020-01063-2.\n\n- Guarini, M. (2006). Particularism and the Classification and Reclassification of Moral Cases. IEEE Intelligent Systems, 21(4), 22–28.\n\n- Gunkel, D. J., and Bryson, J. (2014). The Machine as Moral Agent and Patient. Philosophy & Technology, 27(1), 5–142.\n\n- Gunkel, D. (2012). The Machine Question. Critical Perspectives on AI, Robots, and Ethics. Cambridge, MA: MIT Press."}
{"chunk_id": "iep_utm_edu__b4724b6f15ae5245::c0054", "stable_id": "iep_utm_edu__b4724b6f15ae5245", "url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "final_url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "retrieved_at": "2026-01-22T00:14:58.390095Z", "title": "Ethics of Artificial Intelligence | Internet Encyclopedia of Philosophy", "section": "Rules, Risks, and Ethics of AI", "source_type": "Research / Academic", "content_hash": "9f8ed6c1d97f4e21d96dafbd4fa1143bda57020eae4b1780c3debc92d1ab75a2", "chunk_index": 54, "char_start": 72080, "char_end": 73284, "text": "- Gunkel, D. (2018). Robot Rights. Cambridge, MA: MIT Press.\n\n- Gurney, J. K. (2016). Crashing into the Unknown: An Examination of Crash-Optimization Algorithms through the Two Lanes of Ethics and Law. Alabama Law Review, 79(1), 183–267.\n\n- Himmelreich, J. (2018). Never Mind the Trolley: The Ethics of Autonomous Vehicles in Mundane Situations. Ethical Theory and Moral Practice, 21(3), 669–84.\n\n- Himma, K., and Tavani, H. (2008). The Handbook of Information and Computer Ethics. Hoboken, NJ: Wiley.\n\n- Jobin, A., Ienca, M., and Vayena, E. (2019). The Global Landscape of AI Ethics Guidelines. Nature Machine Intelligence, 1(9), 389–399.\n\n- Johnson, D. (1985/2009). Computer Ethics, 4th ed. New York: Pearson.\n\n- Johnson, D., and Nissenbaum, H. (1995). Computing, Ethics, and Social Values. Englewood Cliffs, NJ: Prentice Hall.\n\n- Jonas, H. (2003/1984). Das Prinzip Verantwortung. Frankfurt am Main: Suhrkamp.\n\n- Kaliarnta, S. (2016). Using Aristotle’s Theory of Friendship to Classify Online Friendships: A Critical Counterpoint. Ethics and Information Technology, 18(2), 65–79.\n\n- Kamm, F. (2007). Intricate ethics: Rights, responsibilities, and permissible harm. Oxford, UK: Oxford University Press."}
{"chunk_id": "iep_utm_edu__b4724b6f15ae5245::c0055", "stable_id": "iep_utm_edu__b4724b6f15ae5245", "url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "final_url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "retrieved_at": "2026-01-22T00:14:58.390095Z", "title": "Ethics of Artificial Intelligence | Internet Encyclopedia of Philosophy", "section": "Rules, Risks, and Ethics of AI", "source_type": "Research / Academic", "content_hash": "9f8ed6c1d97f4e21d96dafbd4fa1143bda57020eae4b1780c3debc92d1ab75a2", "chunk_index": 55, "char_start": 73284, "char_end": 74509, "text": "- Kamm, F. (2020). The Use and Abuse of the Trolley Problem: Self-Driving Cars, Medical Treatments, and the Distribution of Harm. In S. M. Liao (Ed.) The Ethics of Artificial Intelligence, 79–108. New York: Oxford University Press.\n\n- Kant, I. (1980). Lectures on Ethics, trans. Louis Infield, Indianapolis, IN: Hackett Publishing Company.\n\n- Kant, I. (2009). Groundwork of the Metaphysic of Morals. New York: Harper Perennial Modern Classics.\n\n- Keeling, G. (2020). The Ethics of Automated Vehicles. PhD Dissertation, University of Bristol. https://research-information.bris.ac.uk/files/243368588/Pure_Thesis.pdf.\n\n- Kissinger, H. A. (2018). How the Enlightenment Ends: Philosophically, Intellectually—in Every Way—Human Society Is Unprepared for the Rise of Artificial Intelligence. The Atlantic, June. https://www.theatlantic.com/magazine/archive/2018/06/henry-kissinger-ai-could-mean-the-end-of-human-history/559124/.\n\n- Klincewicz, M. (2016). Artificial Intelligence as a Means to Moral Enhancement. In Studies in Logic, Grammar and Rhetoric. https://doi.org/10.1515/slgr-2016-0061.\n\n- Klincewicz, M. (2015). Autonomous Weapons Systems, the Frame Problem and Computer Security. Journal of Military Ethics, 14(2), 162–76."}
{"chunk_id": "iep_utm_edu__b4724b6f15ae5245::c0056", "stable_id": "iep_utm_edu__b4724b6f15ae5245", "url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "final_url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "retrieved_at": "2026-01-22T00:14:58.390095Z", "title": "Ethics of Artificial Intelligence | Internet Encyclopedia of Philosophy", "section": "Rules, Risks, and Ethics of AI", "source_type": "Research / Academic", "content_hash": "9f8ed6c1d97f4e21d96dafbd4fa1143bda57020eae4b1780c3debc92d1ab75a2", "chunk_index": 56, "char_start": 74509, "char_end": 75883, "text": "- Kraemer, F., Van Overveld, K., and Peterson, M. (2011). Is There an Ethics of Algorithms? Ethics and Information Technology, 13, 251–60.\n\n- Kurzweil, R. (2005). The Singularity Is Near. London: Penguin Books.\n\n- Levy, D. (2008). Love and Sex with Robots. London: Harper Perennial.\n\n- Lin, P. (2015). Why Ethics Matters for Autonomous Cars. In M. Maurer, J. C. Gerdes, B. Lenz and H. Winner (Eds.), Autonomes Fahren: Technische, rechtliche und gesellschaftliche Aspekte, 69–85. Berlin: Springer.\n\n- Lin, P., Abney, K. and Bekey, G. A. (Eds). (2014). Robot Ethics: The Ethical and Social Implications of Robotics. Intelligent Robotics and Autonomous Agents. Cambridge, MA and London: MIT Press.\n\n- Lin, P., Abney, K. and Jenkins, R. (Eds.) (2017). Robot Ethics 2.0: From Autonomous Cars to Artificial Intelligence. New York: Oxford University Press.\n\n- Loh, J. (2019). Roboterethik. Eine Einführung. Frankfurt am Main: Suhrkamp.\n\n- Ludwig, S. (2015). Credit Scores in America Perpetuate Racial Injustice: Here’s How. The Guardian, October 13. https://www.theguardian.com/commentisfree/2015/oct/13/your-credit-score-is-racist-heres-why.\n\n- Marchese, K. (2020). Japanese Scientists Develop “Blade Runner” Robot That Can Feel Pain. Design Boom, February 24. https://www.designboom.com/technology/japanese-scientists-develop-hyper-realistic-robot-that-can-feel-pain-02-24-2020/."}
{"chunk_id": "iep_utm_edu__b4724b6f15ae5245::c0057", "stable_id": "iep_utm_edu__b4724b6f15ae5245", "url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "final_url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "retrieved_at": "2026-01-22T00:14:58.390095Z", "title": "Ethics of Artificial Intelligence | Internet Encyclopedia of Philosophy", "section": "Rules, Risks, and Ethics of AI", "source_type": "Research / Academic", "content_hash": "9f8ed6c1d97f4e21d96dafbd4fa1143bda57020eae4b1780c3debc92d1ab75a2", "chunk_index": 57, "char_start": 75883, "char_end": 77196, "text": "- Matthias, A. (2004). The Responsibility Gap: Ascribing Responsibility for the Actions of Learning Automata. Ethics and Information Technology, 6(3), 175–83.\n\n- McCarthy, J., Minsky, M. L., Rochester, N. and Shannon, C. E. (1955). A Proposal for the Dartmouth Summer Research Project on Artificial Intelligence.http://raysolomonoff.com/dartmouth/boxa/dart564props.pdf.\n\n- McFall, M. T. (2012). Real Character-Friends: Aristotelian Friendship, Living Together, And Technology. Ethics and Information Technology, 14, 221–30.\n\n- Metzinger, T. (2013), Two Principles for Robot Ethics. In E. Hilgendorf and J.-P. Günther (Eds.), Robotik und Gesetzgebung, 263–302. Baden-Baden: Nomos.\n\n- Metzinger, T. (2019). Ethics Washing Made in Europe. Der Tagesspiegel. https://www.tagesspiegel.de/politik/eu-guidelines-ethics-washing-made-in-europe/24195496.html.\n\n- Misselhorn, C. (2018). Grundfragen der Maschinenethik. Stuttgart: Reclam.\n\n- Mittelstadt, B., Allo, P., Taddeo, M., Wachter, S. and Floridi, L. (2016). The Ethics of Algorithms: Mapping the Debate. 3(2). https://journals.sagepub.com/doi/full/10.1177/2053951716679679.\n\n- Mosakas, K. (2020). On the Moral Status of Social Robots: Considering the Consciousness Criterion. AI & Society, online first at https://link.springer.com/article/10.1007/s00146-020-01002-1."}
{"chunk_id": "iep_utm_edu__b4724b6f15ae5245::c0058", "stable_id": "iep_utm_edu__b4724b6f15ae5245", "url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "final_url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "retrieved_at": "2026-01-22T00:14:58.390095Z", "title": "Ethics of Artificial Intelligence | Internet Encyclopedia of Philosophy", "section": "Rules, Risks, and Ethics of AI", "source_type": "Research / Academic", "content_hash": "9f8ed6c1d97f4e21d96dafbd4fa1143bda57020eae4b1780c3debc92d1ab75a2", "chunk_index": 58, "char_start": 77196, "char_end": 78459, "text": "- Müller, V. C., and Simpson, T. W. (2014). Autonomous Killer Robots Are Probably Good News. Frontiers in Artificial Intelligence and Applications, 273, 297–305.\n\n- Müller, V. C. (2020). Ethics of Artificial Intelligence and Robotics. Stanford Encyclopedia of Philosophy, https://plato.stanford.edu/entries/ethics-ai/.\n\n- Nyholm, S. (2018a). Attributing Agency to Automated Systems: Reflections on Human-Robot Collaborations and Responsibility-Loci. Science and Engineering Ethics, 24(4), 1201–19.\n\n- Nyholm, S. (2018b). The Ethics of Crashes with Self-Driving Cars: A Roadmap, I. Philosophy Compass, 13(7), e12507.\n\n- Nyholm, S. (2018c). The Ethics of Crashes with Self-Driving Cars, A Roadmap, II. Philosophy Compass, 13(7), e12506.\n\n- Nyholm, S. (2020). Humans and Robots: Ethics, Agency, and Anthropomorphism. London: Rowman and Littlefield.\n\n- Nyholm, S., and Frank. L. (2017). From Sex Robots to Love Robots: Is Mutual Love with a Robot Possible? In J. Danaher and N. McArthur, Robot Sex: Social and Ethical Implications. Cambridge, MA: MIT Press.\n\n- Nyholm, S., and Frank, L. (2019). It Loves Me, It Loves Me Not: Is It Morally Problematic to Design Sex Robots That Appear to Love Their Owners? Techne: Research in Philosophy and Technology, 23(3), 402–24."}
{"chunk_id": "iep_utm_edu__b4724b6f15ae5245::c0059", "stable_id": "iep_utm_edu__b4724b6f15ae5245", "url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "final_url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "retrieved_at": "2026-01-22T00:14:58.390095Z", "title": "Ethics of Artificial Intelligence | Internet Encyclopedia of Philosophy", "section": "Rules, Risks, and Ethics of AI", "source_type": "Research / Academic", "content_hash": "9f8ed6c1d97f4e21d96dafbd4fa1143bda57020eae4b1780c3debc92d1ab75a2", "chunk_index": 59, "char_start": 78459, "char_end": 79835, "text": "- Nyholm, S., and Smids, J. (2016). The Ethics of Accident-Algorithms for Self-Driving Cars: An Applied Trolley Problem? Ethical Theory and Moral Practice, 19(5), 1275–89.\n\n- Okyere-Manu, B. (Ed.) (2021). African Values, Ethics, and Technology: Questions, Issues, and Approaches. London: Palgrave MacMillan.\n\n- O’Neil, C. (2016). Weapons of Math Destruction. London: Allen Lane.\n\n- O’Neill, E., Klincewicz, M. and Kemmer, M. (2021). Ethical Issues with Artificial Ethics Assistants. In C. Veliz (Ed.), Oxford Handbook of Digital Ethics. Oxford: Oxford University Press.\n\n- Ord, T. (2020): The Precipice: Existential Risk and the Future of Humanity. London: Hachette Books.\n\n- Picard, R. (1997). Affective Computing. Cambridge, MA and London: MIT Press.\n\n- Purves, D., Jenkins, R. and Strawser, B. J. (2015). Autonomous Machines, Moral Judgment, and Acting for the Right Reasons. Ethical Theory and Moral Practice, 18(4), 851–72.\n\n- Rawls, J. (1999). The Law of Peoples, with The Idea of Public Reason Revisited. Cambridge, MA: Harvard University Press.\n\n- Rawls, J. (2001). Justice as Fairness: A Restatement. Cambridge, MA: Harvard University Press.\n\n- Resseguier, A., and Rodrigues, R. (2020). AI Ethics Should Not Remain Toothless! A Call to Bring Back the Teeth of Ethics. Big Data & Society, online first at https://journals.sagepub.com/doi/full/10.1177/2053951720942541."}
{"chunk_id": "iep_utm_edu__b4724b6f15ae5245::c0060", "stable_id": "iep_utm_edu__b4724b6f15ae5245", "url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "final_url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "retrieved_at": "2026-01-22T00:14:58.390095Z", "title": "Ethics of Artificial Intelligence | Internet Encyclopedia of Philosophy", "section": "Rules, Risks, and Ethics of AI", "source_type": "Research / Academic", "content_hash": "9f8ed6c1d97f4e21d96dafbd4fa1143bda57020eae4b1780c3debc92d1ab75a2", "chunk_index": 60, "char_start": 79835, "char_end": 81051, "text": "- Richardson, K. (2019). Special Issue: Ethics of AI and Robotics. AI & Society, 34(1).\n\n- Robbins, S. (2019). A Misdirected Principle with a Catch: Explicability for AI. Minds and Machines, 29(4), 495–514.\n\n- Royakkers, L., and van Est, R. (2015). Just Ordinary Robots: Automation from Love to War. Boca Raton, FL: CRC Press.\n\n- Russell, S. (2019). Human Compatible. New York: Viking Press.\n\n- Ryan, M., and Stahl, B. (2020). Artificial Intelligence Guidelines for Developers and Users: Clarifying Their Content and Normative Implications. Journal of Information, Communication and Ethics in Society, online first at https://www.emerald.com/insight/content/doi/10.1108/JICES-12-2019-0138/full/html\n\n- Santoni de Sio, F., and Van den Hoven, J. (2018). Meaningful Human Control over Autonomous Systems: A Philosophical Account. Frontiers in Robotics and AI. https://www.frontiersin.org/articles/10.3389/frobt.2018.00015/full.\n\n- Savulescu, J., and Maslen, H. (2015). Moral Enhancement and Artificial Intelligence: Moral AI? In Beyond Artificial Intelligence, 79–95. Springer.\n\n- Schwitzgebel, E., and Garza, M. (2015). A Defense of the Rights of Artificial Intelligences. Midwest Studies in Philosophy, 39(1), 98–119."}
{"chunk_id": "iep_utm_edu__b4724b6f15ae5245::c0061", "stable_id": "iep_utm_edu__b4724b6f15ae5245", "url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "final_url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "retrieved_at": "2026-01-22T00:14:58.390095Z", "title": "Ethics of Artificial Intelligence | Internet Encyclopedia of Philosophy", "section": "Rules, Risks, and Ethics of AI", "source_type": "Research / Academic", "content_hash": "9f8ed6c1d97f4e21d96dafbd4fa1143bda57020eae4b1780c3debc92d1ab75a2", "chunk_index": 61, "char_start": 81051, "char_end": 82336, "text": "- Searle, J. R. (1980). Minds, Brains, and Programs. Behavioural and Brain Sciences, 3(3), 417–57.\n\n- Sharkey, Noel (2018), Mama Mia, It’s Sophia: A Show Robot or Dangerous Platform to Mislead? Forbes, November 17. https://www.forbes.com/sites/noelsharkey/2018/11/17/mama-mia-its- sophia-a-show-robot-or-dangerous-platform-to-mislead/#407e37877ac9.\n\n- Singer, P. (1975). Animal liberation. London, UK: Avon Books.\n\n- Singer, P. (2009). Speciesism and Moral Status. Metaphilosophy, 40(3–4), 567–81.\n\n- Smids, J. (2020). Danaher’s Ethical Behaviourism: An Adequate Guide to Assessing the Moral Status of a Robot? Science and Engineering Ethics, 26(5), 2849–66.\n\n- Smids, J., Nyholm, S. and Berkers, H. (2020). Robots in the Workplace: A Threat to—or Opportunity for—Meaningful Work? Philosophy & Technology, 33(3), 503–22.\n\n- Sparrow, R. (2007). Killer Robots. Journal of Applied Philosophy, 24(1), 62–77.\n\n- Springer, A., Garcia-Gathright, J. and Cramer, H. (2018). Assessing and Addressing Algorithmic Bias – But Before We Get There. In 2018 AAAI Spring Symposium Series, 450–54. https://www.aaai.org/ocs/index.php/SSS/SSS18/paper/viewPaper/17542.\n\n- Stone, C. D. (1972). Should Trees Have Standing? Toward Legal Rights for Natural Objects. Southern California Law Review, 45, 450–501."}
{"chunk_id": "iep_utm_edu__b4724b6f15ae5245::c0062", "stable_id": "iep_utm_edu__b4724b6f15ae5245", "url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "final_url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "retrieved_at": "2026-01-22T00:14:58.390095Z", "title": "Ethics of Artificial Intelligence | Internet Encyclopedia of Philosophy", "section": "Rules, Risks, and Ethics of AI", "source_type": "Research / Academic", "content_hash": "9f8ed6c1d97f4e21d96dafbd4fa1143bda57020eae4b1780c3debc92d1ab75a2", "chunk_index": 62, "char_start": 82336, "char_end": 83537, "text": "- Stone, C. D. (2010). Should Trees Have Standing? Law, Morality and the Environment. Oxford: Oxford University Press.\n\n- Strawser, B. J. (2010). Moral Predators: The Duty to Employ Uninhabited Aerial Vehicles. Journal of Military Ethics, 9(4), 342–68.\n\n- Sullins, J. (2012), Robots, Love, and Sex: The Ethics of Building a Love Machine. IEEE Transactions on Affective Computing, 3(4), 398–409.\n\n- Sweeney, L. (2013). Discrimination in Online Ad Delivery. Acmqueue, 11(3), 1–19.\n\n- Tigard, D. (2020a). There is No Techno-Responsibility Gap. Philosophy & Technology, online first at https://link.springer.com/article/10.1007/s13347-020-00414-7.\n\n- Tigard, D. (2020b). Responsible AI and Moral Responsibility: A Common Appreciation. AI and Ethics, online first at https://link.springer.com/article/10.1007/s43681-020-00009-0.\n\n- Timmermann, J. (2020). Kant’s “Groundwork of the Metaphysics of Morals”: A Commentary. Cambridge: Cambridge University Press.\n\n- Turing, A. (1950). Computing Machinery and Intelligence. Mind, 59(236), 433–60.\n\n- Vallor, S. (2015). Moral Deskilling and Upskilling in a New Machine Age: Reflections on the Ambiguous Future of Character. Philosophy & Technology, 28(1), 107–24."}
{"chunk_id": "iep_utm_edu__b4724b6f15ae5245::c0063", "stable_id": "iep_utm_edu__b4724b6f15ae5245", "url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "final_url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "retrieved_at": "2026-01-22T00:14:58.390095Z", "title": "Ethics of Artificial Intelligence | Internet Encyclopedia of Philosophy", "section": "Rules, Risks, and Ethics of AI", "source_type": "Research / Academic", "content_hash": "9f8ed6c1d97f4e21d96dafbd4fa1143bda57020eae4b1780c3debc92d1ab75a2", "chunk_index": 63, "char_start": 83537, "char_end": 84926, "text": "- Vallor, S. (2016). Technology and the Virtues: A Philosophical Guide to a Future Worth Wanting. New York: Oxford University Press.\n\n- Veale, M., and Binns, R. (2017). Fairer Machine Learning in the Real World: Mitigating Discrimination without Collecting Sensitive Data. Big Data & Society, 4(2).\n\n- Vinge, V. (1983). First Word. Omni, January, 10.\n\n- Vinge, V. (1993). The Coming Technological Singularity. How to Survive in the Post-Human Era. Whole Earth Review, Winter.\n\n- Wachter, S., Mittelstadt, B. and Russell, C. (2018). Counterfactual Explanations without Opening the Black Box: Automated Decisions and the GDPR. Harvard Journal of Law & Technology, 31(2), 841–87.\n\n- Wallach, W., and Allen, C. (2010). Moral Machines. Teaching Robots Right from Wrong. Oxford: Oxford University Press.\n\n- Wallach, W., Franklin, S. and Allen, C. (2010). A Conceptual and Computational Model of Moral Decision Making in Human and Artificial Agents. Topics in Cognitive Science, 2(3), 454–85.\n\n- Wang, Y., and Kosinski, M. (2018). Deep Neural Networks Are More Accurate Than Humans at Detecting Sexual Orientation from Facial Images, Journal of Personality and Social Psychology, 114(2), 246–57.\n\n- Wareham, C. S. (2020): Artificial Intelligence and African Conceptions of Personhood. Ethics and Information Technology, online first at https://link.springer.com/article/10.1007/s10676-020-09541-3"}
{"chunk_id": "iep_utm_edu__b4724b6f15ae5245::c0064", "stable_id": "iep_utm_edu__b4724b6f15ae5245", "url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "final_url": "https://iep.utm.edu/ethics-of-artificial-intelligence/", "retrieved_at": "2026-01-22T00:14:58.390095Z", "title": "Ethics of Artificial Intelligence | Internet Encyclopedia of Philosophy", "section": "Rules, Risks, and Ethics of AI", "source_type": "Research / Academic", "content_hash": "9f8ed6c1d97f4e21d96dafbd4fa1143bda57020eae4b1780c3debc92d1ab75a2", "chunk_index": 64, "char_start": 84926, "char_end": 85515, "text": "- Wong, P. H. (2012). Dao, Harmony, and Personhood: Towards a Confucian Ethics of Technology. Philosophy & Technology, 25(1), 67–86.\n\n- World Economic Forum and Global Future Council on Human Rights (2018). How to Prevent Discriminatory Outcomes in Machine Learning (white paper). http://www3.weforum.org/docs/WEF_40065_White_Paper_How_to_Prevent_Discriminatory_Outcomes_in_Machine_Learning.pdf.\n\nAuthor Information\n\nJohn-Stewart Gordon\n\nEmail: johnstgordon@pm.me\n\nVytautas Magnus University\n\nLithuania\n\nand\n\nSven Nyholm\n\nEmail: s.r.nyholm@uu.nl\n\nThe University of Utrecht\n\nThe Netherlands"}
{"chunk_id": "ai_psu_edu__36e51f436456047f::c0000", "stable_id": "ai_psu_edu__36e51f436456047f", "url": "https://ai.psu.edu/guidelines/", "final_url": "https://ai.psu.edu/guidelines/", "retrieved_at": "2026-01-22T00:15:07.682665Z", "title": "Guidelines - Official Site of the Penn State AI Hub", "section": "Rules, Risks, and Ethics of AI", "source_type": "University / Official", "content_hash": "b50ca840ccb62f193bf3d390e795c3dcdef2a67ecc8248281c41512636668c32", "chunk_index": 0, "char_start": 0, "char_end": 1360, "text": "The AI landscape is evolving rapidly, with its capabilities constantly improving and new tools being released daily. The current parameters on using Generative AI at Penn State are described below. Note that these guidelines for AI use may change as the field progresses. Ethical considerations, regulations, and guidelines surrounding Generative AI must be regularly reviewed due to its ever-evolving nature.\n\nGeneral Guidance\n\nPenn State encourages safe exploration and use of generative AI tools to further our teaching, research, and service mission. Keep these guidelines in mind:\n\nAI tools have varying levels of accessibility which can disadvantage some users. When using an AI tool for more than one user such as in a class or an administrative office, an accessibility review is needed prior to use to comply with policy AD69.\n\nTo initiate your review, send an email to accessibility@psu.edu with the name of the tool, your intended use, and the expected number of users. Below are EEAAP templates for tools that have already been reviewed. You must submit an EEAAP for your specific group use of these tools.\n\nEEAAP Templates\n\nNOTE: Accessibility Office approval does not mean you can use the tool for all types of information. You must still ensure that the tool is approved for the information that you’re putting in the system per the table below."}
{"chunk_id": "ai_psu_edu__36e51f436456047f::c0001", "stable_id": "ai_psu_edu__36e51f436456047f", "url": "https://ai.psu.edu/guidelines/", "final_url": "https://ai.psu.edu/guidelines/", "retrieved_at": "2026-01-22T00:15:07.682665Z", "title": "Guidelines - Official Site of the Penn State AI Hub", "section": "Rules, Risks, and Ethics of AI", "source_type": "University / Official", "content_hash": "b50ca840ccb62f193bf3d390e795c3dcdef2a67ecc8248281c41512636668c32", "chunk_index": 1, "char_start": 1360, "char_end": 2758, "text": "All requests for disability related accommodations must be individually evaluated in accordance with Federal law. Inquiries about student accommodations should be directed to Student Disability Resources. Inquiries about faculty and staff accommodation should be directed to the Office of Equal Opportunity and Access.\n\nAs GenAI tools become more and more prevalent, providing guidance to students has become crucial. The GenAI Use Icons is a framework to assist Penn State instructors in communicating their policies regarding GenAI utilization in coursework. It enables instructors to clearly articulate whether GenAI tools are or are not allowed for specific assignments. These icons can be used to promote transparency, ethical GenAI usage, and academic integrity among students. They are meant to be a tool to foster conversation between students and faculty as to the expectations for successful completion of assignments.\n\nAlthough the University does not endorse any particular generative AI tool, employees are permitted to use generative AI tools of their choice without any review as long as only public, non-confidential data is involved in such use. These individual use cases do not require any sort of delegation or completion of the Software Request Form. If use is intended for a group and/or class, see the Accessibility Considerations guidance above for accessibility compliance."}
{"chunk_id": "ai_psu_edu__36e51f436456047f::c0002", "stable_id": "ai_psu_edu__36e51f436456047f", "url": "https://ai.psu.edu/guidelines/", "final_url": "https://ai.psu.edu/guidelines/", "retrieved_at": "2026-01-22T00:15:07.682665Z", "title": "Guidelines - Official Site of the Penn State AI Hub", "section": "Rules, Risks, and Ethics of AI", "source_type": "University / Official", "content_hash": "b50ca840ccb62f193bf3d390e795c3dcdef2a67ecc8248281c41512636668c32", "chunk_index": 2, "char_start": 2758, "char_end": 3967, "text": "AI tools and platforms are available based on the sensitivity of information being processed and institutional agreements with vendors that include additional protections. The tables describe the tools and platforms available for each level of information and how to obtain access.\n\nThis category of tools includes “chatbots” and AI assistants for general use and productivity. They are designed to understand and generate human-like responses to text-based, natural language prompts. They can generate text, code, and images, translate languages, write different kinds of creative content, or integrate with productivity and collaboration tools.\n\nInformation levels refer to Penn State’s information classification types (Level 1-4).\n\n| Tool | Information Allowed and Prohibited | Availability | ||\n\n|---|---|---|---|---|\n\n| Microsoft Copilot (formerly Bing Chat Enterprise) | Level 1 and 2: allowed Level 3 and 4: prohibited | Available to faculty, staff, and students (over 18 years of age) | ||\n\n| ($) Microsoft Copilot for M365 integrated with M365 Tools (Teams, Outlook, Word, Excel, PowerPoint, etc) | Level 1 and 2: allowed Level 3 and 4: prohibited | Purchase through Software@Penn State catalog | ||"}
{"chunk_id": "ai_psu_edu__36e51f436456047f::c0003", "stable_id": "ai_psu_edu__36e51f436456047f", "url": "https://ai.psu.edu/guidelines/", "final_url": "https://ai.psu.edu/guidelines/", "retrieved_at": "2026-01-22T00:15:07.682665Z", "title": "Guidelines - Official Site of the Penn State AI Hub", "section": "Rules, Risks, and Ethics of AI", "source_type": "University / Official", "content_hash": "b50ca840ccb62f193bf3d390e795c3dcdef2a67ecc8248281c41512636668c32", "chunk_index": 3, "char_start": 3967, "char_end": 5423, "text": "| All other general AI tools (ChatGPT, Claude, Gemini, etc.) | Public, non-confidential information: allowed Information NOT public or confidential: request via Software Request Form | Free or purchased NOTE: Consider using already purchased tools such as Microsoft Copilot before purchasing other tools |\n\n($) Purchase required\n\nThis category of tools includes API access to enable developers to integrate Large Language Models (LLMs) into their own applications, products, or services. This includes chatbot creation and customization, building and testing applications, access to model training and deployment, coding, predictive analytics, and more. Code and low/no-code offerings are available. These tools are subject to change based on availability.\n\n| Platform | Description | Information Allowed and Prohibited | Requesting Access |\n\n|---|---|---|---|\n\n| ($) Azure AI | Build generative AI models from Azure OpenAI Service, Falcon, Stable Diffusion and Meta. | Level 1 and 2: allowed Level 3: allowed with approved Secure Enclave request) Level 4: prohibited | Faculty and staff may request a cloud provider account via ServiceNow |\n\n| ($) Amazon Bedrock | Build generative AI applications using models from AI21 Labs, Anthropic, Cohere, Meta, Mistral AI, and Stability AI. | Level 1 and 2: allowed Level 3: allowed with approved Secure Enclave request) Level 4: prohibited | Faculty and staff may request a cloud provider account via ServiceNow |"}
{"chunk_id": "ai_psu_edu__36e51f436456047f::c0004", "stable_id": "ai_psu_edu__36e51f436456047f", "url": "https://ai.psu.edu/guidelines/", "final_url": "https://ai.psu.edu/guidelines/", "retrieved_at": "2026-01-22T00:15:07.682665Z", "title": "Guidelines - Official Site of the Penn State AI Hub", "section": "Rules, Risks, and Ethics of AI", "source_type": "University / Official", "content_hash": "b50ca840ccb62f193bf3d390e795c3dcdef2a67ecc8248281c41512636668c32", "chunk_index": 4, "char_start": 5423, "char_end": 5885, "text": "| ($) Google Vertex AI | Build generative AI applications using Google and Open Source models. https://cloud.google.com/vertex-ai/generative-ai/docs/model-garden/explore-models | Level 1 and 2: allowed Level 3: allowed with approved Secure Enclave request) Level 4: prohibited | Faculty and staff may request a cloud provider account via ServiceNow |\n\n| All other platforms | Request via Software Request Form |\n\nWe welcome questions or comments at aihub@psu.edu"}
{"chunk_id": "ctl_stanford_edu__7f5d76108cf20c2c::c0000", "stable_id": "ctl_stanford_edu__7f5d76108cf20c2c", "url": "https://ctl.stanford.edu/aimes/ai-learning-guide-students", "final_url": "https://ctl.stanford.edu/aimes/ai-learning-guide-students", "retrieved_at": "2026-01-22T00:14:21.275983Z", "title": "AI and Your Learning: A Guide for Students | Center for Teaching and Learning", "section": "Using AI for School and Work", "source_type": "University / Official", "content_hash": "aae4bd1f416655f68ba6dc70029afac374497d298aaea21de77f980a0f568fcc", "chunk_index": 0, "char_start": 0, "char_end": 1163, "text": "AI and Your Learning: A Guide for Students\n\nIn this guide, we provide information and guidelines to help you make informed decisions about navigating AI tools.\n\nGenerative AI tools, such as ChatGPT, Claude, and Gemini, have evolved quickly over the past few years and become increasingly widespread in their use. While these tools can be exciting, they can also raise questions and concerns. Whether you’re using these tools already or might want to explore them further, making decisions about AI tools as a student can be complicated. Perhaps you’ve wondered: Can I use AI? Should I use AI? How might using AI tools impact my learning?\n\nCan I Use Generative AI for Assignments or Research?\n\nAI policies vary dramatically from one class to the next, often because courses have distinct learning goals. Before you start using AI as a learning or research tool, make sure to review your instructor’s course policy. It is important for you to know if AI is allowed in your course and if so, how. Which ways of using AI are permissible and which are not?\n\nBefore you start using AI as a learning or research tool, make sure to review your instructor’s course policy."}
{"chunk_id": "ctl_stanford_edu__7f5d76108cf20c2c::c0001", "stable_id": "ctl_stanford_edu__7f5d76108cf20c2c", "url": "https://ctl.stanford.edu/aimes/ai-learning-guide-students", "final_url": "https://ctl.stanford.edu/aimes/ai-learning-guide-students", "retrieved_at": "2026-01-22T00:14:21.275983Z", "title": "AI and Your Learning: A Guide for Students | Center for Teaching and Learning", "section": "Using AI for School and Work", "source_type": "University / Official", "content_hash": "aae4bd1f416655f68ba6dc70029afac374497d298aaea21de77f980a0f568fcc", "chunk_index": 1, "char_start": 1163, "char_end": 2718, "text": "You also need to understand that Stanford’s Generative AI Policy Guidance treats any use of generative AI as analogous to receiving help from another person, and using generative AI to substantially complete an assignment is always prohibited. Your default approach should be to always assume AI use is not allowed unless otherwise indicated in the syllabus or assignment, and to disclose any use of generative AI for help with your assignments. In addition, your instructor might have a specific way they want you to document how and when you utilized AI tools. If you are ever unclear about if, how, and when AI is appropriate, please consult with your instructor.\n\nOn a final note, sometimes you may be using AI-related tools for research. In that case, clarify the rules for using AI in your lab, department, discipline, and/or the specific journal that you might be submitting to. They may have specific AI-related policies that you are expected to follow.\n\nIn summary, when in doubt: ask and document!\n\nHow Might the Design of AI Tools Influence My Learning?\n\nPerhaps you’re thinking about using a tool such as ChatGPT to quiz yourself on course concepts, clarify points of confusion, or get feedback on your writing. While this may seem no different from the ways in which you might learn and interact with a tutor or friend, learning with AI is a bit more complicated. Knowing how AI tools work can help you understand their strengths and limitations and ultimately make informed decisions about when and how you use them to support your learning."}
{"chunk_id": "ctl_stanford_edu__7f5d76108cf20c2c::c0002", "stable_id": "ctl_stanford_edu__7f5d76108cf20c2c", "url": "https://ctl.stanford.edu/aimes/ai-learning-guide-students", "final_url": "https://ctl.stanford.edu/aimes/ai-learning-guide-students", "retrieved_at": "2026-01-22T00:14:21.275983Z", "title": "AI and Your Learning: A Guide for Students | Center for Teaching and Learning", "section": "Using AI for School and Work", "source_type": "University / Official", "content_hash": "aae4bd1f416655f68ba6dc70029afac374497d298aaea21de77f980a0f568fcc", "chunk_index": 2, "char_start": 2718, "char_end": 4257, "text": "On a basic level, generative AI tools such as ChatGPT, Claude, and Google Gemini are called “generative” because they produce text, images, audio, or other content in response to a prompt. Through a complex process of machine learning, which involves training the AI model on vast amounts of data and then fine-tuning the model, an AI tool is then able to generate responses that resemble human-generated language. If you ask ChatGPT a question, it is able to respond not because it “knows” the answer to your question, but because it has identified patterns and relationships within the training data and can reassemble language from that data to produce an answer.\n\nThe generative nature of AI tools offers a number of strengths: the tools are easy to use, they can take on a number of roles, and they provide novel ways to learn and create. However, the underlying structure of AI tools can also lead to problems. Because of the way AI tools “guess” statistically plausible information, AI tools will sometimes provide responses that are incorrect, which are called “hallucinations.”\n\nAdditionally, researchers have documented a variety of biases that can exist in training data (e.g. lack of geographical and population diversity) and note that the algorithms that are then applied to this data can further exacerbate these biases (Mehrabi et al., 2021). Research has also shown that AI tools have a tendency to misattribute sources, or in some cases, create a citation for a source that doesn’t exist (Jaźwińska & Chandrasekar, 2025)."}
{"chunk_id": "ctl_stanford_edu__7f5d76108cf20c2c::c0003", "stable_id": "ctl_stanford_edu__7f5d76108cf20c2c", "url": "https://ctl.stanford.edu/aimes/ai-learning-guide-students", "final_url": "https://ctl.stanford.edu/aimes/ai-learning-guide-students", "retrieved_at": "2026-01-22T00:14:21.275983Z", "title": "AI and Your Learning: A Guide for Students | Center for Teaching and Learning", "section": "Using AI for School and Work", "source_type": "University / Official", "content_hash": "aae4bd1f416655f68ba6dc70029afac374497d298aaea21de77f980a0f568fcc", "chunk_index": 3, "char_start": 4257, "char_end": 5469, "text": "Finally, it’s important to note that while the experience of interacting with an AI tool can feel life-like and similar to talking with a human, these tools have serious limitations in their ability to provide holistic support. Whether you’re facing challenges in your learning or in your broader life as a student, it’s important to connect with human support resources such as tutors, academic coaches, advisors, and counselors. Given these limitations, being able to think critically about the output from AI tools is essential, as not everything they generate will be accurate or useful.\n\nCan AI Tools Help Me Meet My Learning Goals?\n\nA frequently cited quote from Herbert Simon, one of the founders of the field of cognitive science, describes how learning takes place: “Learning results from what the student does and thinks and only from what the student does and thinks. The teacher can advance learning only by influencing what the student does to learn” (cited in Ambrose, 2023, p.1). By extension, the specific strategies you use while studying, because they shape what you do and think about during the learning process, also play an important role in whether your efforts at learning are successful."}
{"chunk_id": "ctl_stanford_edu__7f5d76108cf20c2c::c0004", "stable_id": "ctl_stanford_edu__7f5d76108cf20c2c", "url": "https://ctl.stanford.edu/aimes/ai-learning-guide-students", "final_url": "https://ctl.stanford.edu/aimes/ai-learning-guide-students", "retrieved_at": "2026-01-22T00:14:21.275983Z", "title": "AI and Your Learning: A Guide for Students | Center for Teaching and Learning", "section": "Using AI for School and Work", "source_type": "University / Official", "content_hash": "aae4bd1f416655f68ba6dc70029afac374497d298aaea21de77f980a0f568fcc", "chunk_index": 4, "char_start": 5469, "char_end": 6764, "text": "Strategies that involve thinking deeply about the material, such as comparing and contrasting concepts with each other, explaining in one’s own words, and self-quizzing tend to be more effective than superficial strategies such as highlighting or rereading material (Dunlosky, et. al., 2013).\n\nThe relationship between how you use generative AI and your learning depends on how you use this technology. Does a specific use of generative AI facilitate or take away an opportunity for deeper learning? The answer to this question may not be straightforward. For example, effective active reading strategies include previewing the reading and asking yourself questions that might be answered in the text (McGuire, 2018). While generative AI could be used to help with these strategies by generating a summary and questions, does it take away an opportunity for you to engage directly with the text by previewing it yourself? And do you risk introducing hallucinations into any summaries or questions generated by AI? If you use AI as a study aid, what's important is that you’re using effective strategies to facilitate your learning and not delegating your learning process to an AI tool.\n\nThe following framework suggests questions for you to consider when using generative AI as a learning tool."}
{"chunk_id": "ctl_stanford_edu__7f5d76108cf20c2c::c0005", "stable_id": "ctl_stanford_edu__7f5d76108cf20c2c", "url": "https://ctl.stanford.edu/aimes/ai-learning-guide-students", "final_url": "https://ctl.stanford.edu/aimes/ai-learning-guide-students", "retrieved_at": "2026-01-22T00:14:21.275983Z", "title": "AI and Your Learning: A Guide for Students | Center for Teaching and Learning", "section": "Using AI for School and Work", "source_type": "University / Official", "content_hash": "aae4bd1f416655f68ba6dc70029afac374497d298aaea21de77f980a0f568fcc", "chunk_index": 5, "char_start": 6764, "char_end": 8085, "text": "Questions to Consider When Using Generative AI for Learning\n\nAssignments and Research\n\n- How do I know whether I’m allowed to use AI in my assignment or research?\n\n- How am I allowed to use AI in my assignment or research, if at all?\n\n- How should I document my use of AI?\n\n- Am I required to disclose my use of AI?\n\nAccuracy and Bias\n\n- How will I evaluate the accuracy of AI output?\n\n- In what ways can I monitor AI output for bias?\n\n- How will I identify which sources are contributing to AI output and whether they are being used accurately? How will I properly attribute sources?\n\nAI and Your Learning\n\n- Does using generative AI take away an opportunity for me to engage more deeply with the material? If so, are there alternative learning strategies I could use?\n\nIf you’re not sure, a CTL academic coach can help you find new approaches to studying. - Alternatively, does using generative AI deepen my engagement with the material or otherwise help me reach my learning goals? If so, how?\n\n- In addition to the considerations described above, what other factors are important to me as I make decisions about using AI tools? (e.g., the environment, data privacy).\n\nAdditional AI Resources\n\n- Responsible AI at Stanford: Stanford UIT’s guide to using AI tools and models while keeping your and Stanford's data safe."}
{"chunk_id": "ctl_stanford_edu__7f5d76108cf20c2c::c0006", "stable_id": "ctl_stanford_edu__7f5d76108cf20c2c", "url": "https://ctl.stanford.edu/aimes/ai-learning-guide-students", "final_url": "https://ctl.stanford.edu/aimes/ai-learning-guide-students", "retrieved_at": "2026-01-22T00:14:21.275983Z", "title": "AI and Your Learning: A Guide for Students | Center for Teaching and Learning", "section": "Using AI for School and Work", "source_type": "University / Official", "content_hash": "aae4bd1f416655f68ba6dc70029afac374497d298aaea21de77f980a0f568fcc", "chunk_index": 6, "char_start": 8085, "char_end": 9607, "text": "- OCS Generative AI Policy Guide: Stanford Office of Community Standards' guidance on the honor code implications of generative AI tools.\n\n- Stanford's AI Playground: Stanford University hosted platform that allows users to test out multiple AI models in a safe, university-supported environment.\n\n- Student Guide to Artificial Intelligence: Guide published by Elon University providing an overview of key considerations for student use of AI, including skill-building, ethics, academic integrity, and more.\n\nReferences\n\nDunlosky, J., Rawson, K., Marsh, E., Nathan, M., and Willingham, D. (2013). Improving Students’ Learning With Effective Learning Techniques: Promising Directions From Cognitive and Educational Psychology. Psychological Science in the Public Interest, 14(1), 4-58. https://journals.sagepub.com/doi/full/10.1177/1529100612453266\n\nJaźwińska, K. & Chandrasekar, A. (2025, March 6). AI search has a citation problem. Columbia Journalism Review. https://www.cjr.org/tow_center/we-compared-eight-ai-search-engines-theyre-all-bad-at-citing-news.php\n\nLovett, M., Bridges, M., DiPietro, M., Ambrose, S., & Norman, M. (2023). How Learning Works: 8 Research-Based Principles for Smart Teaching. (2nd ed.). Jossey-Bass.\n\nMcGuire, S. (2018). Teach Yourself How to Learn: Strategies You Can Use to Ace Any Course at Any Level. Stylus Publishing.\n\nMehrabi, N., Morstatter, F., Saxena, N., Lerman, K., & Galstyan, A. (2021). A survey on bias and fairness in machine learning. ACM Computing Surveys (CSUR), 54(6), 1-35."}
{"chunk_id": "www_tc_columbia_edu__5643ed615fb8162e::c0000", "stable_id": "www_tc_columbia_edu__5643ed615fb8162e", "url": "https://www.tc.columbia.edu/digitalfuturesinstitute/blog/dfi-perspectives/ai-for-beginners-new/", "final_url": "https://www.tc.columbia.edu/digitalfuturesinstitute/blog/dfi-perspectives/ai-for-beginners-new/", "retrieved_at": "2026-01-22T00:14:14.539810Z", "title": "AI for Beginners: Artificial Intelligence Explained in Simple Terms", "section": "Using AI for School and Work", "source_type": "Explainer / Guide", "content_hash": "e55ba47ece04fd31f535fa42705ef1efe3bd43704f183fb3d9a1bd52b3907231", "chunk_index": 0, "char_start": 0, "char_end": 1258, "text": "By Debbie Beaudry, Director of Academic Technology\n\nIf you're curious about artificial intelligence but don't know where to start, you're in the right place. We'll be your guide, demystifying AI and exploring its many practical applications, whether you want to use these powerful tools or learn more.\n\nWhat is AI?\n\nAI enables machines to perform tasks that typically require human intelligence. You’ve probably used AI without realizing it—like when your streaming service recommends shows, your shopping site suggests products, or your phone predicts text as you type.\n\nWhat Makes AI Different Now?\n\nAI has been around for years, but in November 2022, OpenAI made ChatGPT widely available. This marked a shift because ChatGPT is a generative AI tool—it doesn’t just analyze data; it creates new content based on what you input.\n\nThe Basics of Prompts\n\nInteracting with generative AI tools like ChatGPT involves entering prompts (questions or instructions) and receiving responses (the AI’s generated output).\n\nTypes of Basic Prompts\n\n- Question Prompt – Ask a question, and the AI provides an answer (e.g., What causes lightning?).\n\n- Instruction Prompt – Give a command, and the AI performs a task (e.g., Create a 45-minute lesson plan on climate change)."}
{"chunk_id": "www_tc_columbia_edu__5643ed615fb8162e::c0001", "stable_id": "www_tc_columbia_edu__5643ed615fb8162e", "url": "https://www.tc.columbia.edu/digitalfuturesinstitute/blog/dfi-perspectives/ai-for-beginners-new/", "final_url": "https://www.tc.columbia.edu/digitalfuturesinstitute/blog/dfi-perspectives/ai-for-beginners-new/", "retrieved_at": "2026-01-22T00:14:14.539810Z", "title": "AI for Beginners: Artificial Intelligence Explained in Simple Terms", "section": "Using AI for School and Work", "source_type": "Explainer / Guide", "content_hash": "e55ba47ece04fd31f535fa42705ef1efe3bd43704f183fb3d9a1bd52b3907231", "chunk_index": 1, "char_start": 1258, "char_end": 2528, "text": "Watch this 90-second video to see this in action.\n\nA short video guiding viewers through basic generative AI prompts\n\nHow AI Works\n\nAI tools are trained on vast amounts of information from the internet. Using complex algorithms, they predict and generate responses that mimic human conversation. These tools belong to a category called large language models (LLMs). Watch our 90-second explainer video for more info.\n\nA brief explainer about Generative AI\n\nWhat AI Can Do\n\nGenerative AI tools can:\n\n- Assist with writing (clarify, summarize, or generate content)\n\n- Spark ideas\n\n- Explain topics or serve as a tutor\n\n- Summarize articles, videos, or audio\n\n- Help shift focus from routine tasks to higher-level thinking\n\nAI Concerns & Limitations\n\nWhile AI is a powerful tool with many benefits, it also comes with challenges and ethical concerns. Understanding these can help you use AI responsibly. Here are some key issues to keep in mind:\n\n- Data Privacy – Be cautious about sharing personal or proprietary information; some AI models may use inputs for further training.\n\n- Inaccuracy (Hallucinations) – AI sometimes generates incorrect or misleading information that sounds plausible.\n\n- Biases – Since AI learns from internet data, it can reflect existing biases."}
{"chunk_id": "www_tc_columbia_edu__5643ed615fb8162e::c0002", "stable_id": "www_tc_columbia_edu__5643ed615fb8162e", "url": "https://www.tc.columbia.edu/digitalfuturesinstitute/blog/dfi-perspectives/ai-for-beginners-new/", "final_url": "https://www.tc.columbia.edu/digitalfuturesinstitute/blog/dfi-perspectives/ai-for-beginners-new/", "retrieved_at": "2026-01-22T00:14:14.539810Z", "title": "AI for Beginners: Artificial Intelligence Explained in Simple Terms", "section": "Using AI for School and Work", "source_type": "Explainer / Guide", "content_hash": "e55ba47ece04fd31f535fa42705ef1efe3bd43704f183fb3d9a1bd52b3907231", "chunk_index": 2, "char_start": 2528, "char_end": 3599, "text": "- Ethical Issues – AI tools are trained on content scraped from the web, often without permission from creators. Lawsuits regarding copyright violations are ongoing.\n\n- Accessibility - Not all AI tools are built to be accessible for people with disabilities.\n\n- Environmental Impact – AI models require vast computing power and consume significant electricity and water.\n\n- Human Labor – AI moderation sometimes relies on underpaid workers who review harmful content, sometimes at personal cost.\n\nWhich AI Tool to Try?\n\nSince ChatGPT’s release, many other generative AI text tools have emerged. Most offer free versions but also have paid upgrades for more advanced features. Here are some popular options:\n\nWhere to Start\n\nThe best way to understand AI is to try it yourself. Start with a free version of one of these tools and ask a few basic questions. Remember: Do not enter personal or sensitive information.\n\nGenerative AI is a powerful tool—but like any technology, it has strengths and risks. The key is using it wisely and responsibly.\n\nAI transparency statement"}
{"chunk_id": "www_tc_columbia_edu__5643ed615fb8162e::c0003", "stable_id": "www_tc_columbia_edu__5643ed615fb8162e", "url": "https://www.tc.columbia.edu/digitalfuturesinstitute/blog/dfi-perspectives/ai-for-beginners-new/", "final_url": "https://www.tc.columbia.edu/digitalfuturesinstitute/blog/dfi-perspectives/ai-for-beginners-new/", "retrieved_at": "2026-01-22T00:14:14.539810Z", "title": "AI for Beginners: Artificial Intelligence Explained in Simple Terms", "section": "Using AI for School and Work", "source_type": "Explainer / Guide", "content_hash": "e55ba47ece04fd31f535fa42705ef1efe3bd43704f183fb3d9a1bd52b3907231", "chunk_index": 3, "char_start": 3599, "char_end": 4967, "text": "My first step was to write the piece in a Google Doc without the use of generative AI. I then entered into ChatGPT the prompt “I'm writing a blog post about the basics of generative AI. The audience is adults who are non-technical. The writing style should be clear and casual. Here is a rough draft. Could you please make the text clearer, more concise, and organized?” I then pasted into ChatGPT the text of my draft. When ChatGPT came back with a response, I pasted the ChatGPT version back into a Google Doc and I revised it. The AI had made some of my text too concise so important meaning was lost. I also made small edits because some of the language didn’t feel like mine. The final version was reviewed by humans on our team who provided feedback.\n\nDisclaimer: The applications and their respective companies shared in this article have not been vetted or endorsed by Teachers College. Teachers College does not assume any responsibility for the accessibility, privacy, or security of these applications. The usage of these applications is at the sole discretion of the user, and Teachers College disclaims any legal liability associated with their usage. It is the responsibility of the users to conduct their own research and adhere to applicable laws, regulations, and best practices when incorporating these applications into their educational activities."}
{"chunk_id": "sites_campbell_edu__62497847e244cab4::c0000", "stable_id": "sites_campbell_edu__62497847e244cab4", "url": "https://sites.campbell.edu/academictechnology/2025/03/06/ai-in-higher-education-a-summary-of-recent-surveys-of-students-and-faculty/", "final_url": "https://sites.campbell.edu/academictechnology/2025/03/06/ai-in-higher-education-a-summary-of-recent-surveys-of-students-and-faculty/", "retrieved_at": "2026-01-22T00:14:23.461690Z", "title": "AI in Higher Education: A Meta Summary of Recent Surveys of Students and Faculty – Campbell Academic Technology Services", "section": "Using AI for School and Work", "source_type": "University / Official", "content_hash": "6207ddcc63c73b4300cb0baf4ee85c1f680cabfa1737a96bc2c273fe9edc1c65", "chunk_index": 0, "char_start": 0, "char_end": 1596, "text": "Overview:\n\nArtificial intelligence (AI) is rapidly transforming the landscape of higher education, impacting how students learn, how faculty teach, and how institutions operate. This report summarizes the findings of recent surveys that explored AI usage, AI competencies, and attitudes toward AI among students and faculty in higher education settings. The surveys included in this report were conducted in 2024 and 2025, providing a snapshot of the current state of AI in higher education and offering insights into its potential future. This summary was created with the help of Google Gemini Advanced Pro 1.5 which greatly aided in finding the survey data and in compiling the final results for this report.\n\nAI Usage Among Students\n\nSeveral surveys conducted in 2024 and 2025 revealed that a significant majority of students are using AI in their studies. A global survey by the Digital Education Council found that 86% of students use AI in their studies, with 54% using it weekly and nearly one in four using it daily 1. This high rate of adoption is mirrored in other surveys, such as the Chegg survey of 11,706 undergraduate students across 15 countries, which found that 80% of students worldwide have used generative AI to support their university studies 2. Similarly, a survey of 1,041 full-time undergraduate students in the UK by HEPI and Kortext found that 92% of students were using AI tools in their studies, up from 66% the previous year 3. This rapid increase in student AI adoption in a short period highlights the growing importance of AI in the student learning experience."}
{"chunk_id": "sites_campbell_edu__62497847e244cab4::c0001", "stable_id": "sites_campbell_edu__62497847e244cab4", "url": "https://sites.campbell.edu/academictechnology/2025/03/06/ai-in-higher-education-a-summary-of-recent-surveys-of-students-and-faculty/", "final_url": "https://sites.campbell.edu/academictechnology/2025/03/06/ai-in-higher-education-a-summary-of-recent-surveys-of-students-and-faculty/", "retrieved_at": "2026-01-22T00:14:23.461690Z", "title": "AI in Higher Education: A Meta Summary of Recent Surveys of Students and Faculty – Campbell Academic Technology Services", "section": "Using AI for School and Work", "source_type": "University / Official", "content_hash": "6207ddcc63c73b4300cb0baf4ee85c1f680cabfa1737a96bc2c273fe9edc1c65", "chunk_index": 1, "char_start": 1596, "char_end": 2925, "text": "Students are using a variety of AI tools to support their studies, with ChatGPT emerging as the most popular, followed by Grammarly and Microsoft Copilot 1. These tools are being used for a wide range of tasks, including:\n\n- Searching for information: Students are using AI to quickly and efficiently find relevant information for their coursework, research projects, and general knowledge inquiries 1.\n\n- Checking grammar and improving writing: AI-powered writing assistants like Grammarly are helping students improve the clarity, conciseness, and accuracy of their writing 1.\n\n- Summarizing and paraphrasing documents: AI tools are being used to condense lengthy articles and research papers into concise summaries, and to rephrase text while maintaining its original meaning 1.\n\n- Creating first drafts: Students are leveraging AI to generate initial drafts of essays, reports, and other written assignments, providing a starting point for their own writing and editing process 1.\n\n- Explaining complex concepts: AI tutors and chatbots are helping students understand difficult concepts by providing clear explanations, examples, and step-by-step guidance 4.\n\n- Suggesting research ideas: AI tools are being used to generate research topics, identify relevant sources, and explore different perspectives on a given subject 4."}
{"chunk_id": "sites_campbell_edu__62497847e244cab4::c0002", "stable_id": "sites_campbell_edu__62497847e244cab4", "url": "https://sites.campbell.edu/academictechnology/2025/03/06/ai-in-higher-education-a-summary-of-recent-surveys-of-students-and-faculty/", "final_url": "https://sites.campbell.edu/academictechnology/2025/03/06/ai-in-higher-education-a-summary-of-recent-surveys-of-students-and-faculty/", "retrieved_at": "2026-01-22T00:14:23.461690Z", "title": "AI in Higher Education: A Meta Summary of Recent Surveys of Students and Faculty – Campbell Academic Technology Services", "section": "Using AI for School and Work", "source_type": "University / Official", "content_hash": "6207ddcc63c73b4300cb0baf4ee85c1f680cabfa1737a96bc2c273fe9edc1c65", "chunk_index": 2, "char_start": 2925, "char_end": 4693, "text": "Despite the widespread use of AI and the potential benefits it offers, many students express concerns about its accuracy, reliability, and ethical implications. In the Chegg survey, 53% of students who had used AI to support their studies expressed concern about receiving incorrect or inaccurate information 2. This concern is particularly relevant in educational contexts where accuracy and trustworthiness of information are paramount. Students also voiced concerns about data privacy and the potential for AI to undermine critical thinking skills 5. If students become overly reliant on AI to provide answers and complete tasks, they may not develop the critical thinking and problem-solving skills necessary for academic success and future careers.\n\nFurthermore, a survey conducted at the Tecnologico de Monterrey in Guadalajara found that 55% of students believe that AI could negatively impact academic integrity 6. This concern stems from the potential for students to misuse AI tools to cheat on assignments or exams, such as by generating essays or answers automatically without demonstrating their own understanding or effort.\n\nAI Usage Among Faculty\n\nSurveys of faculty in higher education suggest that AI adoption among educators is increasing, but still lags behind student use. A survey by Ellucian found that 93% of higher education staff expect to expand their use of AI for work purposes over the next two years 7. However, the Digital Education Council’s 2025 Global AI Faculty Survey found that while 61% of faculty have used AI in teaching, 88% do so minimally 8. This suggests that many faculty may be hesitant to fully integrate AI into their teaching practices or may be less aware of the potential benefits and applications of AI in education."}
{"chunk_id": "sites_campbell_edu__62497847e244cab4::c0003", "stable_id": "sites_campbell_edu__62497847e244cab4", "url": "https://sites.campbell.edu/academictechnology/2025/03/06/ai-in-higher-education-a-summary-of-recent-surveys-of-students-and-faculty/", "final_url": "https://sites.campbell.edu/academictechnology/2025/03/06/ai-in-higher-education-a-summary-of-recent-surveys-of-students-and-faculty/", "retrieved_at": "2026-01-22T00:14:23.461690Z", "title": "AI in Higher Education: A Meta Summary of Recent Surveys of Students and Faculty – Campbell Academic Technology Services", "section": "Using AI for School and Work", "source_type": "University / Official", "content_hash": "6207ddcc63c73b4300cb0baf4ee85c1f680cabfa1737a96bc2c273fe9edc1c65", "chunk_index": 3, "char_start": 4693, "char_end": 6045, "text": "A survey by the American Association of Colleges & Universities (AAC&U) and Elon University found that most higher education leaders say that smaller numbers of faculty use AI as part of their jobs, with 62% estimating that fewer than half of faculty use the tools 9. This disparity between faculty and student AI usage raises questions about how institutions can effectively support faculty in adopting and integrating AI in ways that enhance teaching and learning.\n\nFaculty are exploring various ways to use AI in their work, including:\n\n- Supporting curriculum design: AI can assist faculty in designing more effective and engaging curricula by analyzing student data, identifying learning gaps, and suggesting relevant resources 10.\n\n- Automating administrative workflows: AI can automate repetitive administrative tasks, such as grading, scheduling, and communication, freeing up faculty time for more meaningful activities like teaching and research 10.\n\n- Enhancing teaching practices: AI can be used to create personalized learning experiences, provide targeted feedback, and adapt instruction to individual student needs 11.\n\n- Personalizing learning: AI-powered learning platforms can tailor content, pacing, and activities to each student’s learning style and preferences, creating a more individualized and effective learning experience 11."}
{"chunk_id": "sites_campbell_edu__62497847e244cab4::c0004", "stable_id": "sites_campbell_edu__62497847e244cab4", "url": "https://sites.campbell.edu/academictechnology/2025/03/06/ai-in-higher-education-a-summary-of-recent-surveys-of-students-and-faculty/", "final_url": "https://sites.campbell.edu/academictechnology/2025/03/06/ai-in-higher-education-a-summary-of-recent-surveys-of-students-and-faculty/", "retrieved_at": "2026-01-22T00:14:23.461690Z", "title": "AI in Higher Education: A Meta Summary of Recent Surveys of Students and Faculty – Campbell Academic Technology Services", "section": "Using AI for School and Work", "source_type": "University / Official", "content_hash": "6207ddcc63c73b4300cb0baf4ee85c1f680cabfa1737a96bc2c273fe9edc1c65", "chunk_index": 4, "char_start": 6045, "char_end": 7400, "text": "Similar to students, faculty also express concerns about the accuracy, ethical implications, and potential downsides of AI in education. In the Ellucian survey, 49% of respondents were worried about bias in AI models, and 59% reported concerns about data security and privacy 12. Faculty also voiced concerns about the potential for AI to undermine academic integrity and critical thinking skills 12. A survey of faculty at Northeastern University found that the most common concerns about AI included the potential for cheating, over-reliance on AI, and ethical concerns 13. These concerns highlight the need for institutions to provide clear guidelines and support for faculty in using AI responsibly and ethically.\n\nAI Competencies Among Students and Faculty\n\nWhile both students and faculty are increasingly using AI, surveys suggest that many feel unprepared for an AI-driven world. In the Digital Education Council’s student survey, 58% of students reported that they do not feel they have sufficient AI knowledge and skills, and 48% do not feel adequately prepared for an AI-enabled workplace 14. This lack of AI competency is not limited to students. In the AAC&U and Elon University survey, 59% of higher education leaders believed that last spring’s graduates were not prepared for work in companies where skill in using AI tools is important 9."}
{"chunk_id": "sites_campbell_edu__62497847e244cab4::c0005", "stable_id": "sites_campbell_edu__62497847e244cab4", "url": "https://sites.campbell.edu/academictechnology/2025/03/06/ai-in-higher-education-a-summary-of-recent-surveys-of-students-and-faculty/", "final_url": "https://sites.campbell.edu/academictechnology/2025/03/06/ai-in-higher-education-a-summary-of-recent-surveys-of-students-and-faculty/", "retrieved_at": "2026-01-22T00:14:23.461690Z", "title": "AI in Higher Education: A Meta Summary of Recent Surveys of Students and Faculty – Campbell Academic Technology Services", "section": "Using AI for School and Work", "source_type": "University / Official", "content_hash": "6207ddcc63c73b4300cb0baf4ee85c1f680cabfa1737a96bc2c273fe9edc1c65", "chunk_index": 5, "char_start": 7400, "char_end": 8851, "text": "The Digital Education Council’s faculty survey found that 40% of faculty feel that they are just beginning their AI literacy journey, and only 17% are at an advanced or expert level 15. This disconnect between high AI usage and low AI competency among both students and faculty underscores the urgent need for AI literacy initiatives in higher education. Institutions need to invest in training and resources to help students and educators develop the competencies needed to thrive in an AI-driven world.\n\nFurthermore, it is important to consider the equity implications of AI preparedness. A survey by Inside Higher Ed found that first-generation students were less likely to be confident in appropriate use cases for AI compared to their continuing-generation peers 5. This finding suggests that AI has the potential to exacerbate existing inequalities in education, particularly for students from underrepresented backgrounds who may have less access to AI tools and resources or less support in developing AI literacy.\n\nAttitudes Toward AI in Higher Education\n\nSurveys reveal a mixed bag of attitudes toward AI in higher education, with both optimism and concern about its potential impact. Many students and faculty express optimism about the potential for AI to enhance learning and teaching 11. AI is seen as a tool that can personalize learning, provide individualized support, and improve efficiency in both teaching and administrative tasks."}
{"chunk_id": "sites_campbell_edu__62497847e244cab4::c0006", "stable_id": "sites_campbell_edu__62497847e244cab4", "url": "https://sites.campbell.edu/academictechnology/2025/03/06/ai-in-higher-education-a-summary-of-recent-surveys-of-students-and-faculty/", "final_url": "https://sites.campbell.edu/academictechnology/2025/03/06/ai-in-higher-education-a-summary-of-recent-surveys-of-students-and-faculty/", "retrieved_at": "2026-01-22T00:14:23.461690Z", "title": "AI in Higher Education: A Meta Summary of Recent Surveys of Students and Faculty – Campbell Academic Technology Services", "section": "Using AI for School and Work", "source_type": "University / Official", "content_hash": "6207ddcc63c73b4300cb0baf4ee85c1f680cabfa1737a96bc2c273fe9edc1c65", "chunk_index": 6, "char_start": 8851, "char_end": 10271, "text": "However, there are also concerns about the potential negative impacts of AI, such as increased cheating, over-reliance on AI tools, and the widening of digital inequities 16. A survey of faculty at Metropolitan State University of Denver found that 42% of respondents believed that AI tools could have significant time-saving and efficiency-increasing benefits when used ethically and appropriately 17. However, 29% of respondents were concerned about AI’s impact on academic integrity and a decrease in critical thinking skills 17.\n\nInterestingly, there are regional differences in attitudes toward AI in higher education. A survey by the Digital Education Council found that faculty in the US and Canada have a more negative view of AI compared to other regions 18. While 65% of faculty worldwide saw AI as an opportunity, only 57% of faculty in the US and Canada shared this view. This difference in perspective may be attributed to various factors, such as cultural differences, concerns about job security, or varying levels of exposure to and understanding of AI technologies.\n\nThese findings suggest that institutions need to proactively address the concerns and challenges associated with AI while also promoting its potential benefits. This includes developing clear policies and guidelines for AI use, providing training and support for students and faculty, and fostering a culture of responsible AI adoption."}
{"chunk_id": "sites_campbell_edu__62497847e244cab4::c0007", "stable_id": "sites_campbell_edu__62497847e244cab4", "url": "https://sites.campbell.edu/academictechnology/2025/03/06/ai-in-higher-education-a-summary-of-recent-surveys-of-students-and-faculty/", "final_url": "https://sites.campbell.edu/academictechnology/2025/03/06/ai-in-higher-education-a-summary-of-recent-surveys-of-students-and-faculty/", "retrieved_at": "2026-01-22T00:14:23.461690Z", "title": "AI in Higher Education: A Meta Summary of Recent Surveys of Students and Faculty – Campbell Academic Technology Services", "section": "Using AI for School and Work", "source_type": "University / Official", "content_hash": "6207ddcc63c73b4300cb0baf4ee85c1f680cabfa1737a96bc2c273fe9edc1c65", "chunk_index": 7, "char_start": 10271, "char_end": 11558, "text": "Comparing Student and Faculty Survey Findings\n\nWhile many of the surveys focused on either students or faculty, some key comparisons can be drawn:\n\n| Category | Student Findings | Faculty Findings |\n\n| AI Usage | Higher rates of AI use, particularly for coursework and assessments | Lower rates of AI use, with many faculty using AI minimally |\n\n| AI Competencies | Express a need for more training and support in developing AI literacy and skills | Express a need for more training and support in developing AI literacy and skills |\n\n| Attitudes Toward AI | A mix of optimism and concern about AI in higher education | A mix of optimism and concern about AI in higher education, with some regional variations |\n\nThese comparisons highlight some important trends in AI adoption in higher education. Students appear to be more readily embracing AI tools, while faculty are more cautious in their adoption and integration of AI. Both students and faculty recognize the need for greater AI literacy and skills development, suggesting a shared responsibility for institutions to provide adequate support and training.\n\nPotential Implications for the Future of AI in Higher Education\n\nThe findings of these surveys have several potential implications for the future of AI in higher education:"}
{"chunk_id": "sites_campbell_edu__62497847e244cab4::c0008", "stable_id": "sites_campbell_edu__62497847e244cab4", "url": "https://sites.campbell.edu/academictechnology/2025/03/06/ai-in-higher-education-a-summary-of-recent-surveys-of-students-and-faculty/", "final_url": "https://sites.campbell.edu/academictechnology/2025/03/06/ai-in-higher-education-a-summary-of-recent-surveys-of-students-and-faculty/", "retrieved_at": "2026-01-22T00:14:23.461690Z", "title": "AI in Higher Education: A Meta Summary of Recent Surveys of Students and Faculty – Campbell Academic Technology Services", "section": "Using AI for School and Work", "source_type": "University / Official", "content_hash": "6207ddcc63c73b4300cb0baf4ee85c1f680cabfa1737a96bc2c273fe9edc1c65", "chunk_index": 8, "char_start": 11558, "char_end": 13089, "text": "- Increased integration of AI in teaching and learning: As AI tools become more sophisticated and accessible, we can expect to see even greater integration of AI in teaching and learning practices. This may include the use of AI-powered tutors, personalized learning platforms, and automated assessment tools. AI has the potential to democratize education by providing personalized support and expanding access to learning resources 5. However, institutions will need to carefully consider how to integrate AI in ways that enhance, rather than replace, human interaction and critical thinking.\n\n- Emphasis on AI ethics and responsible use: Institutions will need to place a greater emphasis on AI ethics and responsible use. This includes developing clear policies and guidelines for AI use, providing training on AI bias and fairness, and promoting critical thinking skills in evaluating AI-generated content. It is crucial to ensure that AI is used in a way that aligns with the values and mission of higher education, and that it does not perpetuate or exacerbate existing inequalities.\n\n- Evolving role of educators: The role of educators will likely evolve in response to the rise of AI. Educators may need to adapt their teaching methods to incorporate AI tools and to focus on developing students’ higher-order thinking skills, such as critical thinking, problem-solving, and creativity. As AI takes on more routine tasks, educators can focus on fostering deeper learning, mentorship, and personalized guidance for students."}
{"chunk_id": "sites_campbell_edu__62497847e244cab4::c0009", "stable_id": "sites_campbell_edu__62497847e244cab4", "url": "https://sites.campbell.edu/academictechnology/2025/03/06/ai-in-higher-education-a-summary-of-recent-surveys-of-students-and-faculty/", "final_url": "https://sites.campbell.edu/academictechnology/2025/03/06/ai-in-higher-education-a-summary-of-recent-surveys-of-students-and-faculty/", "retrieved_at": "2026-01-22T00:14:23.461690Z", "title": "AI in Higher Education: A Meta Summary of Recent Surveys of Students and Faculty – Campbell Academic Technology Services", "section": "Using AI for School and Work", "source_type": "University / Official", "content_hash": "6207ddcc63c73b4300cb0baf4ee85c1f680cabfa1737a96bc2c273fe9edc1c65", "chunk_index": 9, "char_start": 13089, "char_end": 14480, "text": "- Addressing digital equity: Institutions will need to address the digital divide and ensure that all students have equitable access to AI tools and resources. This may include providing technology support, offering affordable AI training programs, and promoting inclusive AI practices. It is essential to ensure that the benefits of AI are shared by all students, regardless of their background or socioeconomic status.\n\nConclusion\n\nAI has already had a profound impact on higher education, transforming how students learn, how faculty teach, and how institutions operate. The findings of these recent surveys provide valuable insights into the current state of AI adoption, competencies, and attitudes among students and faculty at various higher education institutions. These trends highlight the associated challenges institutions are facing as they adopt AI platforms while trying to balance the benefits and challenges of AI tool use in learning and teaching.\n\nAppendix: Matrix Summary of Key Survey Findings\n\n| Survey | Focus | Key Findings | Methodology |\n\n| Digital Education Council Global AI Student Survey 2024 1 | AI usage among students | 86% of students use AI in their studies; ChatGPT is the most popular tool; students express concerns about AI literacy and university integration of AI | 3,839 responses from bachelor, masters, and doctorate students across 16 countries |"}
{"chunk_id": "sites_campbell_edu__62497847e244cab4::c0010", "stable_id": "sites_campbell_edu__62497847e244cab4", "url": "https://sites.campbell.edu/academictechnology/2025/03/06/ai-in-higher-education-a-summary-of-recent-surveys-of-students-and-faculty/", "final_url": "https://sites.campbell.edu/academictechnology/2025/03/06/ai-in-higher-education-a-summary-of-recent-surveys-of-students-and-faculty/", "retrieved_at": "2026-01-22T00:14:23.461690Z", "title": "AI in Higher Education: A Meta Summary of Recent Surveys of Students and Faculty – Campbell Academic Technology Services", "section": "Using AI for School and Work", "source_type": "University / Official", "content_hash": "6207ddcc63c73b4300cb0baf4ee85c1f680cabfa1737a96bc2c273fe9edc1c65", "chunk_index": 10, "char_start": 14480, "char_end": 15935, "text": "| Chegg Global Student Survey 2025 2 | AI usage among students | 80% of students worldwide have used generative AI; accuracy is a top concern; students want AI tools designed for education | 11,706 undergraduate students aged 18-21 years across 15 countries |\n\n| HEPI/Kortext Student Generative AI Survey 2025 3 | AI usage among students | 92% of students use AI tools; AI use has increased significantly since 2024; students use AI to save time and improve work quality | 1,041 online interviews with full-time undergraduate students in the UK |\n\n| Ellucian AI in Higher Education Survey 2024 12 | AI usage among faculty and administrators | 93% of higher ed staff expect to expand AI use; concerns about bias, data privacy, and impact on critical thinking | 445 faculty and administrators from more than 330 institutions across the U.S. and Canada |\n\n| AAC&U/Elon University Survey of Higher Education Leaders 2024 9 | AI usage and attitudes among higher education leaders | 89% of leaders say students use AI; 59% report increased cheating; 91% believe AI will enhance learning | 337 college leaders responded to at least some portion of the survey |\n\n| Digital Education Council Global AI Faculty Survey 2025 15 | AI usage and attitudes among faculty | 61% of faculty have used AI in teaching; 88% do so minimally; concerns about student critical evaluation of AI outputs | 1,681 faculty members at 52 higher education institutions from 28 countries |"}
{"chunk_id": "sites_campbell_edu__62497847e244cab4::c0011", "stable_id": "sites_campbell_edu__62497847e244cab4", "url": "https://sites.campbell.edu/academictechnology/2025/03/06/ai-in-higher-education-a-summary-of-recent-surveys-of-students-and-faculty/", "final_url": "https://sites.campbell.edu/academictechnology/2025/03/06/ai-in-higher-education-a-summary-of-recent-surveys-of-students-and-faculty/", "retrieved_at": "2026-01-22T00:14:23.461690Z", "title": "AI in Higher Education: A Meta Summary of Recent Surveys of Students and Faculty – Campbell Academic Technology Services", "section": "Using AI for School and Work", "source_type": "University / Official", "content_hash": "6207ddcc63c73b4300cb0baf4ee85c1f680cabfa1737a96bc2c273fe9edc1c65", "chunk_index": 11, "char_start": 15935, "char_end": 17144, "text": "| Metropolitan State University of Denver Faculty Survey 2024 17 | Faculty perceptions of AI | 78% of faculty unfamiliar with AI; 42% believe AI can have time-saving benefits; concerns about academic integrity and critical thinking | Survey responses were collected from faculty members at Metropolitan State University of Denver |\n\nWorks cited\n\n1. Survey: 86% of Students Already Use AI in Their Studies — Campus …, accessed March 6, 2025, https://campustechnology.com/articles/2024/08/28/survey-86-of-students-already-use-ai-in-their-studies.aspx\n\n2. Chegg Global Student Survey 2025: 80% of … – Chegg, Inc., accessed March 6, 2025, https://investor.chegg.com/Press-Releases/press-release-details/2025/Chegg-Global-Student-Survey-2025-80-of-Undergraduates-Worldwide-Have-Used-GenAI-to-Support-their-Studies–But-Accuracy-a-Top-Concern/default.aspx\n\n3. HEPI/Kortext AI survey shows explosive increase in the use of …, accessed March 6, 2025, https://www.hepi.ac.uk/2025/02/26/hepi-kortext-ai-survey-shows-explosive-increase-in-the-use-of-generative-ai-tools-by-students/\n\n4. Student Generative AI Survey 2025 – HEPI, accessed March 6, 2025, https://www.hepi.ac.uk/2025/02/26/student-generative-ai-survey-2025/"}
{"chunk_id": "sites_campbell_edu__62497847e244cab4::c0012", "stable_id": "sites_campbell_edu__62497847e244cab4", "url": "https://sites.campbell.edu/academictechnology/2025/03/06/ai-in-higher-education-a-summary-of-recent-surveys-of-students-and-faculty/", "final_url": "https://sites.campbell.edu/academictechnology/2025/03/06/ai-in-higher-education-a-summary-of-recent-surveys-of-students-and-faculty/", "retrieved_at": "2026-01-22T00:14:23.461690Z", "title": "AI in Higher Education: A Meta Summary of Recent Surveys of Students and Faculty – Campbell Academic Technology Services", "section": "Using AI for School and Work", "source_type": "University / Official", "content_hash": "6207ddcc63c73b4300cb0baf4ee85c1f680cabfa1737a96bc2c273fe9edc1c65", "chunk_index": 12, "char_start": 17144, "char_end": 18365, "text": "5. College students uncertain about AI policies in classrooms, accessed March 6, 2025, https://www.insidehighered.com/news/student-success/academic-life/2024/09/16/college-students-uncertain-about-ai-policies\n\n6. AI in Higher Education: A Revolution or a Risk? – Observatory, accessed March 6, 2025, https://observatory.tec.mx/edu-bits-2/ai-in-higher-education-a-revolution-or-a-risk/\n\n7. Survey of U.S. Higher Education Faculty 2024: Contributions of Content & Design to AI Applications – TechPipeline, accessed March 6, 2025, https://techpipeline.com/product/survey-of-u-s-higher-education-faculty-2024-contributions-of-content-design-to-ai-applications/\n\n8. Empowering faculty to lead AI decision-making – Community …, accessed March 6, 2025, https://www.ccdaily.com/2025/02/empowering-faculty-to-lead-ai-decision-making/\n\n9. Elon/AAC&U survey focuses on AI’s impact on teaching and learning …, accessed March 6, 2025, https://www.elon.edu/u/news/2025/01/23/elon-aacu-survey-focuses-on-ais-impact-on-teaching-and-learning/\n\n10. Survey: Higher Ed AI Adoption Faces Financial, Policy Hurdles, accessed March 6, 2025, https://www.govtech.com/education/higher-ed/survey-higher-ed-ai-adoption-faces-financial-policy-hurdles"}
{"chunk_id": "sites_campbell_edu__62497847e244cab4::c0013", "stable_id": "sites_campbell_edu__62497847e244cab4", "url": "https://sites.campbell.edu/academictechnology/2025/03/06/ai-in-higher-education-a-summary-of-recent-surveys-of-students-and-faculty/", "final_url": "https://sites.campbell.edu/academictechnology/2025/03/06/ai-in-higher-education-a-summary-of-recent-surveys-of-students-and-faculty/", "retrieved_at": "2026-01-22T00:14:23.461690Z", "title": "AI in Higher Education: A Meta Summary of Recent Surveys of Students and Faculty – Campbell Academic Technology Services", "section": "Using AI for School and Work", "source_type": "University / Official", "content_hash": "6207ddcc63c73b4300cb0baf4ee85c1f680cabfa1737a96bc2c273fe9edc1c65", "chunk_index": 13, "char_start": 18365, "char_end": 19576, "text": "11. 2024 in Review | AI & Education – Cengage Group, accessed March 6, 2025, https://www.cengagegroup.com/news/perspectives/2024/2024-in-review-ai–education/\n\n12. Ellucian’s AI Survey of Higher Education Professionals Reveals …, accessed March 6, 2025, https://www.ellucian.com/news/ellucians-ai-survey-higher-education-professionals-reveals-surge-ai-adoption-despite-concerns\n\n13. faculty.northeastern.edu, accessed March 6, 2025, https://faculty.northeastern.edu/senate/wp-content/uploads/sites/2/2024/03/Addendum-4-Survey-Results-Adopt-or-Avoid-Northeastern-Faculty-views-on-using-generative-AI.pdf\n\n14. What Students Want: Key Results from DEC Global AI Student …, accessed March 6, 2025, https://www.digitaleducationcouncil.com/post/what-students-want-key-results-from-dec-global-ai-student-survey-2024\n\n15. What Faculty Want: Key Results from the Global AI Faculty Survey …, accessed March 6, 2025, https://www.digitaleducationcouncil.com/post/what-faculty-want-key-results-from-the-global-ai-faculty-survey-2025\n\n16. Survey: Higher Ed Leaders Doubt Student Preparedness for AI, accessed March 6, 2025, https://www.govtech.com/education/higher-ed/survey-higher-ed-leaders-doubt-student-preparedness-for-ai"}
{"chunk_id": "sites_campbell_edu__62497847e244cab4::c0014", "stable_id": "sites_campbell_edu__62497847e244cab4", "url": "https://sites.campbell.edu/academictechnology/2025/03/06/ai-in-higher-education-a-summary-of-recent-surveys-of-students-and-faculty/", "final_url": "https://sites.campbell.edu/academictechnology/2025/03/06/ai-in-higher-education-a-summary-of-recent-surveys-of-students-and-faculty/", "retrieved_at": "2026-01-22T00:14:23.461690Z", "title": "AI in Higher Education: A Meta Summary of Recent Surveys of Students and Faculty – Campbell Academic Technology Services", "section": "Using AI for School and Work", "source_type": "University / Official", "content_hash": "6207ddcc63c73b4300cb0baf4ee85c1f680cabfa1737a96bc2c273fe9edc1c65", "chunk_index": 14, "char_start": 19576, "char_end": 20786, "text": "17. Survey highlights faculty perception of generative AI – Early Bird, accessed March 6, 2025, https://early-bird.msudenver.edu/survey-highlights-faculty-perception-of-generative-ai/\n\n18. Surveys suggest a steep, rocky hill ahead for education’s adaptation …, accessed March 6, 2025, https://cte.ku.edu/surveys-suggest-steep-rocky-hill-ahead-educations-adaptation-ai\n\nGreat job, David. Miss your perspective and energy.\n\nNice job here! Thank you. –Lisa Martin, LxD\n\nI found it very interesting. I did not know exactly how much AI was being used or in what way. Great job.\n\nLearning styles are a myth.\n\nAwesome job 😎\n\nThis report offers a timely and valuable overview of AI’s evolving role in higher education, capturing key perspectives from both students and faculty during a pivotal period of adoption. The use of Google Gemini Advanced Pro 1.5 to synthesize recent survey data enhances the reliability and efficiency of the findings, while also serving as a practical example of how AI tools can support academic research and reporting. As institutions navigate the opportunities and challenges of AI integration, insights like these will be essential for informed, equitable, and effective implementation."}
{"chunk_id": "blog_methodistcollege_edu__35d435c16f5275ac::c0000", "stable_id": "blog_methodistcollege_edu__35d435c16f5275ac", "url": "https://blog.methodistcollege.edu/how-to-use-artificial-intelligence-as-a-study-tool", "final_url": "https://blog.methodistcollege.edu/how-to-use-artificial-intelligence-as-a-study-tool", "retrieved_at": "2026-01-22T00:14:30.882216Z", "title": "How to Use Artificial Intelligence as a Study Tool", "section": "Using AI for School and Work", "source_type": "University / Official", "content_hash": "d2859fe5a696d961225927924d81c1574ee6d51459339edb000d712ba261d149", "chunk_index": 0, "char_start": 0, "char_end": 1232, "text": "Artificial intelligence can be a powerful learning tool for college students, providing resources that make studying and completing assignments more accessible.\n\n“Technology is constantly revolutionizing the way we learn, and the impact it has on education is undeniable,” said Mark Billington, director of the Center for Research Education and Teaching Excellence (CREATE) at Nebraska Methodist College (NMC).\n\nWhile artificial intelligence platforms, like ChatGPT, are getting a reputation for writing essays in seconds, students can use them ethically.\n\nUsing artificial intelligence in higher education can help with student success and give faculty another resource to support different learning styles.\n\n“The reality is that this technology is going to rapidly evolve, so students and faculty will need to create more learning opportunities centered around multimedia products that demonstrate innovation and creativity,” said Heather Henrichs, arts and sciences professor at NMC.\n\n“There’s an opportunity for more personalized learning, allowing students to use AI to bridge learning gaps in real-time. A future goal for all of us should be maintaining focus on emotional intelligence while embracing the healthy uses of AI.”"}
{"chunk_id": "blog_methodistcollege_edu__35d435c16f5275ac::c0001", "stable_id": "blog_methodistcollege_edu__35d435c16f5275ac", "url": "https://blog.methodistcollege.edu/how-to-use-artificial-intelligence-as-a-study-tool", "final_url": "https://blog.methodistcollege.edu/how-to-use-artificial-intelligence-as-a-study-tool", "retrieved_at": "2026-01-22T00:14:30.882216Z", "title": "How to Use Artificial Intelligence as a Study Tool", "section": "Using AI for School and Work", "source_type": "University / Official", "content_hash": "d2859fe5a696d961225927924d81c1574ee6d51459339edb000d712ba261d149", "chunk_index": 1, "char_start": 1232, "char_end": 2438, "text": "Maintaining Academic Integrity Using AI\n\nStudents must maintain academic integrity when using an AI tool for college assignments - using artificial intelligence tools like chatbots to generate ideas or as a learning tool, not to complete assignments.\n\nChatGPT, launched in fall 2021 by an artificial intelligence research group, creates detailed responses to questions and prompts, according to CNN Business. The chatbot has been used to draft research papers, violating academic integrity policies.\n\nAcademic integrity violations include cheating, collusion or complicity, falsification or fabrication, misrepresentation and plagiarism.\n\n“As a student, if you're using a chatbot to write your paper, you may be accused of cheating, plagiarism and lack of academic integrity. The consequences of this can be severe and include expulsion, loss of transcripts and the forfeiture of tuition and fees,” Billington said.\n\n“Not only that, students are cheating themselves out of the opportunity to learn and grow. Assignments are meant to help students understand a topic, learn how to learn and think for themselves. These are essential skills for any student to master if they want to advance in their career.”"}
{"chunk_id": "blog_methodistcollege_edu__35d435c16f5275ac::c0002", "stable_id": "blog_methodistcollege_edu__35d435c16f5275ac", "url": "https://blog.methodistcollege.edu/how-to-use-artificial-intelligence-as-a-study-tool", "final_url": "https://blog.methodistcollege.edu/how-to-use-artificial-intelligence-as-a-study-tool", "retrieved_at": "2026-01-22T00:14:30.882216Z", "title": "How to Use Artificial Intelligence as a Study Tool", "section": "Using AI for School and Work", "source_type": "University / Official", "content_hash": "d2859fe5a696d961225927924d81c1574ee6d51459339edb000d712ba261d149", "chunk_index": 2, "char_start": 2438, "char_end": 3657, "text": "Henrichs said students should be aware that AI tools can utilize an algorithm similar to search engines, which can increase bias or focus on a topic.\n\n“It has been recognized that AI apps or software can demonstrate bias. Students need to be sure to screen, read and recognize items generated that are showing implicit bias,” Henrichs said. “There also appears to be limitations to historical context with AI, which means it’s not capable of making proper correlations or even timelines.”\n\nUsing Artificial Intelligence as a Learning Tool\n\nArtificial intelligence can be an innovative learning tool, helping students improve their writing or study more effectively.\n\n“By creating personalized study guides, students can learn the material more efficiently and effectively,” Billington said. “They can also dig deeper into the areas of their own interest.”\n\nCollege students can use artificial intelligence to:\n\n-\n\nGenerate ideas or creativity.\n\n-\n\nExplore how to frame ideas when struggling with outlining information.\n\n-\n\nCreate a formative assessment.\n\n-\n\nGenerate practice questions from the topic.\n\n-\n\nPractice speech skills for a persuasive argument.\n\n-\n\nCreate study plans.\n\n-\n\nAssist with note-taking strategies."}
{"chunk_id": "blog_methodistcollege_edu__35d435c16f5275ac::c0003", "stable_id": "blog_methodistcollege_edu__35d435c16f5275ac", "url": "https://blog.methodistcollege.edu/how-to-use-artificial-intelligence-as-a-study-tool", "final_url": "https://blog.methodistcollege.edu/how-to-use-artificial-intelligence-as-a-study-tool", "retrieved_at": "2026-01-22T00:14:30.882216Z", "title": "How to Use Artificial Intelligence as a Study Tool", "section": "Using AI for School and Work", "source_type": "University / Official", "content_hash": "d2859fe5a696d961225927924d81c1574ee6d51459339edb000d712ba261d149", "chunk_index": 3, "char_start": 3657, "char_end": 4922, "text": "-\n\nPractice providing feedback or education to others that is appropriate and leveled to the intended audience.\n\nA Step-by-Step Guide to Using a Chatbot in College\n\nWhen using a chatbot to help you write a paper or create a study guide, follow these steps:\n\nAssistance writing a paper:\n\n1. Organize your thoughts and ideas about your topic.\n\n2. Ask the chatbot for an outline of a paper about your topic.\n\n3. Compare your ideas with the chatbot's suggestions and have a conversation with the bot about your paper. Treat the bot like an assistant.\n\n4. Ask the chatbot for opposing views and consider both sides of the argument.\n\n5. Write your paper on your own, fact-checking any information provided by the bot and citing your sources.\n\n6. Once you've finished your paper, ask the chatbot to edit your work using the appropriate style guidelines. You could also ask the bot to make your writing more engaging.\n\nBuild a personalized study guide:\n\n1. Choose a topic you want to learn more about.\n\n2. Ask the chatbot for the fundamentals of that topic.\n\n3. If there are any concepts you're unfamiliar with, ask the bot to explain them.\n\n4. Ask the bot for a few examples of the topic.\n\n5. Have the bot create flashcards for the keywords discussed in your conversation."}
{"chunk_id": "blog_methodistcollege_edu__35d435c16f5275ac::c0004", "stable_id": "blog_methodistcollege_edu__35d435c16f5275ac", "url": "https://blog.methodistcollege.edu/how-to-use-artificial-intelligence-as-a-study-tool", "final_url": "https://blog.methodistcollege.edu/how-to-use-artificial-intelligence-as-a-study-tool", "retrieved_at": "2026-01-22T00:14:30.882216Z", "title": "How to Use Artificial Intelligence as a Study Tool", "section": "Using AI for School and Work", "source_type": "University / Official", "content_hash": "d2859fe5a696d961225927924d81c1574ee6d51459339edb000d712ba261d149", "chunk_index": 4, "char_start": 4922, "char_end": 5694, "text": "6. Copy and paste the flashcards into Quizlet or another study tool to help you review the material more effectively and efficiently.\n\nStudent Academic Support at NMC\n\nWhile artificial intelligence can be used as a supplemental learning tool, there are many ways to receive academic support on campus.\n\nThe Office of Academic Success at NMC works to enhance a student’s academic performance by providing academic support and resources in the form of tutoring, supplemental instruction, ADA accommodations, academic coaching, skills workshops and individual attention.\n\nAccess to technology can also make the learning experience easier for students.\n\nThe NMClink: Technology Enriched Learning program provides students with learning tools that improve academic performance."}
{"chunk_id": "scs_oregonstate_edu__f3cf6e6718506e10::c0000", "stable_id": "scs_oregonstate_edu__f3cf6e6718506e10", "url": "https://scs.oregonstate.edu/using-artificial-intelligence", "final_url": "https://scs.oregonstate.edu/using-artificial-intelligence", "retrieved_at": "2026-01-22T00:14:25.625837Z", "title": "Using Artificial Intelligence | Student Community Standards | Office of the Dean of Students | Oregon State University", "section": "Using AI for School and Work", "source_type": "University / Official", "content_hash": "7449c688c8f4f2d34eef07554fc13e45c073e32b08fcc73aa1a129e440d43b0c", "chunk_index": 0, "char_start": 0, "char_end": 1559, "text": "The thoughtful and appropriate use of generative artificial intelligence (AI) can enrich your learning experience at Oregon State. OSU is committed to preparing you for a future where artificial intelligence use is commonplace. Because of this, your faculty are at the forefront of the AI conversation, exploring ways to integrate Artificial Intelligence into the university’s curriculum. Though advanced, artificial intelligence has limitations, including hallucinations and bias. It is important to understand those shortcomings and the continued value of human creation.\n\nThe misuse of artificial intelligence on an assignment could violate the university's Academic Misconduct Policy. Relevant provisions that AI misuse could violate include:\n\n- Plagiarism: directly copying AI-generated material without a citation or reference is considered plagiarism\n\n- Cheating: using AI to complete an assignment without prior authorization from your instructor\n\n- Fabrication: using AI to create counterfeit citations, interview responses, or research results\n\nAt OSU, individual faculty have the freedom to set course-specific expectations for artificial intelligence use. It is important to recognize that different classes may have widely varying or even contradictory expectations. You must follow the specific expectations for each course. Always begin the term by carefully reading your syllabus to understand your instructor's expectations regarding AI usage. In order to use generative AI on an assignment, you must be able to answer “yes” to two questions:"}
{"chunk_id": "scs_oregonstate_edu__f3cf6e6718506e10::c0001", "stable_id": "scs_oregonstate_edu__f3cf6e6718506e10", "url": "https://scs.oregonstate.edu/using-artificial-intelligence", "final_url": "https://scs.oregonstate.edu/using-artificial-intelligence", "retrieved_at": "2026-01-22T00:14:25.625837Z", "title": "Using Artificial Intelligence | Student Community Standards | Office of the Dean of Students | Oregon State University", "section": "Using AI for School and Work", "source_type": "University / Official", "content_hash": "7449c688c8f4f2d34eef07554fc13e45c073e32b08fcc73aa1a129e440d43b0c", "chunk_index": 1, "char_start": 1559, "char_end": 2805, "text": "1. Has your instructor provided expectations on how AI can be used on this assignment?\n\n2. Do you understand what your instructor’s expectations are?\n\nPotential uses of AI within course settings could include the following:\n\n- Idea generation: Brainstorming research topics, thesis statements, or outlines.\n\n- Clarification: Explaining difficult concepts or assignment prompts.\n\n- Editing and proofreading: Checking grammar, tone, and clarity in writing.\n\n- Coding assistance: Debugging, explaining algorithms, or generating sample code for learning.\n\n- Formatting help: Creating citations, tables, or properly formatted documents.\n\n- Practice and feedback: Simulating exam questions or providing feedback on drafts.\n\n- Summarization: Condensing readings or lecture notes to highlight key points.\n\n- Translation or accessibility: Translating text or converting it into plain language or alternate formats.\n\nYou should always seek clarification on what type of use is permitted and what is prohibited. It is ok to ask questions about AI!\n\nWhenever generative AI usage is allowed, make sure to cite how you employed it in your work. The following decision tree will help walk you through whether or not you should use AI on a particular assignment."}
{"chunk_id": "scs_oregonstate_edu__f3cf6e6718506e10::c0002", "stable_id": "scs_oregonstate_edu__f3cf6e6718506e10", "url": "https://scs.oregonstate.edu/using-artificial-intelligence", "final_url": "https://scs.oregonstate.edu/using-artificial-intelligence", "retrieved_at": "2026-01-22T00:14:25.625837Z", "title": "Using Artificial Intelligence | Student Community Standards | Office of the Dean of Students | Oregon State University", "section": "Using AI for School and Work", "source_type": "University / Official", "content_hash": "7449c688c8f4f2d34eef07554fc13e45c073e32b08fcc73aa1a129e440d43b0c", "chunk_index": 2, "char_start": 2805, "char_end": 4137, "text": "Citing Generative Artificial Intelligence\n\nCiting generative artificial intelligence (AI) in an academic setting is critical, as it reflects both the rigorous scholarly approach to research and the ethical considerations surrounding AI technology. When incorporating AI-generated content, such as text or art, into academic work, proper citation not only gives credit to the underlying algorithms and data sources but also acknowledges the human input in training and fine-tuning these models. Additionally, citing AI sources helps maintain transparency and integrity in academic discourse, allowing readers to trace the origins and authenticity of the information presented.\n\nHowever, the nuances of citing generative AI lie in the evolving landscape of AI capabilities and the ethical complexities involved. The MLA Style Center and the American Psychological Association (APA), among others, have provided guidelines on how to cite AI in your work. Links to these resources are linked below.\n\nYou should always check your syllabus to confirm your instructor’s expectations for AI citations. If you have questions, ask!\n\nHow to Cite AI\n\nAI-generated content should be cited much like content used from journals, books, and websites, with the company used as the author. Examples provided by common academic style guides are below."}
{"chunk_id": "scs_oregonstate_edu__f3cf6e6718506e10::c0003", "stable_id": "scs_oregonstate_edu__f3cf6e6718506e10", "url": "https://scs.oregonstate.edu/using-artificial-intelligence", "final_url": "https://scs.oregonstate.edu/using-artificial-intelligence", "retrieved_at": "2026-01-22T00:14:25.625837Z", "title": "Using Artificial Intelligence | Student Community Standards | Office of the Dean of Students | Oregon State University", "section": "Using AI for School and Work", "source_type": "University / Official", "content_hash": "7449c688c8f4f2d34eef07554fc13e45c073e32b08fcc73aa1a129e440d43b0c", "chunk_index": 3, "char_start": 4137, "char_end": 5384, "text": "- Modern Language Association (MLA Style)\n\nFormat: “Title of Source” prompt. Title of Container/AI Tool, Day Month version, Publisher, Date, URL.\n\nExample of Works-Cited citation:\n\n“How to cite in MLA” prompt. ChatGPT, 17 Mar. version, OpenAI, 27 Mar. 2024, https://chat.openai.com/chat\n\n- American Psychological Association (APA Style)\n\nFormat: Author. (Date). Title (Version) [Large language model]. URL\n\nExample of Works-Cited citation:\n\nOpenAI. (2024). ChatGPT (Mar 14 version) [Large language model]. https://chat.openai.com/chat\n\nGlossary of Important AI Terms\n\nGenerative Artificial Intelligence (Gen AI): artificial intelligence that is capable of generating new content (such as images or text) in response to a submitted prompt (such as a query) by learning from a large reference database of examples\n\nMachine Learning: a computational method that is a subfield of artificial intelligence and that enables a computer to learn to perform tasks by analyzing a large dataset without being explicitly programmed\n\nDeep Learning: a form machine learning in which the computer network rapidly teaches itself to understand a concept without human intervention by performing a large number of iterative calculations on an extremely large dataset"}
{"chunk_id": "scs_oregonstate_edu__f3cf6e6718506e10::c0004", "stable_id": "scs_oregonstate_edu__f3cf6e6718506e10", "url": "https://scs.oregonstate.edu/using-artificial-intelligence", "final_url": "https://scs.oregonstate.edu/using-artificial-intelligence", "retrieved_at": "2026-01-22T00:14:25.625837Z", "title": "Using Artificial Intelligence | Student Community Standards | Office of the Dean of Students | Oregon State University", "section": "Using AI for School and Work", "source_type": "University / Official", "content_hash": "7449c688c8f4f2d34eef07554fc13e45c073e32b08fcc73aa1a129e440d43b0c", "chunk_index": 4, "char_start": 5384, "char_end": 5992, "text": "Neural Network: a computer architecture in which a number of processors are interconnected in a manner suggestive of the connections between neurons in a human brain and which is able to learn by a process of trial and error\n\nHallucination: a plausible but false or misleading response generated by an artificial intelligence algorithm\n\nDefinitions provided by Meriam-Webster Dictionary\n\nLibrary Resources\n\nOSU's Library provides a helpful primer on generative AI. You can access it at this link.\n\nAcademic Integrity\n\nIf an instructor has alleged that you may have misused an AI tool, please visit this page."}
